{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68391f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ðŸ“¦ IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path().resolve().parents[3]\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "from lore_sa.client_utils import ClientUtilsMixin\n",
    "\n",
    "# Otros\n",
    "from pathlib import Path\n",
    "from filelock import FileLock  # pip install filelock\n",
    "import pandas as pd, os\n",
    "from graphviz import Digraph\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import cProfile, pstats, io\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from lore_sa.client_utils.explanation_intersection import ExplanationIntersection\n",
    "from lore_sa.client_utils import LabelShardPartitioner\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# âš™ï¸ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "\n",
    "NUM_TRAIN_ROUNDS = 2        # rondas donde entrenas la NN\n",
    "NUM_SERVER_ROUNDS = 3       # la Ãºltima solo para explicaciones\n",
    "NUM_CLIENTS = 6\n",
    "SEED = 42\n",
    "\n",
    "NON_IID = True   # o False para los experimentos IID\n",
    "NON_IID_ALPHA = 1.0  # por ejemplo, Dirichlet mÃ¡s sesgado\n",
    "\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "GLOBAL_TEST_IDX = None\n",
    "GLOBAL_TEST_HASHES = None\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# ðŸ§¹ Borrar TODOS los CSV individuales de clientes\n",
    "# ==============================================\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Borra TODO lo que haya dentro (csv, pth, imÃ¡genes, etc.)\n",
    "for f in results_dir.iterdir():\n",
    "    if f.is_file():\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ðŸ”§ UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ðŸ“¥ PREPROCESADO DATASET\n",
    "# =======================\n",
    "def preprocess_df(df: pd.DataFrame, dataset_name: str, class_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"adult\" in dataset_name.lower():\n",
    "        df.drop(columns=['fnlwgt', 'education-num', 'capital-gain', 'capital-loss'],\n",
    "                inplace=True, errors=\"ignore\")\n",
    "\n",
    "    elif \"churn\" in dataset_name.lower():\n",
    "        df.drop(columns=['customerID', 'TotalCharges'],\n",
    "                inplace=True, errors=\"ignore\")\n",
    "        if \"MonthlyCharges\" in df.columns:\n",
    "            df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        if \"tenure\" in df.columns:\n",
    "            df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        if \"SeniorCitizen\" in df.columns:\n",
    "            df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=[c for c in [\"MonthlyCharges\", \"tenure\"] if c in df.columns], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in dataset_name.lower():\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    # object -> category (solo baja cardinalidad)\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if col != class_col and df[col].nunique(dropna=True) < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _stable_row_hash(df: pd.DataFrame) -> np.ndarray:\n",
    "    return pd.util.hash_pandas_object(df, index=False).astype(\"uint64\").to_numpy()\n",
    "\n",
    "# =======================\n",
    "# ðŸ“¥ CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name: str, class_col: str):\n",
    "    \"\"\"\n",
    "    Lee TODO el pool (train con num_partitions=1) para fijar:\n",
    "    - cat_features (categorical cols)\n",
    "    - num_features\n",
    "    - categories_global (OHE categories_ en el orden de cat_features)\n",
    "    - onehot_columns (nombres finales onehot)\n",
    "    \"\"\"\n",
    "    fds_tmp = FederatedDataset(\n",
    "        dataset=flower_dataset_name,\n",
    "        partitioners={\"train\": IidPartitioner(num_partitions=1)}\n",
    "    )\n",
    "    df_all = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "    df_all = preprocess_df(df_all, flower_dataset_name, class_col)\n",
    "\n",
    "    # asegurar category dtype\n",
    "    for col in df_all.select_dtypes(include=[\"object\"]).columns:\n",
    "        if col != class_col and df_all[col].nunique(dropna=True) < 50:\n",
    "            df_all[col] = df_all[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [c for c in df_all.columns if df_all[c].dtype.name == \"category\" and c != class_col]\n",
    "    num_features = [c for c in df_all.columns if df_all[c].dtype.kind in \"fi\" and c != class_col]\n",
    "\n",
    "    if len(cat_features) > 0:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        ohe.fit(df_all[cat_features])\n",
    "        categories_global = ohe.categories_\n",
    "        onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    else:\n",
    "        categories_global = []\n",
    "        onehot_columns = []\n",
    "\n",
    "    return cat_features, num_features, categories_global, onehot_columns, df_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ðŸ“¥ CARGA GENERAL + TEST GLOBAL SIN FUGA\n",
    "# ============================================\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      X_train, y_train,\n",
    "      X_test_local, y_test_local,\n",
    "      X_test_global, y_test_global,\n",
    "      tabular_dataset, feature_names_out, label_encoder,\n",
    "      num_transformer, numeric_features,\n",
    "      encoder (ColumnTransformerEnc), preprocessor\n",
    "    \"\"\"\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "    global GLOBAL_TEST_IDX, GLOBAL_TEST_HASHES\n",
    "\n",
    "    # 1) Info global OHE + df_all pool\n",
    "    cat_features, num_features, categories_global, onehot_columns, df_all = get_global_onehot_info(\n",
    "        flower_dataset_name, class_col\n",
    "    )\n",
    "\n",
    "    # 2) LabelEncoder global (clases estables)\n",
    "    if not UNIQUE_LABELS:\n",
    "        le_global = LabelEncoder()\n",
    "        le_global.fit(df_all[class_col])\n",
    "        UNIQUE_LABELS[:] = le_global.classes_.tolist()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "\n",
    "    y_all = label_encoder.transform(df_all[class_col])\n",
    "\n",
    "    # 3) Definir TEST GLOBAL una sola vez (idx + hashes estables)\n",
    "    if GLOBAL_TEST_IDX is None:\n",
    "        idx = np.arange(len(df_all))\n",
    "        _, GLOBAL_TEST_IDX = train_test_split(\n",
    "            idx,\n",
    "            test_size=0.2,\n",
    "            random_state=SEED,\n",
    "            stratify=y_all if len(np.unique(y_all)) > 1 else None\n",
    "        )\n",
    "        row_hash_all = _stable_row_hash(df_all)\n",
    "        GLOBAL_TEST_HASHES = set(row_hash_all[GLOBAL_TEST_IDX].tolist())\n",
    "\n",
    "    # 4) Crear/usar FederatedDataset particionado (por filas)\n",
    "    if fds is None:\n",
    "        if NON_IID:\n",
    "            partitioner = LabelShardPartitioner(\n",
    "                num_partitions=num_partitions,\n",
    "                partition_by=class_col,\n",
    "                n_classes_per_client=1,  # o 2 si quieres algo menos extremo\n",
    "                shards_per_class=3,        # prueba 2, 4, etc.\n",
    "                shuffle=True,\n",
    "                seed=SEED,\n",
    "            )\n",
    "        else:\n",
    "            partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "\n",
    "        fds = FederatedDataset(\n",
    "            dataset=flower_dataset_name,\n",
    "            partitioners={\"train\": partitioner},\n",
    "        )\n",
    "\n",
    "    df_client = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "    df_client = preprocess_df(df_client, flower_dataset_name, class_col)\n",
    "\n",
    "    # 5) Eliminar filas que estÃ©n en el TEST GLOBAL (sin fuga)\n",
    "    row_hash_client = _stable_row_hash(df_client)\n",
    "    keep_mask = ~np.isin(row_hash_client, np.fromiter(GLOBAL_TEST_HASHES, dtype=\"uint64\"))\n",
    "    df_client = df_client.loc[keep_mask].copy()\n",
    "\n",
    "    # 6) TabularDataset/descriptor (cliente) para LORE\n",
    "    tabular_dataset = TabularDataset(df_client.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    # Asegurar distinct_values local (por si viene vacÃ­o)\n",
    "    for col, info in descriptor.get(\"categorical\", {}).items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(df_client[col].dropna().unique())\n",
    "\n",
    "    # 7) X/y cliente (sin onehot aÃºn)\n",
    "    y = label_encoder.transform(df_client[class_col])\n",
    "    X_raw = df_client.drop(columns=[class_col])\n",
    "\n",
    "    numeric_features = list(descriptor.get(\"numeric\", {}).keys())\n",
    "    categorical_features = list(descriptor.get(\"categorical\", {}).keys())\n",
    "\n",
    "    # Ojo: aquÃ­ mantenemos el mismo orden que usa el descriptor del cliente\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    num_idx = list(range(len(numeric_features)))\n",
    "    cat_idx = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    # 8) Preprocessor con categorÃ­as globales (dim estable)\n",
    "    transformers = [(\"num\", \"passthrough\", num_idx)]\n",
    "    if len(categorical_features) > 0:\n",
    "        # IMPORTANTÃSIMO: categories_global estÃ¡ en el orden de cat_features (global)\n",
    "        # pero aquÃ­ categorical_features puede venir en otro orden -> reordenamos categories_global\n",
    "        cat_to_pos = {c: i for i, c in enumerate(cat_features)}\n",
    "        cats_ordered = [categories_global[cat_to_pos[c]] for c in categorical_features]\n",
    "\n",
    "        transformers.append((\n",
    "            \"cat\",\n",
    "            OneHotEncoder(\n",
    "                sparse_output=False,\n",
    "                handle_unknown=\"ignore\",\n",
    "                categories=cats_ordered\n",
    "            ),\n",
    "            cat_idx\n",
    "        ))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "\n",
    "    # 9) Split local + FIT SOLO con train local\n",
    "    X_train_raw, X_test_local_raw, y_train, y_test_local = train_test_split(\n",
    "        X_raw[FEATURES], y,\n",
    "        test_size=0.3,\n",
    "        random_state=SEED,\n",
    "        stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train_raw.to_numpy())\n",
    "    X_test_local = preprocessor.transform(X_test_local_raw.to_numpy())\n",
    "\n",
    "    # 10) Construir test global REAL (mismo preprocessor del cliente)\n",
    "    df_global = df_all.iloc[GLOBAL_TEST_IDX].copy()\n",
    "    df_global = preprocess_df(df_global, flower_dataset_name, class_col)\n",
    "\n",
    "    X_test_global = preprocessor.transform(df_global.drop(columns=[class_col])[FEATURES].to_numpy())\n",
    "    y_test_global = label_encoder.transform(df_global[class_col])\n",
    "\n",
    "    # 11) Feature names finales (num + onehot) para NN/servidor\n",
    "    feature_names_out = []\n",
    "    feature_names_out += list(numeric_features)\n",
    "    if len(categorical_features) > 0:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features).tolist()\n",
    "        feature_names_out += cat_names\n",
    "\n",
    "    FEATURES[:] = feature_names_out  # ahora sÃ­: columnas finales (onehot)\n",
    "\n",
    "    # 12) Encoder LORE con distinct_values globales (para reglas legibles)\n",
    "    descriptor_global = descriptor.copy()\n",
    "    if \"categorical\" in descriptor_global and len(categorical_features) > 0:\n",
    "        # cats_ordered ya estÃ¡ en orden de categorical_features\n",
    "        for i, col in enumerate(categorical_features):\n",
    "            if col in descriptor_global[\"categorical\"]:\n",
    "                descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(cats_ordered[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    num_transformer = preprocessor.named_transformers_[\"num\"] if \"num\" in preprocessor.named_transformers_ else None\n",
    "\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test_local, y_test_local,\n",
    "        X_test_global, y_test_global,\n",
    "        tabular_dataset, feature_names_out, label_encoder,\n",
    "        num_transformer, numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "\n",
    "# =======================\n",
    "# âœ… DATASET\n",
    "# =======================\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "# CLASS_COLUMN = \"HeartDisease\"\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "CLASS_COLUMN = \"diagnosis\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "# CLASS_COLUMN = \"Outcome\"\n",
    "\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c49bf6",
   "metadata": {},
   "source": [
    "### HOLDOUT DEL SERVIDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 10:10:48,889 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2026-02-17 10:10:49,105 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2026-02-17 10:10:49,284 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 564\n",
      "2026-02-17 10:10:49,417 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2026-02-17 10:10:49,423 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2026-02-17 10:10:49,807 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2026-02-17 10:10:49,948 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2026-02-17 10:10:50,079 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 564\n",
      "2026-02-17 10:10:50,202 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2026-02-17 10:10:50,202 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2026-02-17 10:10:50,555 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2026-02-17 10:10:50,682 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 564\n",
      "2026-02-17 10:10:50,824 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e?recursive=False&expand=False HTTP/11\" 200 207\n",
      "2026-02-17 10:10:50,961 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2026-02-17 10:10:51,118 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2026-02-17 10:10:51,286 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 564\n",
      "2026-02-17 10:10:51,415 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2026-02-17 10:10:51,415 filelock     DEBUG    Attempting to acquire lock 2269613856320 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:51,415 filelock     DEBUG    Lock 2269613856320 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:51,430 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2026-02-17 10:10:51,430 filelock     DEBUG    Attempting to release lock 2269613856320 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:51,446 filelock     DEBUG    Lock 2269613856320 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:51,499 filelock     DEBUG    Attempting to acquire lock 2269615570384 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:51,501 filelock     DEBUG    Lock 2269615570384 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:51,502 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2026-02-17 10:10:51,504 filelock     DEBUG    Attempting to release lock 2269615570384 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:51,505 filelock     DEBUG    Lock 2269615570384 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:51,868 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2026-02-17 10:10:51,995 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 564\n",
      "2026-02-17 10:10:52,121 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2026-02-17 10:10:52,238 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2026-02-17 10:10:52,388 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2026-02-17 10:10:52,525 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2026-02-17 10:10:52,669 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2026-02-17 10:10:52,800 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2026-02-17 10:10:52,930 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2026-02-17 10:10:53,096 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 564\n",
      "2026-02-17 10:10:53,214 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2026-02-17 10:10:53,230 filelock     DEBUG    Attempting to acquire lock 2269614770032 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:53,231 filelock     DEBUG    Lock 2269614770032 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:53,232 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2026-02-17 10:10:53,235 filelock     DEBUG    Attempting to release lock 2269614770032 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:53,236 filelock     DEBUG    Lock 2269614770032 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2026-02-17 10:10:53,237 filelock     DEBUG    Attempting to acquire lock 2269614772576 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:53,238 filelock     DEBUG    Lock 2269614772576 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:53,239 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2026-02-17 10:10:53,240 filelock     DEBUG    Attempting to release lock 2269614772576 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2026-02-17 10:10:53,241 filelock     DEBUG    Lock 2269614772576 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ X_train (primeras filas):\n",
      "       0      1       2       3        4        5        6        7       8   \\\n",
      "0   14.86  23.21  100.40   671.4  0.10440  0.19800  0.16970  0.08878  0.1737   \n",
      "1   16.03  15.51  105.80   793.2  0.09491  0.13710  0.12040  0.07041  0.1782   \n",
      "2   13.71  20.83   90.20   577.9  0.11890  0.16450  0.09366  0.05985  0.2196   \n",
      "3   13.61  24.98   88.05   582.7  0.09488  0.08511  0.08625  0.04489  0.1609   \n",
      "4   17.06  21.00  111.80   918.6  0.11190  0.10560  0.15080  0.09934  0.1727   \n",
      "5   18.63  25.11  124.80  1088.0  0.10640  0.18870  0.23190  0.12440  0.2183   \n",
      "6   15.13  29.81   96.71   719.5  0.08320  0.04605  0.04686  0.02739  0.1852   \n",
      "7   17.91  21.02  124.40   994.0  0.12300  0.25760  0.31890  0.11980  0.2113   \n",
      "8   19.73  19.82  130.70  1206.0  0.10620  0.18490  0.24170  0.09740  0.1733   \n",
      "9   13.28  20.28   87.32   545.2  0.10410  0.14360  0.09847  0.06158  0.1974   \n",
      "10  16.74  21.59  110.10   869.5  0.09610  0.13360  0.13480  0.06018  0.1896   \n",
      "11  11.08  18.83   73.30   361.6  0.12160  0.21540  0.16890  0.06367  0.2196   \n",
      "12  16.26  21.88  107.50   826.8  0.11650  0.12830  0.17990  0.07981  0.1869   \n",
      "13  17.08  27.15  111.20   930.9  0.09898  0.11100  0.10070  0.06431  0.1793   \n",
      "14  14.68  20.13   94.74   684.5  0.09867  0.07200  0.07395  0.05259  0.1586   \n",
      "15  17.02  23.98  112.80   899.3  0.11970  0.14960  0.24170  0.12030  0.2248   \n",
      "16  23.27  22.04  152.10  1686.0  0.08439  0.11450  0.13240  0.09702  0.1801   \n",
      "17  15.66  23.20  110.20   773.5  0.11090  0.31140  0.31760  0.13770  0.2495   \n",
      "18  16.24  18.77  108.80   805.1  0.10660  0.18020  0.19480  0.09052  0.1876   \n",
      "19  17.01  20.26  109.70   904.3  0.08772  0.07304  0.06950  0.05390  0.2026   \n",
      "20  18.81  19.98  120.90  1102.0  0.08923  0.05884  0.08020  0.05843  0.1550   \n",
      "21  20.31  27.06  132.90  1288.0  0.10000  0.10880  0.15190  0.09333  0.1814   \n",
      "22  17.14  16.40  116.00   912.7  0.11860  0.22760  0.22290  0.14010  0.3040   \n",
      "23  15.46  19.48  101.70   748.9  0.10920  0.12230  0.14660  0.08087  0.1931   \n",
      "24  15.06  19.83  100.30   705.6  0.10390  0.15530  0.17000  0.08815  0.1855   \n",
      "25  14.90  22.53  102.10   685.0  0.09947  0.22250  0.27330  0.09711  0.2041   \n",
      "26  10.95  21.35   71.90   371.1  0.12270  0.12180  0.10440  0.05669  0.1895   \n",
      "27  19.16  26.60  126.20  1138.0  0.10200  0.14530  0.19210  0.09664  0.1902   \n",
      "28  13.17  21.81   85.42   531.5  0.09714  0.10470  0.08259  0.05252  0.1746   \n",
      "29  18.31  18.58  118.60  1041.0  0.08588  0.08468  0.08169  0.05814  0.1621   \n",
      "30  19.53  18.90  129.50  1217.0  0.11500  0.16420  0.21970  0.10620  0.1792   \n",
      "31  17.19  22.07  111.60   928.3  0.09726  0.08995  0.09061  0.06527  0.1867   \n",
      "32  20.48  21.46  132.50  1306.0  0.08355  0.08348  0.09042  0.06022  0.1467   \n",
      "33  15.70  20.31  101.20   766.6  0.09597  0.08799  0.06593  0.05189  0.1618   \n",
      "34  15.85  23.95  103.70   782.7  0.08401  0.10020  0.09938  0.05364  0.1847   \n",
      "35  23.09  19.83  152.10  1682.0  0.09342  0.12750  0.16760  0.10030  0.1505   \n",
      "36  19.55  28.77  133.60  1207.0  0.09260  0.20630  0.17840  0.11440  0.1893   \n",
      "37  21.10  20.52  138.10  1384.0  0.09684  0.11750  0.15720  0.11550  0.1554   \n",
      "38  14.99  25.20   95.54   698.8  0.09387  0.05131  0.02398  0.02899  0.1565   \n",
      "39  19.45  19.33  126.50  1169.0  0.10350  0.11880  0.13790  0.08591  0.1776   \n",
      "40  12.77  22.47   81.72   506.3  0.09055  0.05761  0.04711  0.02704  0.1585   \n",
      "41  13.73  22.61   93.60   578.3  0.11310  0.22930  0.21280  0.08025  0.2069   \n",
      "\n",
      "         9   ...     20     21      22      23       24       25       26  \\\n",
      "0   0.06672  ...  16.08  27.78  118.60   784.7  0.13160  0.46480  0.45890   \n",
      "1   0.05976  ...  18.76  21.98  124.30  1070.0  0.14350  0.44780  0.49560   \n",
      "2   0.07451  ...  17.06  28.14  110.60   897.0  0.16540  0.36820  0.26780   \n",
      "3   0.05871  ...  16.99  35.27  108.60   906.5  0.12650  0.19430  0.31690   \n",
      "4   0.06071  ...  20.99  33.15  143.20  1362.0  0.14490  0.20530  0.39200   \n",
      "5   0.06197  ...  23.15  34.01  160.50  1670.0  0.14910  0.42570  0.61330   \n",
      "6   0.05294  ...  17.26  36.91  110.10   931.4  0.11480  0.09866  0.15470   \n",
      "7   0.07115  ...  20.80  27.78  149.60  1304.0  0.18730  0.59170  0.90340   \n",
      "8   0.06697  ...  25.28  25.59  159.80  1933.0  0.17100  0.59550  0.84890   \n",
      "9   0.06782  ...  17.38  28.00  113.10   907.2  0.15300  0.37240  0.36640   \n",
      "10  0.05656  ...  20.01  29.02  133.50  1229.0  0.15630  0.38350  0.54090   \n",
      "11  0.07950  ...  13.24  32.82   91.76   508.1  0.21840  0.93790  0.84020   \n",
      "12  0.06532  ...  17.73  25.21  113.70   975.2  0.14260  0.21160  0.33440   \n",
      "13  0.06281  ...  22.96  34.49  152.10  1648.0  0.16000  0.24440  0.26390   \n",
      "14  0.05922  ...  19.07  30.88  123.40  1138.0  0.14640  0.18710  0.29140   \n",
      "15  0.06382  ...  20.88  32.09  136.10  1344.0  0.16340  0.35590  0.55880   \n",
      "16  0.05553  ...  28.01  28.22  184.20  2403.0  0.12280  0.35830  0.39480   \n",
      "17  0.08104  ...  19.85  31.64  143.70  1226.0  0.15040  0.51720  0.61810   \n",
      "18  0.06684  ...  18.55  25.09  126.90  1031.0  0.13650  0.47060  0.50260   \n",
      "19  0.05223  ...  19.80  25.05  130.00  1210.0  0.11110  0.14860  0.19320   \n",
      "20  0.04996  ...  19.96  24.30  129.00  1236.0  0.12430  0.11600  0.22100   \n",
      "21  0.05572  ...  24.33  39.16  162.30  1844.0  0.15220  0.29450  0.37880   \n",
      "22  0.07413  ...  22.25  21.40  152.40  1461.0  0.15450  0.39490  0.38530   \n",
      "23  0.05796  ...  19.26  26.00  124.90  1156.0  0.15460  0.23940  0.37910   \n",
      "24  0.06284  ...  18.23  24.23  123.50  1025.0  0.15510  0.42030  0.52030   \n",
      "25  0.06898  ...  16.35  27.57  125.40   832.7  0.14190  0.70900  0.90190   \n",
      "26  0.06870  ...  12.84  35.34   87.22   514.0  0.19090  0.26980  0.40230   \n",
      "27  0.06220  ...  23.72  35.90  159.80  1724.0  0.17820  0.38410  0.57540   \n",
      "28  0.06177  ...  16.23  29.89  105.50   740.7  0.15030  0.39040  0.37280   \n",
      "29  0.05425  ...  21.31  26.36  139.20  1410.0  0.12340  0.24450  0.35380   \n",
      "30  0.06552  ...  25.93  26.24  171.10  2053.0  0.14950  0.41160  0.61210   \n",
      "31  0.05580  ...  21.58  29.33  140.50  1436.0  0.15580  0.25670  0.38890   \n",
      "32  0.05177  ...  24.22  26.17  161.70  1750.0  0.12280  0.23110  0.31580   \n",
      "33  0.05549  ...  20.11  32.82  129.30  1269.0  0.14140  0.35470  0.29020   \n",
      "34  0.05338  ...  16.84  27.66  112.00   876.5  0.11310  0.19240  0.23220   \n",
      "35  0.05484  ...  30.79  23.87  211.50  2782.0  0.11990  0.36250  0.37940   \n",
      "36  0.06232  ...  25.05  36.27  178.60  1926.0  0.12810  0.53290  0.42510   \n",
      "37  0.05661  ...  25.68  32.07  168.20  2022.0  0.13680  0.31010  0.43990   \n",
      "38  0.05504  ...  14.99  25.20   95.54   698.8  0.09387  0.05131  0.02398   \n",
      "39  0.05647  ...  25.70  24.57  163.10  1972.0  0.14970  0.31610  0.43170   \n",
      "40  0.06065  ...  14.49  33.37   92.04   653.6  0.14190  0.15230  0.21770   \n",
      "41  0.07682  ...  15.03  32.01  108.80   697.7  0.16510  0.77250  0.69430   \n",
      "\n",
      "         27      28       29  \n",
      "0   0.17270  0.3000  0.08701  \n",
      "1   0.19810  0.3019  0.09124  \n",
      "2   0.15560  0.3196  0.11510  \n",
      "3   0.11840  0.2651  0.07397  \n",
      "4   0.18270  0.2623  0.07599  \n",
      "5   0.18480  0.3444  0.09782  \n",
      "6   0.06575  0.3233  0.06165  \n",
      "7   0.19640  0.3245  0.11980  \n",
      "8   0.25070  0.2749  0.12970  \n",
      "9   0.14920  0.3739  0.10270  \n",
      "10  0.18130  0.4863  0.08633  \n",
      "11  0.25240  0.4154  0.14030  \n",
      "12  0.10470  0.2736  0.07953  \n",
      "13  0.15550  0.3010  0.09060  \n",
      "14  0.16090  0.3029  0.08216  \n",
      "15  0.18470  0.3530  0.08482  \n",
      "16  0.23460  0.3589  0.09187  \n",
      "17  0.24620  0.3277  0.10190  \n",
      "18  0.17320  0.2770  0.10630  \n",
      "19  0.10960  0.3275  0.06469  \n",
      "20  0.12940  0.2567  0.05737  \n",
      "21  0.16970  0.3151  0.07999  \n",
      "22  0.25500  0.4066  0.10590  \n",
      "23  0.15140  0.2837  0.08019  \n",
      "24  0.21150  0.2834  0.08234  \n",
      "25  0.24750  0.2866  0.11550  \n",
      "26  0.14240  0.2964  0.09606  \n",
      "27  0.18720  0.3258  0.09720  \n",
      "28  0.16070  0.3693  0.09618  \n",
      "29  0.15710  0.3206  0.06938  \n",
      "30  0.19800  0.2968  0.09929  \n",
      "31  0.19840  0.3216  0.07570  \n",
      "32  0.14450  0.2238  0.07127  \n",
      "33  0.15410  0.3437  0.08631  \n",
      "34  0.11190  0.2809  0.06287  \n",
      "35  0.22640  0.2908  0.07277  \n",
      "36  0.19410  0.2818  0.10050  \n",
      "37  0.22800  0.2268  0.07425  \n",
      "38  0.02899  0.1565  0.05504  \n",
      "39  0.19990  0.3379  0.08950  \n",
      "40  0.09331  0.2829  0.08067  \n",
      "41  0.22080  0.3596  0.14310  \n",
      "\n",
      "[42 rows x 30 columns]\n",
      "\n",
      "ðŸ“Š DistribuciÃ³n de clases en y_train:\n",
      "1    42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“¦ X_test_local (primeras filas):\n",
      "       0      1       2       3        4        5        6        7       8   \\\n",
      "0   19.40  18.18  127.20  1145.0  0.10370  0.14420  0.16260  0.09464  0.1893   \n",
      "1   20.64  17.35  134.80  1335.0  0.09446  0.10760  0.15270  0.08941  0.1571   \n",
      "2   20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
      "3   21.75  20.99  147.30  1491.0  0.09401  0.19610  0.21950  0.10880  0.1721   \n",
      "4   18.61  20.25  122.10  1094.0  0.09440  0.10660  0.14900  0.07731  0.1697   \n",
      "5   16.27  20.71  106.90   813.7  0.11690  0.13190  0.14780  0.08488  0.1948   \n",
      "6   16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302  0.1590   \n",
      "7   17.42  25.56  114.50   948.0  0.10060  0.11460  0.16820  0.06597  0.1308   \n",
      "8   18.08  21.84  117.40  1024.0  0.07371  0.08642  0.11030  0.05778  0.1770   \n",
      "9   17.27  25.42  112.40   928.8  0.08331  0.11090  0.12040  0.05736  0.1467   \n",
      "10  13.81  23.75   91.56   597.8  0.13230  0.17680  0.15580  0.09176  0.2251   \n",
      "11  18.45  21.91  120.20  1075.0  0.09430  0.09709  0.11530  0.06847  0.1692   \n",
      "12  17.46  39.28  113.40   920.6  0.09812  0.12980  0.14170  0.08811  0.1809   \n",
      "13  20.55  20.86  137.80  1308.0  0.10460  0.17390  0.20850  0.13220  0.2127   \n",
      "14  15.75  19.22  107.10   758.6  0.12430  0.23640  0.29140  0.12420  0.2375   \n",
      "15  13.00  21.82   87.50   519.8  0.12730  0.19320  0.18590  0.09353  0.2350   \n",
      "16  16.65  21.38  110.00   904.6  0.11210  0.14570  0.15250  0.09170  0.1995   \n",
      "17  15.30  25.27  102.40   732.4  0.10820  0.16970  0.16830  0.08751  0.1926   \n",
      "18  24.63  21.60  165.50  1841.0  0.10300  0.21060  0.23100  0.14710  0.1991   \n",
      "\n",
      "         9   ...     20     21     22      23       24      25      26  \\\n",
      "0   0.05892  ...  23.79  28.65  152.4  1628.0  0.15180  0.3749  0.4316   \n",
      "1   0.05478  ...  25.37  23.17  166.8  1946.0  0.15620  0.3055  0.4159   \n",
      "2   0.05883  ...  22.54  16.67  152.2  1575.0  0.13740  0.2050  0.4000   \n",
      "3   0.06194  ...  28.19  28.18  195.9  2384.0  0.12720  0.4725  0.5807   \n",
      "4   0.05699  ...  21.31  27.26  139.9  1403.0  0.13380  0.2117  0.3446   \n",
      "5   0.06277  ...  19.28  30.38  129.8  1121.0  0.15900  0.2947  0.3597   \n",
      "6   0.05648  ...  18.98  34.12  126.7  1124.0  0.11390  0.3094  0.3403   \n",
      "7   0.05866  ...  18.07  28.07  120.4  1021.0  0.12430  0.1793  0.2803   \n",
      "8   0.05340  ...  19.76  24.70  129.1  1228.0  0.08822  0.1963  0.2535   \n",
      "9   0.05407  ...  20.38  35.46  132.8  1284.0  0.14360  0.4122  0.5036   \n",
      "10  0.07421  ...  19.20  41.85  128.5  1153.0  0.22260  0.5209  0.4646   \n",
      "11  0.05727  ...  22.52  31.39  145.6  1590.0  0.14650  0.2275  0.3965   \n",
      "12  0.05966  ...  22.51  44.87  141.2  1408.0  0.13650  0.3735  0.3241   \n",
      "13  0.06251  ...  24.30  25.48  160.2  1809.0  0.12680  0.3135  0.4433   \n",
      "14  0.07603  ...  17.36  24.17  119.4   915.3  0.15500  0.5046  0.6872   \n",
      "15  0.07389  ...  15.49  30.73  106.2   739.3  0.17030  0.5401  0.5390   \n",
      "16  0.06330  ...  26.46  31.56  177.0  2215.0  0.18050  0.3578  0.4695   \n",
      "17  0.06540  ...  20.27  36.71  149.3  1269.0  0.16410  0.6110  0.6335   \n",
      "18  0.06739  ...  29.92  26.93  205.7  2642.0  0.13420  0.4188  0.4658   \n",
      "\n",
      "         27      28       29  \n",
      "0   0.22520  0.3590  0.07787  \n",
      "1   0.21120  0.2689  0.07055  \n",
      "2   0.16250  0.2364  0.07678  \n",
      "3   0.18410  0.2833  0.08858  \n",
      "4   0.14900  0.2341  0.07421  \n",
      "5   0.15830  0.3103  0.08200  \n",
      "6   0.14180  0.2218  0.07820  \n",
      "7   0.10990  0.1603  0.06818  \n",
      "8   0.09181  0.2369  0.06558  \n",
      "9   0.17390  0.2500  0.07944  \n",
      "10  0.20130  0.4432  0.10860  \n",
      "11  0.13790  0.3109  0.07610  \n",
      "12  0.20660  0.2853  0.08496  \n",
      "13  0.21480  0.3077  0.07569  \n",
      "14  0.21350  0.4245  0.10500  \n",
      "15  0.20600  0.4378  0.10720  \n",
      "16  0.20950  0.3613  0.09564  \n",
      "17  0.20240  0.4027  0.09876  \n",
      "18  0.24750  0.3157  0.09671  \n",
      "\n",
      "[19 rows x 30 columns]\n",
      "\n",
      "ðŸ“Š DistribuciÃ³n de clases en y_test_local:\n",
      "1    19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Š DistribuciÃ³n de clases en y_test_global:\n",
      "0    72\n",
      "1    42\n",
      "Name: count, dtype: int64\n",
      "['radiusMEAN', 'textureMEAN', 'perimeterMEAN', 'areaMEAN', 'smoothnessMEAN', 'compactnessMEAN', 'concavityMEAN', 'concave pointsMEAN', 'symmetryMEAN', 'fractaldimensionMEAN', 'radiusSE', 'textureSE', 'perimeterSE', 'areaSE', 'smoothnessSE', 'compactnessSE', 'concavitySE', 'concave pointsSE', 'symmetrySE', 'fractalDimensionSE', 'radiusWORST', 'textureWORST', 'perimeterWORST', 'areaWORST', 'smoothnessWORST', 'compactnessWORST', 'concavityWORST', 'concavePointsWORST', 'symmetryWORST', 'fractalDimensionWORST']\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train,\n",
    " X_test_local, y_test_local,\n",
    " X_test_global, y_test_global,\n",
    " dataset, feature_names, label_encoder,\n",
    " scaler, numeric_features, encoder, preprocessor) = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=3, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\nðŸ“¦ X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuciÃ³n de clases en y_train:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"\\nðŸ“¦ X_test_local (primeras filas):\")\n",
    "print(pd.DataFrame(X_test_local))\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuciÃ³n de clases en y_test_local:\")\n",
    "print(pd.Series(y_test_local).value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuciÃ³n de clases en y_test_global:\")\n",
    "print(pd.Series(y_test_global).value_counts())\n",
    "\n",
    "\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸŒ¼ CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import EnsembleDecisionTreeSurrogate, SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model, num_idx, mean, scale):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.mean = np.asarray(mean, dtype=np.float32)\n",
    "        self.scale = np.asarray(scale, dtype=np.float32)\n",
    "        self.scale_safe = np.where(self.scale == 0, 1.0, self.scale)\n",
    "\n",
    "    def _scale_internally(self, X):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        Xs = X.copy()\n",
    "        # soporta [n, d] o [d]\n",
    "        if Xs.ndim == 1:\n",
    "            Xs = Xs[None, :]\n",
    "        Xs[:, self.num_idx] = (Xs[:, self.num_idx] - self.mean) / self.scale_safe\n",
    "        return Xs\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return probs.cpu().numpy()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient, ClientUtilsMixin):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, X_test_global, y_test_global, scaler_nn_mean, scaler_nn_scale, num_idx, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.nn_model_local = copy.deepcopy(nn_model)\n",
    "        self.nn_model_global = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_global = X_test_global\n",
    "        self.y_test_global = y_test_global\n",
    "        self.scaler_nn_mean = np.asarray(scaler_nn_mean, dtype=np.float32)\n",
    "        self.scaler_nn_scale = np.where(np.asarray(scaler_nn_scale, np.float32)==0, 1.0, np.asarray(scaler_nn_scale, np.float32))\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        self.local_ckpt = f\"results/bb_local_client_{self.client_id}.pth\"\n",
    "        self.local_trained = False\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, model, epochs=10, lr=1e-3):\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # âœ… Escalado igual que en TorchNNWrapper\n",
    "        X = np.asarray(self.X_train, dtype=np.float32).copy()\n",
    "        scale_safe = np.where(self.scaler_nn_scale == 0, 1.0, self.scaler_nn_scale)\n",
    "        X[:, self.num_idx] = (X[:, self.num_idx] - self.scaler_nn_mean) / scale_safe\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_tensor)\n",
    "            loss = loss_fn(logits, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # 1ï¸âƒ£ Cargar pesos GLOBAL recibidos del servidor\n",
    "        set_model_params(\n",
    "            self.tree_model,\n",
    "            self.nn_model_global,\n",
    "            {\"tree\": [\n",
    "                self.tree_model.get_params()[\"max_depth\"],\n",
    "                self.tree_model.get_params()[\"min_samples_split\"],\n",
    "                self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "            ], \"nn\": parameters}\n",
    "        )\n",
    "\n",
    "        round_number = int(config.get(\"server_round\", 1))\n",
    "\n",
    "        # 2ï¸âƒ£ Baseline LOCAL: cargar si existe, si no entrenar 1 vez y guardar\n",
    "        if not self.local_trained:\n",
    "            if os.path.exists(self.local_ckpt):\n",
    "                state = torch.load(self.local_ckpt, map_location=\"cpu\")\n",
    "                self.nn_model_local.load_state_dict(state)\n",
    "                self.nn_model_local.eval()\n",
    "                bb_local_tmp = TorchNNWrapper(self.nn_model_local, self.num_idx, self.scaler_nn_mean, self.scaler_nn_scale)\n",
    "                with torch.no_grad():\n",
    "                    acc_train_load = accuracy_score(self.y_train_nn, bb_local_tmp.predict(self.X_train))\n",
    "                # print(f\"[CLIENTE {self.client_id}] ðŸ“¦ LOCAL TRAIN acc tras cargar ckpt:\", acc_train_load)\n",
    "                self.local_trained = True\n",
    "            else:\n",
    "                # baseline parte del global recibido (ronda 1 tÃ­picamente)\n",
    "                self.nn_model_local = copy.deepcopy(self.nn_model_global)\n",
    "                self._train_nn(self.nn_model_local, epochs=80, lr=1e-3)\n",
    "                self.nn_model_local.eval()\n",
    "                bb_local_tmp = TorchNNWrapper(self.nn_model_local, self.num_idx, self.scaler_nn_mean, self.scaler_nn_scale)\n",
    "                with torch.no_grad():\n",
    "                    acc_train_now = accuracy_score(self.y_train_nn, bb_local_tmp.predict(self.X_train))\n",
    "                # print(f\"[CLIENTE {self.client_id}] âœ… LOCAL TRAIN acc justo tras entrenar:\", acc_train_now)\n",
    "                torch.save(self.nn_model_local.state_dict(), self.local_ckpt)\n",
    "                self.local_trained = True\n",
    "                print(f\"[CLIENTE {self.client_id}] âœ… LOCAL baseline entrenado y guardado\")\n",
    "\n",
    "        # 3ï¸âƒ£ Entrenar GLOBAL (federado) en ESTA ronda con datos del cliente (FedAvg)\n",
    "        self._train_nn(self.nn_model_global, epochs=10, lr=1e-3)\n",
    "        # print(f\"[CLIENTE {self.client_id}] ðŸŒ GLOBAL entrenado (ronda {round_number})\")\n",
    "\n",
    "        # 4ï¸âƒ£ Ãrbol local (si aplica)\n",
    "        if round_number <= NUM_TRAIN_ROUNDS:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # 5ï¸âƒ£ Enviar al servidor los pesos del GLOBAL entrenado\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model_global)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        # 0) Set global params received from server (tree hyperparams + NN weights)\n",
    "        set_model_params(\n",
    "            self.tree_model,\n",
    "            self.nn_model_global,\n",
    "            {\"tree\": [\n",
    "                self.tree_model.get_params()[\"max_depth\"],\n",
    "                self.tree_model.get_params()[\"min_samples_split\"],\n",
    "                self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "            ], \"nn\": parameters}\n",
    "        )\n",
    "\n",
    "        round_number = int(config.get(\"server_round\", 1))\n",
    "        explain_only = bool(config.get(\"explain_only\", False))\n",
    "\n",
    "        # âœ… FIX CLAVE: en Ray/Flower el actor puede â€œrenacerâ€ en evaluate()\n",
    "        # y perder el nn_model_local entrenado. En la ronda final lo recargamos SIEMPRE.\n",
    "        if explain_only:\n",
    "            if not os.path.exists(self.local_ckpt):\n",
    "                raise RuntimeError(\n",
    "                    f\"[CLIENTE {self.client_id}] âŒ No existe ckpt local para explicar: {self.local_ckpt}\"\n",
    "                )\n",
    "            state = torch.load(self.local_ckpt, map_location=\"cpu\")\n",
    "            self.nn_model_local.load_state_dict(state)\n",
    "            self.nn_model_local.eval()\n",
    "            self.local_trained = True\n",
    "            print(f\"[CLIENTE {self.client_id}] ðŸ“¦ LOCAL baseline recargado en evaluate()\")\n",
    "\n",
    "        # (Opcional pero recomendable) asegurar eval mode del global al explicar\n",
    "        if explain_only:\n",
    "            self.nn_model_global.eval()\n",
    "\n",
    "        # Recibir SuperTree + mappings si vienen\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(\n",
    "                    SuperTree.SuperNode.from_dict(supertree_dict)\n",
    "                )\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] âŒ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        # ðŸ”¹ CASO 1: rondas de entrenamiento (1..NUM_TRAIN_ROUNDS)\n",
    "        if not explain_only:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            supertree = SuperTree()\n",
    "            root_node = supertree.rec_buildTree(\n",
    "                self.tree_model,\n",
    "                list(range(self.X_train.shape[1])),\n",
    "                len(self.unique_labels)\n",
    "            )\n",
    "            root_node = supertree.prune_redundant_leaves_local(root_node)\n",
    "\n",
    "            self._save_local_tree(\n",
    "                root_node,\n",
    "                round_number,\n",
    "                FEATURES,\n",
    "                self.numeric_features,\n",
    "                scaler=None,\n",
    "                unique_labels=UNIQUE_LABELS,\n",
    "                encoder=self.encoder\n",
    "            )\n",
    "            tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "            return 0.0, len(self.X_test), {\n",
    "                f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "                f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "                f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "                f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "                f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"]),\n",
    "            }\n",
    "\n",
    "        # ðŸ”¹ CASO 2: ronda final (solo explicaciÃ³n con Supertree final)\n",
    "        print(f\"[CLIENTE {self.client_id}] ðŸ” Ronda final: solo explicaciones\")\n",
    "\n",
    "        # Si quieres Ã¡rbol local para mÃ©tricas comparativas (no afecta a la NN)\n",
    "        self.tree_model.fit(self.X_train, self.y_train)\n",
    "        y_pred_tree_local = self.tree_model.predict(self.X_test)\n",
    "\n",
    "        self.local_metrics = {\n",
    "            \"acc_local_tree\": accuracy_score(self.y_test, y_pred_tree_local),\n",
    "            \"prec_local_tree\": precision_score(self.y_test, y_pred_tree_local, average=\"weighted\", zero_division=0),\n",
    "            \"rec_local_tree\": recall_score(self.y_test, y_pred_tree_local, average=\"weighted\", zero_division=0),\n",
    "            \"f1_local_tree\": f1_score(self.y_test, y_pred_tree_local, average=\"weighted\", zero_division=0),\n",
    "        }\n",
    "\n",
    "        # Usamos el SuperTree final recibido + LORE (y lo que toque)\n",
    "        if self.received_supertree is not None:\n",
    "            self.explain_all_test_instances(config)\n",
    "            # self.explain_all_test_instances(config, only_idx=0)\n",
    "\n",
    "        return 0.0, len(self.X_test), {}\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def _explain_one_instance(self, num_row, config, save_trees=False):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "        \n",
    "        self.nn_model_local.eval()\n",
    "        self.nn_model_global.eval()\n",
    "\n",
    "\n",
    "        # Wrapper que escala SOLO para la NN (espacio NN)\n",
    "        bb_local = TorchNNWrapper(\n",
    "            model=self.nn_model_local,\n",
    "            num_idx=self.num_idx,\n",
    "            mean=self.scaler_nn_mean,\n",
    "            scale=self.scaler_nn_scale\n",
    "        )\n",
    "\n",
    "        bb_global = TorchNNWrapper(\n",
    "            model=self.nn_model_global,\n",
    "            num_idx=self.num_idx,\n",
    "            mean=self.scaler_nn_mean,\n",
    "            scale=self.scaler_nn_scale\n",
    "        )\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            None,                 # <-- sin scaler (en crudo)\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # AsegÃºrate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        row = np.asarray(self.X_test[num_row], dtype=np.float32)\n",
    "\n",
    "        probs_local = bb_local.predict_proba(row[None, :])\n",
    "        pred_class_idx_local = int(probs_local.argmax(axis=1)[0])\n",
    "        pred_class_local = self.label_encoder.inverse_transform([pred_class_idx_local])[0]\n",
    "\n",
    "        probs_global = bb_global.predict_proba(row[None, :])\n",
    "        pred_class_idx_global = int(probs_global.argmax(axis=1)[0])\n",
    "        pred_class_global = self.label_encoder.inverse_transform([pred_class_idx_global])[0]\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")\n",
    "\n",
    "        # ExplicaciÃ³n LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "\n",
    "        # print(\"Instancia a explicar para el cliente:\", self.client_id)\n",
    "        # print(x_instance)\n",
    "        # print(\"ðŸ¤– NN local pred:\", pred_class_local)\n",
    "        # print(\"ðŸŒ NN global pred:\", pred_class_global)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        # =========================\n",
    "        # Vecindad GLOBAL\n",
    "        # =========================\n",
    "        bbox_global_for_Z = sklearn_classifier_bbox.sklearnBBox(bb_global)\n",
    "        lore_vecindad_global  = TabularGeneticGeneratorLore(bbox_global_for_Z, local_tabular_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        explanation_global  = lore_vecindad_global.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS,\n",
    "                                                    client_id=self.client_id, round_number=round_number)\n",
    "        \n",
    "\n",
    "        lore_tree_global = explanation_global[\"merged_tree\"]\n",
    "        Z_global = explanation_global[\"neighborhood_Z\"]\n",
    "        y_bb_global = explanation_global[\"neighborhood_Yb\"]\n",
    "        dfZ_global = pd.DataFrame(Z_global, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        \n",
    "        if save_trees:\n",
    "            self.save_lore_tree_image(lore_tree_global.root,round_number,self.feature_names,self.numeric_features,UNIQUE_LABELS,self.encoder,folder=\"lore_tree_global\")\n",
    "            \n",
    "\n",
    "        \n",
    "        # =========================\n",
    "        # Vecindad LOCAL (NUEVA)\n",
    "        # =========================\n",
    "        bbox_local = sklearn_classifier_bbox.sklearnBBox(bb_local)\n",
    "        lore_vecindad_local = TabularGeneticGeneratorLore(bbox_local, local_tabular_dataset)\n",
    "\n",
    "        explanation_local = lore_vecindad_local.explain_instance(x_instance,merge=True,num_classes=len(UNIQUE_LABELS),feature_names=self.feature_names,categorical_features=list(self.global_mapping.keys()),global_mapping=self.global_mapping,UNIQUE_LABELS=UNIQUE_LABELS,\n",
    "                                                                 client_id=self.client_id,round_number=round_number)\n",
    "\n",
    "        lore_tree_local = explanation_local[\"merged_tree\"]\n",
    "        Z_local = explanation_local[\"neighborhood_Z\"]\n",
    "        y_bb_local = explanation_local[\"neighborhood_Yb\"]\n",
    "        dfZ_local = pd.DataFrame(Z_local, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        if save_trees:\n",
    "            self.save_lore_tree_image(lore_tree_local.root,round_number,self.feature_names,self.numeric_features,UNIQUE_LABELS,self.encoder,folder=\"lore_tree_local\")\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # \n",
    "        # # MERGED TREE\n",
    "\n",
    "        # merged_tree = SuperTree()\n",
    "        # merged_tree.mergeDecisionTrees(\n",
    "        #     roots=[lore_tree.root, self.received_supertree],\n",
    "        #     num_classes=len(self.unique_labels),\n",
    "        #     feature_names=self.feature_names,\n",
    "        #     categorical_features=list(self.global_mapping.keys()), \n",
    "        #     global_mapping=self.global_mapping\n",
    "        # )\n",
    "        \n",
    "        # merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "\n",
    "        # if save_trees:\n",
    "        #     self.save_mergedTree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # tree_str = self.tree_to_str(merged_tree.root,self.feature_names,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        lore_tree_local_str = self.tree_to_str(lore_tree_local.root, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        lore_tree_global_str = self.tree_to_str(lore_tree_global.root, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        supertree_str = self.tree_to_str(self.received_supertree, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # # rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "        rules_lore_local = self.extract_rules_from_str(lore_tree_local_str, target_class_label=pred_class_local)\n",
    "        rules_lore_global = self.extract_rules_from_str(lore_tree_global_str, target_class_label=pred_class_global)\n",
    "        rules_supertree_global = self.extract_rules_from_str(supertree_str, target_class_label=pred_class_global)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            import re\n",
    "            import numpy as np\n",
    "\n",
    "            def norm_op(op: str) -> str:\n",
    "                return op.replace(\"â‰¤\", \"<=\").replace(\"â‰¥\", \">=\")\n",
    "\n",
    "            def onehot_value(var: str):\n",
    "                # busca columnas tipo \"var_*\" y devuelve el sufijo del mÃ¡ximo\n",
    "                prefix = var + \"_\"\n",
    "                cols = [k for k in instancia.keys() if k.startswith(prefix)]\n",
    "                if not cols:\n",
    "                    return None\n",
    "                best = max(cols, key=lambda c: float(instancia.get(c, 0.0)))\n",
    "                return best.split(prefix, 1)[1]\n",
    "\n",
    "            for cond in regla:\n",
    "                cond = cond.strip()\n",
    "\n",
    "                # Intervalo: 'age > 44.33 âˆ§ â‰¤ 48.50'\n",
    "                if \"âˆ§\" in cond:\n",
    "                    m = re.match(r'(.+?)([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)\\s*âˆ§\\s*([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = norm_op(m.group(2)), float(m.group(3))\n",
    "                        op2, val2 = norm_op(m.group(4)), float(m.group(5))\n",
    "                        v = float(instancia[var])\n",
    "                        if not (eval(f\"v {op1} {val1}\") and eval(f\"v {op2} {val2}\")):\n",
    "                            return False\n",
    "                        continue\n",
    "\n",
    "                # NumÃ©ricas\n",
    "                if \"â‰¤\" in cond:\n",
    "                    var, val = cond.split(\"â‰¤\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if float(instancia[var]) > val:\n",
    "                        return False\n",
    "\n",
    "                elif \">=\" in cond or \"â‰¥\" in cond:\n",
    "                    var, val = cond.replace(\"â‰¥\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if float(instancia[var]) < val:\n",
    "                        return False\n",
    "\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if float(instancia[var]) <= val:\n",
    "                        return False\n",
    "\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if float(instancia[var]) >= val:\n",
    "                        return False\n",
    "\n",
    "                # CategÃ³ricas\n",
    "                elif \"â‰ \" in cond:\n",
    "                    var, val = cond.split(\"â‰ \")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "\n",
    "                    # Caso 1: instancia decodificada (instancia[var] = \"Up\")\n",
    "                    if var in instancia and isinstance(instancia[var], str):\n",
    "                        if instancia[var] == val:\n",
    "                            return False\n",
    "                        continue\n",
    "\n",
    "                    # Caso 2: one-hot directa (col = \"STSlope_Up\")\n",
    "                    col = f\"{var}_{val}\"\n",
    "                    if col in instancia:\n",
    "                        if float(instancia[col]) >= 0.5:\n",
    "                            return False\n",
    "                        continue\n",
    "\n",
    "                    # Caso 3: one-hot por argmax (si hay varias var_*)\n",
    "                    oh = onehot_value(var)\n",
    "                    if oh is not None and oh == val:\n",
    "                        return False\n",
    "\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "\n",
    "                    # Caso 1: instancia decodificada\n",
    "                    if var in instancia and isinstance(instancia[var], str):\n",
    "                        if instancia[var] != val:\n",
    "                            return False\n",
    "                        continue\n",
    "\n",
    "                    # Caso 2: one-hot directa\n",
    "                    col = f\"{var}_{val}\"\n",
    "                    if col in instancia:\n",
    "                        if float(instancia[col]) < 0.5:\n",
    "                            return False\n",
    "                        continue\n",
    "\n",
    "                    # Caso 3: one-hot por argmax\n",
    "                    oh = onehot_value(var)\n",
    "                    if oh is None or oh != val:\n",
    "                        return False\n",
    "\n",
    "                else:\n",
    "                    # Si llega una condiciÃ³n rara, mejor fallar seguro\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        regla_factual_lore_local = None\n",
    "        for r in rules_lore_local:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_lore_local = r\n",
    "                break\n",
    "\n",
    "\n",
    "        regla_factual_lore_global = None\n",
    "        for r in rules_lore_global:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_lore_global = r\n",
    "                break\n",
    "\n",
    "        # âœ… factual = primera regla que cumple (ya lo estabas haciendo arriba)\n",
    "        rules_factual_local = [regla_factual_lore_local] if regla_factual_lore_local is not None else []\n",
    "        rules_factual_global = [regla_factual_lore_global] if regla_factual_lore_global is not None else []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # # # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "\n",
    "\n",
    "        # Extraer 1 contrafactual tipo LORE por cada clase distinta a la predicha\n",
    "        cf_rules_LORE_local_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class_local:\n",
    "                rules_clase = self.extract_rules_from_str(lore_tree_local_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    \n",
    "                    # cf_rules_LORE_local_por_clase[clase] = min(rules_clase, key=len) # Elige la mÃ¡s sencilla (menos condiciones)\n",
    "                    cf_rules_LORE_local_por_clase[clase] = rules_clase  # âœ… todos\n",
    "\n",
    "\n",
    "\n",
    "        cf_rules_LORE_global_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class_global:\n",
    "                rules_clase = self.extract_rules_from_str(lore_tree_global_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    \n",
    "                    # cf_rules_LORE_global_por_clase[clase] = min(rules_clase, key=len) # Elige la mÃ¡s sencilla (menos condiciones)\n",
    "                    cf_rules_LORE_global_por_clase[clase] = rules_clase  # âœ… todos\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"client_id:\", self.client_id)\n",
    "\n",
    "        # print(\"rules_factual_local:\", rules_factual_local)\n",
    "        # print(\"rules_factual_global:\", rules_factual_global)\n",
    "\n",
    "        # print(\"cf_rules_LORE_local_por_clase:\", cf_rules_LORE_local_por_clase)\n",
    "        # print(\"cf_rules_LORE_global_por_clase:\", cf_rules_LORE_global_por_clase)\n",
    "\n",
    "\n",
    "        # --- Factual robusto ---\n",
    "        if not rules_factual_local:\n",
    "            # intenta con el primero que encontraste con break (si lo guardaste)\n",
    "            if regla_factual_lore_local is not None:\n",
    "                rules_factual_local = [regla_factual_lore_local]\n",
    "\n",
    "        if not rules_factual_global:\n",
    "            if regla_factual_lore_global is not None:\n",
    "                rules_factual_global = [regla_factual_lore_global]\n",
    "\n",
    "\n",
    "\n",
    "        # rows = dfZ.to_dict(orient=\"records\")\n",
    "        # idx_ok = next((i for i,r in enumerate(rows) if cumple_regla(r, factual_local)), None)\n",
    "        # print(\"primer idx en Z que cumple factual_local:\", idx_ok)\n",
    "\n",
    "\n",
    "        dfZ_eval_global = dfZ_global\n",
    "        dfZ_eval_local  = dfZ_local\n",
    "\n",
    "\n",
    "        def mask_regla_en_Z(dfZ_eval, regla):\n",
    "            m = np.zeros(len(dfZ_eval), dtype=bool)\n",
    "            for i, row in enumerate(dfZ_eval.to_dict(orient=\"records\")):\n",
    "                m[i] = cumple_regla(row, regla)\n",
    "            return m\n",
    "\n",
    "        has_factual = bool(rules_factual_local) and bool(rules_factual_global)\n",
    "\n",
    "\n",
    "        if not has_factual:\n",
    "            print(f\"[CLIENTE {self.client_id}] âš ï¸ Sin factual para instancia {num_row}\")\n",
    "            jaccard_cov_global = covL_g = covG_g = covInter_g = covUnion_g = np.nan\n",
    "            jaccard_cov_local = covL_l = covG_l = covInter_l = covUnion_l = np.nan\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            ### Global\n",
    "            mL_g = mask_regla_en_Z(dfZ_eval_global, rules_factual_local[0])\n",
    "            mG_g = mask_regla_en_Z(dfZ_eval_global, rules_factual_global[0])\n",
    "\n",
    "            inter_g = np.logical_and(mL_g, mG_g).sum()\n",
    "            union_g = np.logical_or(mL_g, mG_g).sum()\n",
    "\n",
    "            jaccard_cov_global = 0.0 if union_g == 0 else inter_g / union_g\n",
    "            covL_g = mL_g.mean()\n",
    "            covG_g = mG_g.mean()\n",
    "            covInter_g = np.logical_and(mL_g, mG_g).mean()\n",
    "            covUnion_g = np.logical_or(mL_g, mG_g).mean()\n",
    "\n",
    "            ### Local\n",
    "            mL_l = mask_regla_en_Z(dfZ_eval_local, rules_factual_local[0])\n",
    "            mG_l = mask_regla_en_Z(dfZ_eval_local, rules_factual_global[0])\n",
    "\n",
    "            inter_l = np.logical_and(mL_l, mG_l).sum()\n",
    "            union_l = np.logical_or(mL_l, mG_l).sum()\n",
    "\n",
    "            jaccard_cov_local = 0.0 if union_l == 0 else inter_l / union_l\n",
    "            covL_l = mL_l.mean()\n",
    "            covG_l = mG_l.mean()\n",
    "            covInter_l = np.logical_and(mL_l, mG_l).mean()\n",
    "            covUnion_l = np.logical_or(mL_l, mG_l).mean()\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(f\"Jaccard factual_local vs factual_global: {jaccard_cov:.4f} (inter={inter}, union={union})\")\n",
    "        # print(f\"Coverage factual_local: {covL:.4f}, factual_global: {covG:.4f}, intersecciÃ³n: {covInter:.4f}, uniÃ³n: {covUnion:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================================================================================================================================================================================\n",
    "        # ðŸ“ MÃ‰TRICAS DE EXPLICABILIDAD LOCAL (vecindario Z)\n",
    "        # \n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ========================================================================================================================================================================================================\n",
    "\n",
    "        mask_same_class_local = (y_bb_local == pred_class_idx_local)\n",
    "        mask_diff_class_local = (y_bb_local != pred_class_idx_local)\n",
    "\n",
    "        Z_plus_local = dfZ_local[mask_same_class_local]\n",
    "        Z_minus_local = dfZ_local[mask_diff_class_local]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus_local).mean() if len(Z_plus_local) > 0 else 0.0\n",
    "        b = pairwise_distances([x], Z_minus_local).mean() if len(Z_minus_local) > 0 else 0.0\n",
    "\n",
    "        silhouette_local = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette_local = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        mask_same_class_global = (y_bb_global == pred_class_idx_global)\n",
    "        mask_diff_class_global = (y_bb_global != pred_class_idx_global)\n",
    "\n",
    "        Z_plus_global = dfZ_global[mask_same_class_global]\n",
    "        Z_minus_global = dfZ_global[mask_diff_class_global]\n",
    "\n",
    "        a_global = pairwise_distances([x], Z_plus_global).mean() if len(Z_plus_global) > 0 else 0.0\n",
    "        b_global = pairwise_distances([x], Z_minus_global).mean() if len(Z_minus_global) > 0 else 0.0\n",
    "\n",
    "        silhouette_global = 0.0\n",
    "        if (a_global + b_global) > 0:\n",
    "            silhouette_global = (b_global - a_global) / max(a_global, b_global)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # ==================================================================================================================\n",
    "        # MÃ‰TRICAS DE COMPARATIVA DE PRECISION DE LOS ÃRBOLES CON EL TEST (QUE TAN BUENOS SON LOS ARBOLES QUE HEMOS GENERADO)\n",
    "        # ==================================================================================================================\n",
    "\n",
    "        y_true = self.y_test\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred_supertree = self.received_supertree.predict(self.X_test)\n",
    "\n",
    "        Xg = self.X_test_global\n",
    "        if Xg.ndim == 1:\n",
    "            Xg = Xg.reshape(1, -1)\n",
    "\n",
    "        y_pred_superTree_globalTest = self.received_supertree.predict(self.X_test_global)\n",
    "        y_pred_localTree_globalTest = self.tree_model.predict(self.X_test_global)\n",
    "        \n",
    "\n",
    "        # Accuracy, precision, Recall, F1\n",
    "        acc_supertree = accuracy_score(y_true, y_pred_supertree)\n",
    "\n",
    "        prec_supertree       = precision_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "        rec_super  = recall_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "        f1_super   = f1_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "\n",
    "        acc_super_globalTest = accuracy_score(self.y_test_global, y_pred_superTree_globalTest)\n",
    "        prec_super_globalTest = precision_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "        rec_super_globalTest = recall_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "        f1_super_globalTest = f1_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "\n",
    "        acc_localTree_globalTest = accuracy_score(self.y_test_global, y_pred_localTree_globalTest)\n",
    "        prec_localTree_globalTest = precision_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "        rec_localTree_globalTest = recall_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "        f1_localTree_globalTest = f1_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ================= CSV por cliente =================\n",
    "        row = {\n",
    "            \"round\": int(round_number),\n",
    "            \"dataset\": DATASET_NAME,\n",
    "            \"client_id\": int(self.client_id),\n",
    "            \"bbox_pred_class_global\": str(pred_class_global),\n",
    "            \"bbox_pred_class_local\": str(pred_class_local),\n",
    "\n",
    "            # Vecindario\n",
    "            \"silhouette_global\": float(silhouette_global),\n",
    "            \"silhouette_local\": float(silhouette_local),\n",
    "\n",
    "            # ================= MÃ©tricas de como de buenos son los Ã¡rboles =================\n",
    "            \"acc_superTree_localTest\": float(acc_supertree),\n",
    "            \"prec_superTree_localTest\": float(prec_supertree),\n",
    "            \"rec_superTree_localTest\": float(rec_super),\n",
    "            \"f1_superTree_localTest\": float(f1_super),\n",
    "\n",
    "            \"acc_superTree_globalTest\": float(acc_super_globalTest),\n",
    "            \"prec_superTree_globalTest\": float(prec_super_globalTest),\n",
    "            \"rec_superTree_globalTest\": float(rec_super_globalTest),\n",
    "            \"f1_superTree_globalTest\": float(f1_super_globalTest),\n",
    "\n",
    "            # ðŸ”¹ mÃ©tricas LOCALES (guardadas antes)\n",
    "            \"acc_localTree_localTest\": self.local_metrics[\"acc_local_tree\"],\n",
    "            \"prec_localTree_localTest\": self.local_metrics[\"prec_local_tree\"],\n",
    "            \"rec_localTree_localTest\": self.local_metrics[\"rec_local_tree\"],\n",
    "            \"f1_localTree_localTest\": self.local_metrics[\"f1_local_tree\"],\n",
    "\n",
    "            \"acc_localTree_globalTest\": float(acc_localTree_globalTest),\n",
    "            \"prec_localTree_globalTest\": float(prec_localTree_globalTest),\n",
    "            \"rec_localTree_globalTest\": float(rec_localTree_globalTest),\n",
    "            \"f1_localTree_globalTest\": float(f1_localTree_globalTest),\n",
    "\n",
    "            # Jaccard en vecindad GLOBAL\n",
    "            \"jaccard_cov_globalZ\": float(jaccard_cov_global),\n",
    "            \"covL_globalZ\": float(covL_g),\n",
    "            \"covG_globalZ\": float(covG_g),\n",
    "            \"covInter_globalZ\": float(covInter_g),\n",
    "            \"covUnion_globalZ\": float(covUnion_g),\n",
    "\n",
    "            # Jaccard en vecindad LOCAL\n",
    "            \"jaccard_cov_localZ\": float(jaccard_cov_local),\n",
    "            \"covL_localZ\": float(covL_l),\n",
    "            \"covG_localZ\": float(covG_l),\n",
    "            \"covInter_localZ\": float(covInter_l),\n",
    "            \"covUnion_localZ\": float(covUnion_l),    \n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # (Opcional) MÃ©tricas contrafactuales por clase en columnas â€œanchasâ€\n",
    "\n",
    "        # for cl in self.unique_labels:\n",
    "        #     row[f\"cf_cov_lore_{cl}_TEST\"]  = self._to_float(coverage_cf_lore.get(cl,0))\n",
    "        #     row[f\"cf_comp_lore_{cl}_TEST\"] = int(comp_cf_lore_simpl.get(cl,0))\n",
    "        #     row[f\"cf_prec_lore_{cl}_TEST\"] = self._to_float(precision_cf_lore.get(cl,0))\n",
    "\n",
    "\n",
    "        # Guardar\n",
    "        self._append_client_csv(row, filename=\"Balanced\")\n",
    "\n",
    "        return row\n",
    "\n",
    "    # ======================================================================\n",
    "    # Bucle sobre todo el test\n",
    "    # ======================================================================\n",
    "    def explain_all_test_instances(self, config, only_idx=None):\n",
    "        results = []\n",
    "\n",
    "        # Si only_idx es None â†’ explicamos TODO el test\n",
    "        # Si only_idx es un entero â†’ explicamos solo esa instancia\n",
    "        if only_idx is None:\n",
    "            indices = range(len(self.X_test))\n",
    "            desc_text = f\"Cliente {self.client_id} explicando test completo\"\n",
    "            save_trees_flag = False      \n",
    "\n",
    "        else:\n",
    "            indices = [only_idx]\n",
    "            desc_text = f\"Cliente {self.client_id} explicando instancia {only_idx}\"\n",
    "            save_trees_flag = True\n",
    "\n",
    "\n",
    "        for i in tqdm(indices, desc=desc_text):\n",
    "            try:\n",
    "\n",
    "                row = self._explain_one_instance(i, config, save_trees=save_trees_flag)\n",
    "                results.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Cliente {self.client_id}] âš ï¸ Error en instancia {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "        full_path = f\"results/metrics_Balanced_cliente_{self.client_id}.csv\"\n",
    "        df = pd.read_csv(full_path)\n",
    "\n",
    "        mean_metrics = df.mean(numeric_only=True)\n",
    "        count_metrics = df.count(numeric_only=True)  # no-NaN\n",
    "\n",
    "        mean_df = pd.DataFrame({\"mean\": mean_metrics, \"count\": count_metrics})\n",
    "\n",
    "        # ratio: % instancias donde se pudo calcular jaccard (no NaN)\n",
    "        mean_df.loc[\"ratio_has_factual_globalZ\", [\"mean\", \"count\"]] = [\n",
    "            df[\"jaccard_cov_globalZ\"].notna().mean(),\n",
    "            int(df[\"jaccard_cov_globalZ\"].notna().sum()),\n",
    "        ]\n",
    "\n",
    "        mean_df.loc[\"ratio_has_factual_localZ\", [\"mean\", \"count\"]] = [\n",
    "            df[\"jaccard_cov_localZ\"].notna().mean(),\n",
    "            int(df[\"jaccard_cov_localZ\"].notna().sum()),\n",
    "        ]\n",
    "\n",
    "        mean_df.to_csv(\n",
    "            f\"results/metrics_cliente_{self.client_id}_balanced_mean.csv\",\n",
    "            index_label=\"metric\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "\n",
    "    (X_train, y_train,\n",
    "     X_test_local, y_test_local,\n",
    "     X_test_global, y_test_global,\n",
    "     dataset, feature_names, label_encoder,\n",
    "     scaler, numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=NUM_CLIENTS)\n",
    "    \n",
    "    tree_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "\n",
    "    num_idx = list(range(len(numeric_features)))\n",
    "\n",
    "    scaler_nn = StandardScaler().fit(X_train[:, num_idx])\n",
    "\n",
    "    # âœ… SIEMPRE MISMO NÃšMERO DE CLASES GLOBAL\n",
    "    n_clases_global = len(UNIQUE_LABELS)  # o len(label_encoder.classes_)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = n_clases_global\n",
    "\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test_local,\n",
    "                        y_test=y_test_local,\n",
    "                        X_test_global=X_test_global,\n",
    "                        y_test_global=y_test_global,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor,         \n",
    "                        scaler_nn_mean=scaler_nn.mean_,  \n",
    "                        scaler_nn_scale=scaler_nn.scale_,\n",
    "                        num_idx=num_idx).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ“¦ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# âš™ï¸ CONFIGURACIÃ“N GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinÃ¡micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ§  UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net estÃ¡ en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    sums: Dict[str, float] = {}\n",
    "    counts: Dict[str, int] = {}\n",
    "\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                sums[k] = sums.get(k, 0.0) + n * float(v)\n",
    "                counts[k] = counts.get(k, 0) + n\n",
    "\n",
    "    return {k: sums[k] / counts[k] for k in sums}\n",
    "\n",
    "# ============================\n",
    "# ðŸš€ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargÃ³ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "        # ============================\n",
    "        # ðŸ”¹ Ronda final: NO fusionar nada\n",
    "        # ============================\n",
    "        if server_round > NUM_TRAIN_ROUNDS:\n",
    "            return aggregated_metrics\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda {server_round}\")\n",
    "            from collections import defaultdict\n",
    "\n",
    "            tree_nodes = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            feature_names = None\n",
    "            numeric_features = None\n",
    "            class_names = None\n",
    "\n",
    "            # 1) recolectar mapeos categÃ³ricos y metadatos\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                # distinct_values_* para global_mapping\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"distinct_values_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        enc = json.loads(v)\n",
    "                        client_encoders[cid] = enc\n",
    "                        for feat, d in enc.items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            # 2) recolectar Ã¡rboles y demÃ¡s metadatos por cliente\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"tree_ensemble_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(v)\n",
    "\n",
    "                        # lee estos una sola vez (son iguales por cliente)\n",
    "                        if feature_names is None and f\"encoded_feature_names_{cid}\" in metrics:\n",
    "                            feature_names = json.loads(metrics[f\"encoded_feature_names_{cid}\"])\n",
    "                        if numeric_features is None and f\"numeric_features_{cid}\" in metrics:\n",
    "                            numeric_features = json.loads(metrics[f\"numeric_features_{cid}\"])\n",
    "                        if class_names is None and f\"unique_labels_{cid}\" in metrics:\n",
    "                            class_names = json.loads(metrics[f\"unique_labels_{cid}\"])\n",
    "\n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "                            tree_nodes.append(root)\n",
    "\n",
    "            if not tree_nodes:\n",
    "                return aggregated_metrics\n",
    "\n",
    "            # 3) fusionar\n",
    "            st = SuperTree()\n",
    "            st.mergeDecisionTrees(\n",
    "                roots=tree_nodes,\n",
    "                num_classes=len(class_names),\n",
    "                feature_names=feature_names,\n",
    "                categorical_features=list(global_mapping.keys()),\n",
    "                global_mapping=global_mapping,\n",
    "            )\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            st.prune_redundant_leaves_full()\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            # st.merge_equal_class_leaves()\n",
    "\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # 4) guardar/emitir\n",
    "            save_supertree_plot(\n",
    "                root_node=st.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=class_names,\n",
    "                numeric_features=numeric_features,\n",
    "                global_mapping=global_mapping,   # sin scaler\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(st.root.to_dict())\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] âŒ Error en SuperTree: {e}\")\n",
    "\n",
    "        return aggregated_metrics\n",
    "\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# ðŸ§© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "\n",
    "            # Siempre mandamos el Ãºltimo SuperTree disponible\n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "\n",
    "            # Ronda final: modo solo explicaciÃ³n\n",
    "            if server_round == NUM_SERVER_ROUNDS:\n",
    "                ins.config[\"explain_only\"] = True\n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUMÃ‰RICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real  = left\n",
    "            right_real = right\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} â‰¤ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} âˆˆ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features,\n",
    "                scaler=None,  # ya no se usa\n",
    "                global_mapping=global_mapping, depth=depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACIÃ“N] Error interpretando categÃ³rica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres numÃ©ricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACIÃ“N] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACIÃ“N] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\",\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0]); node_id[0] += 1\n",
    "\n",
    "        # etiqueta\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            label = f\"class: {class_names[class_index]}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            label = fname.split(\"_\", 1)[0] if \"_\" in fname else fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent: dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # OneHot\n",
    "            if \"_\" in fname:\n",
    "                _, val = fname.split(\"_\", 1)\n",
    "                add_node(node.children[0], curr, f'â‰  \"{val.strip()}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val.strip()}\"')\n",
    "            # NumÃ©rica\n",
    "            elif fname in numeric_features:\n",
    "                thr = node.intervals[0] if node.intervals else node.thresh\n",
    "                add_node(node.children[0], curr, f\"â‰¤ {thr:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thr:.2f}\")\n",
    "            # CategÃ³rica ordinal\n",
    "            elif fname in global_mapping:\n",
    "                vals = global_mapping[fname]\n",
    "                val = vals[node.intervals[0]] if node.intervals else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'â‰  \"{val}\"')\n",
    "            else:\n",
    "                for ch in node.children:\n",
    "                    add_node(ch, curr, \"?\")\n",
    "\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"pdf\", cleanup=True)\n",
    "    return f\"{filename}.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ”§ INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 10:10:58,518\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2026-02-17 10:11:02,245 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 6 clients (out of 6)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 6 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] âœ… LOCAL baseline entrenado y guardado[CLIENTE 3] âœ… LOCAL baseline entrenado y guardado\n",
      "\n",
      "[CLIENTE 4] âœ… LOCAL baseline entrenado y guardado\n",
      "[CLIENTE 2] âœ… LOCAL baseline entrenado y guardado\n",
      "[CLIENTE 6] âœ… LOCAL baseline entrenado y guardado\n",
      "[CLIENTE 5] âœ… LOCAL baseline entrenado y guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 6 clients (out of 6)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 6 clients (out of 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 1] ðŸ” Ronda final: solo explicaciones\n",
      "[CLIENTE 5] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 5] ðŸ” Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cliente 1 explicando test completo:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 2] ðŸ” Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 4] ðŸ” Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 3] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 3] ðŸ” Ronda final: solo explicaciones\n",
      "[CLIENTE 6] ðŸ“¦ LOCAL baseline recargado en evaluate()\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 6] ðŸ” Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:   3%|â–Ž         | 1/29 [00:31<14:32, 31.16s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 5] âš ï¸ Sin factual para instancia 1 (local=1, global=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Cliente 1 explicando test completo:   7%|â–‹         | 2/29 [01:07<15:27, 34.37s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  10%|â–ˆ         | 3/29 [01:39<14:25, 33.29s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  14%|â–ˆâ–        | 4/29 [02:13<13:56, 33.47s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  17%|â–ˆâ–‹        | 5/29 [02:47<13:26, 33.61s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  21%|â–ˆâ–ˆ        | 6/29 [03:23<13:09, 34.34s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  24%|â–ˆâ–ˆâ–       | 7/29 [03:56<12:30, 34.09s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  28%|â–ˆâ–ˆâ–Š       | 8/29 [04:32<12:08, 34.67s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [05:06<11:28, 34.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [05:38<10:42, 33.82s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 6] âš ï¸ Sin factual para instancia 10 (local=1, global=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [06:15<10:23, 34.64s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [06:49<09:44, 34.40s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Cliente 1 explicando test completo:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [07:24<09:12, 34.54s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [08:03<08:58, 35.89s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [08:53<06:16, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] âš ï¸ Sin factual para instancia 15 (local=1, global=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Cliente 1 explicando test completo:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [09:35<06:34, 32.86s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Cliente 2 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [09:42<00:00, 34.25s/it]\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [10:09<06:06, 33.29s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Cliente 3 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [10:32<00:00, 37.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 5] âš ï¸ Sin factual para instancia 18 (local=1, global=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [10:46<05:44, 34.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Cliente 4 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [11:10<00:00, 35.28s/it]\n",
      "Cliente 1 explicando test completo:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [11:10<04:41, 31.30s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 5] âš ï¸ Sin factual para instancia 21 (local=1, global=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cliente 1 explicando test completo:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [11:32<03:48, 28.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [12:09<02:20, 23.40s/it]\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [12:27<01:48, 21.69s/it]\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [12:45<01:22, 20.64s/it]\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [13:04<01:00, 20.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Cliente 1 explicando test completo:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [13:23<00:39, 19.85s/it]\n",
      "\n",
      "\n",
      "\n",
      "Cliente 6 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [13:40<00:00, 29.29s/it]\n",
      "Cliente 1 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [13:52<00:00, 28.69s/it]\n",
      "Cliente 5 explicando test completo: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [13:52<00:00, 27.74s/it]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 6 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 851.47s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar aÃºn mÃ¡s\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesiÃ³n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ab03c",
   "metadata": {},
   "source": [
    "### BALANCED METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a663325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voy a usar estos ficheros:\n",
      "  - metrics_cliente_1_balanced_mean.csv\n",
      "  - metrics_cliente_2_balanced_mean.csv\n",
      "  - metrics_cliente_3_balanced_mean.csv\n",
      "  - metrics_cliente_4_balanced_mean.csv\n",
      "  - metrics_cliente_5_balanced_mean.csv\n",
      "  - metrics_cliente_6_balanced_mean.csv\n",
      "\n",
      "âœ… Promedios globales (ponderados por count) guardados en: experimentos_Overlapping_classes\\breastcancer\\6_Clients_Mean_global.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ðŸ“Š Promedio global a partir de los *balanced_mean*\n",
    "# - Lee results/metrics_cliente_*_balanced_mean.csv con columnas: metric, mean, count\n",
    "# - Calcula media GLOBAL ponderada por count (no macro-media)\n",
    "# - Colapsa mÃ©tricas por clase (TEST y Z) tambiÃ©n ponderando por count\n",
    "# - Guarda en: experimentos_FeatureSkew/<DATASET>/<SKEW_FEATS>/{NCLIENTS}_Clients_Mean_global.csv\n",
    "# ==========================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------\n",
    "# âœ… AJUSTA ESTO en tu notebook/entorno:\n",
    "# - DATASET_NAME (ya lo tienes)\n",
    "# - SKEW_FEATURE (str o lista de str, ej. [\"Age\",\"Sex\"])\n",
    "# -------------------------------------------------\n",
    "\n",
    "def _slug_feat(name: str) -> str:\n",
    "    \"\"\"Normaliza nombres para usarlos como carpeta.\"\"\"\n",
    "    name = str(name).strip().replace(\" \", \"_\")\n",
    "    name = re.sub(r\"[^A-Za-z0-9_\\-]\", \"\", name)\n",
    "    return name\n",
    "\n",
    "def _skew_tag(skew_feature) -> str:\n",
    "    \"\"\"Convierte skew_feature (str/list/tuple/set) en 'Age_Sex'.\"\"\"\n",
    "    if isinstance(skew_feature, (list, tuple, set)):\n",
    "        parts = [_slug_feat(x) for x in skew_feature]\n",
    "    else:\n",
    "        parts = [_slug_feat(skew_feature)]\n",
    "    parts = [p for p in parts if p]\n",
    "    return \"_\".join(parts) if parts else \"UnknownSkew\"\n",
    "\n",
    "def _weighted_mean(subdf: pd.DataFrame, mean_col: str = \"mean\", count_col: str = \"count\"):\n",
    "    \"\"\"Media ponderada ignorando NaNs.\"\"\"\n",
    "    m = subdf[mean_col]\n",
    "    c = subdf[count_col]\n",
    "    mask = m.notna() & c.notna() & (c > 0)\n",
    "    if mask.sum() == 0:\n",
    "        return float(\"nan\")\n",
    "    return float((m[mask] * c[mask]).sum() / c[mask].sum())\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ðŸ“¥ Cargar ficheros balanced_mean\n",
    "# -------------------------------------------------\n",
    "csv_dir = Path(\"results\")\n",
    "files = sorted(csv_dir.glob(\"metrics_cliente_*_balanced_mean.csv\"))\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No encuentro ficheros metrics_cliente_*_balanced_mean.csv en results/\")\n",
    "\n",
    "print(\"Voy a usar estos ficheros:\")\n",
    "for f in files:\n",
    "    print(\"  -\", f.name)\n",
    "\n",
    "n_clients = len(files)\n",
    "DATASET_NAME = DATASET_NAME.split(\"/\")[-1]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ðŸ“ Carpeta destino\n",
    "# -------------------------------------------------\n",
    "out_dir = Path(\"experimentos_Overlapping_classes\") / DATASET_NAME\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ðŸ“„ Leer cada fichero y acumular en formato largo:\n",
    "# metric | mean | count | client_file\n",
    "# -------------------------------------------------\n",
    "dfs_long = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # Normaliza nombres por si vinieran raros\n",
    "    df = df.rename(columns={\n",
    "        df.columns[0]: \"metric\",\n",
    "        df.columns[1]: \"mean\",\n",
    "    })\n",
    "    if len(df.columns) >= 3:\n",
    "        df = df.rename(columns={df.columns[2]: \"count\"})\n",
    "    else:\n",
    "        # Si no hay count, asumimos 1 (macro-media)\n",
    "        df[\"count\"] = 1.0\n",
    "\n",
    "    df[\"client_file\"] = f.stem\n",
    "    dfs_long.append(df[[\"metric\", \"mean\", \"count\", \"client_file\"]])\n",
    "\n",
    "all_long = pd.concat(dfs_long, ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# âœ… Media global ponderada por count, por mÃ©trica\n",
    "# -------------------------------------------------\n",
    "means_df = (\n",
    "    all_long\n",
    "    .groupby(\"metric\", as_index=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"mean\": _weighted_mean(g, \"mean\", \"count\"),\n",
    "        \"count\": float(g[\"count\"].dropna().sum())\n",
    "    }))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# ðŸ“‰ Colapsar clases (TEST y Z) (ponderado)\n",
    "# - crea nuevas mÃ©tricas \"colapsadas\"\n",
    "# - luego elimina las mÃ©tricas por-clase originales\n",
    "# ==========================================\n",
    "collapse_patterns = {\n",
    "    # --------- TEST ----------\n",
    "    \"cf_cov_merged_TEST\":   r\"^cf_cov_merged_[^_]+_TEST$\",\n",
    "    \"cf_comp_merged_TEST\":  r\"^cf_comp_merged_[^_]+_TEST$\",\n",
    "    \"cf_prec_merged_TEST\":  r\"^cf_prec_merged_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_lore_TEST\":     r\"^cf_cov_lore_[^_]+_TEST$\",\n",
    "    \"cf_comp_lore_TEST\":    r\"^cf_comp_lore_[^_]+_TEST$\",\n",
    "    \"cf_prec_lore_TEST\":    r\"^cf_prec_lore_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_super_TEST\":    r\"^cf_cov_super_[^_]+_TEST$\",\n",
    "    \"cf_comp_super_TEST\":   r\"^cf_comp_super_[^_]+_TEST$\",\n",
    "    \"cf_prec_super_TEST\":   r\"^cf_prec_super_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_local_local_TEST\":  r\"^cf_cov_local_local_[^_]+_TEST$\",\n",
    "    \"cf_comp_local_local_TEST\": r\"^cf_comp_local_local_[^_]+_TEST$\",\n",
    "    \"cf_prec_local_local_TEST\": r\"^cf_prec_local_local_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_local_super_TEST\":  r\"^cf_cov_local_super_[^_]+_TEST$\",\n",
    "    \"cf_comp_local_super_TEST\": r\"^cf_comp_local_super_[^_]+_TEST$\",\n",
    "    \"cf_prec_local_super_TEST\": r\"^cf_prec_local_super_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_localZ_TEST\":   r\"^cf_cov_localZ_[^_]+_TEST$\",\n",
    "    \"cf_comp_localZ_TEST\":  r\"^cf_comp_localZ_[^_]+_TEST$\",\n",
    "    \"cf_prec_localZ_TEST\":  r\"^cf_prec_localZ_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_superZ_TEST\":   r\"^cf_cov_superZ_[^_]+_TEST$\",\n",
    "    \"cf_comp_superZ_TEST\":  r\"^cf_comp_superZ_[^_]+_TEST$\",\n",
    "    \"cf_prec_superZ_TEST\":  r\"^cf_prec_superZ_[^_]+_TEST$\",\n",
    "\n",
    "    # --------- Z ----------\n",
    "    \"cf_cov_merged_Z\":   r\"^cf_cov_merged_[^_]+_Z$\",\n",
    "    \"cf_prec_merged_Z\":  r\"^cf_prec_merged_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_lore_Z\":     r\"^cf_cov_lore_[^_]+_Z$\",\n",
    "    \"cf_prec_lore_Z\":    r\"^cf_prec_lore_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_super_Z\":    r\"^cf_cov_super_[^_]+_Z$\",\n",
    "    \"cf_prec_super_Z\":   r\"^cf_prec_super_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_local_local_Z\":  r\"^cf_cov_local_local_[^_]+_Z$\",\n",
    "    \"cf_prec_local_local_Z\": r\"^cf_prec_local_local_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_local_super_Z\":  r\"^cf_cov_local_super_[^_]+_Z$\",\n",
    "    \"cf_prec_local_super_Z\": r\"^cf_prec_local_super_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_localZ_Z\":   r\"^cf_cov_localZ_[^_]+_Z$\",\n",
    "    \"cf_prec_localZ_Z\":  r\"^cf_prec_localZ_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_superZ_Z\":   r\"^cf_cov_superZ_[^_]+_Z$\",\n",
    "    \"cf_prec_superZ_Z\":  r\"^cf_prec_superZ_[^_]+_Z$\",\n",
    "}\n",
    "\n",
    "rows_new = []\n",
    "for new_name, pattern in collapse_patterns.items():\n",
    "    sub = means_df[means_df[\"metric\"].str.match(pattern)]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    rows_new.append({\n",
    "        \"metric\": new_name,\n",
    "        \"mean\": _weighted_mean(sub, \"mean\", \"count\"),\n",
    "        \"count\": float(sub[\"count\"].dropna().sum())\n",
    "    })\n",
    "\n",
    "if rows_new:\n",
    "    means_df = pd.concat([means_df, pd.DataFrame(rows_new)], ignore_index=True)\n",
    "\n",
    "# Elimina las mÃ©tricas por-clase originales (las que acabas de colapsar)\n",
    "pattern_drop = (\n",
    "    r\"^cf_(cov|prec|comp)_\"\n",
    "    r\"(merged|lore|super|local_local|local_super|localZ|superZ)_\"\n",
    "    r\"[^_]+_(TEST|Z)$\"\n",
    ")\n",
    "means_df = means_df[~means_df[\"metric\"].str.match(pattern_drop)].reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ðŸ’¾ Guardar resultado final (solo metric,mean)\n",
    "# -------------------------------------------------\n",
    "out_path = out_dir / f\"{n_clients}_Clients_Mean_global.csv\"\n",
    "means_df[[\"metric\", \"mean\"]].to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nâœ… Promedios globales (ponderados por count) guardados en: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
