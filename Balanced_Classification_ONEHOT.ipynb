{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68391f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:32:05,655\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-12-23 10:32:09,784 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.piping.pipe(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-12-23 10:32:09,794 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.rendering.render(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-12-23 10:32:09,796 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.unflattening.unflatten(['stagger', 'fanout', 'chain', 'encoding'])\n",
      "2025-12-23 10:32:09,799 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.viewing.view(['quiet'])\n",
      "2025-12-23 10:32:09,807 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.quote(['is_html_string', 'is_valid_id', 'dot_keywords', 'endswith_odd_number_of_backslashes', 'escape_unescaped_quotes'])\n",
      "2025-12-23 10:32:09,808 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.a_list(['kwargs', 'attributes'])\n",
      "2025-12-23 10:32:09,808 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.attr_list(['kwargs', 'attributes'])\n",
      "2025-12-23 10:32:09,809 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.clear(['keep_attrs'])\n",
      "2025-12-23 10:32:09,809 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.__iter__(['subgraph'])\n",
      "2025-12-23 10:32:09,810 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.node(['_attributes'])\n",
      "2025-12-23 10:32:09,810 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.edge(['_attributes'])\n",
      "2025-12-23 10:32:09,811 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.attr(['_attributes'])\n",
      "2025-12-23 10:32:09,811 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.subgraph(['name', 'comment', 'graph_attr', 'node_attr', 'edge_attr', 'body'])\n",
      "2025-12-23 10:32:09,814 graphviz._tools DEBUG    deprecate positional args: graphviz.piping.Pipe._pipe_legacy(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-12-23 10:32:09,819 graphviz._tools DEBUG    deprecate positional args: graphviz.saving.Save.save(['directory'])\n",
      "2025-12-23 10:32:09,820 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.render(['directory', 'view', 'cleanup', 'format', 'renderer', 'formatter', 'neato_no_op', 'quiet', 'quiet_view'])\n",
      "2025-12-23 10:32:09,821 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.view(['directory', 'cleanup', 'quiet', 'quiet_view'])\n",
      "2025-12-23 10:32:09,823 graphviz._tools DEBUG    deprecate positional args: graphviz.unflattening.Unflatten.unflatten(['stagger', 'fanout', 'chain'])\n",
      "2025-12-23 10:32:09,823 graphviz._tools DEBUG    deprecate positional args: graphviz.graphs.BaseGraph.__init__(['comment', 'filename', 'directory', 'format', 'engine', 'encoding', 'graph_attr', 'node_attr', 'edge_attr', 'body', 'strict'])\n",
      "2025-12-23 10:32:09,825 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.from_file(['directory', 'format', 'engine', 'encoding', 'renderer', 'formatter'])\n",
      "2025-12-23 10:32:09,826 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.__init__(['filename', 'directory', 'format', 'engine', 'encoding'])\n",
      "2025-12-23 10:32:09,827 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.save(['directory'])\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# üì¶ IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "from lore_sa.client_utils import ClientUtilsMixin\n",
    "\n",
    "# Otros\n",
    "from pathlib import Path\n",
    "from filelock import FileLock  # pip install filelock\n",
    "import pandas as pd, os\n",
    "from graphviz import Digraph\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import cProfile, pstats, io\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ‚öôÔ∏è VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "\n",
    "NUM_TRAIN_ROUNDS = 2        # rondas donde entrenas la NN\n",
    "NUM_SERVER_ROUNDS = 3       # la √∫ltima solo para explicaciones\n",
    "NUM_CLIENTS = 2\n",
    "SEED = 42\n",
    "\n",
    "NON_IID = False   # o False para los experimentos IID\n",
    "NON_IID_ALPHA = 0.5  # por ejemplo, Dirichlet m√°s sesgado\n",
    "\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "GLOBAL_TEST_IDX = None\n",
    "GLOBAL_TEST_HASHES = None\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# üßπ Borrar TODOS los CSV individuales de clientes\n",
    "# ==============================================\n",
    "\n",
    "csv_dir = Path(\"results\")\n",
    "all_csvs = list(csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Solo borrar si hay alguno\n",
    "if all_csvs:\n",
    "    for f in all_csvs:\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass  # Ignora errores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# üîß UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# üì• PREPROCESADO DATASET\n",
    "# =======================\n",
    "def preprocess_df(df: pd.DataFrame, dataset_name: str, class_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"adult\" in dataset_name.lower():\n",
    "        df.drop(columns=['fnlwgt', 'education-num', 'capital-gain', 'capital-loss'],\n",
    "                inplace=True, errors=\"ignore\")\n",
    "\n",
    "    elif \"churn\" in dataset_name.lower():\n",
    "        df.drop(columns=['customerID', 'TotalCharges'],\n",
    "                inplace=True, errors=\"ignore\")\n",
    "        if \"MonthlyCharges\" in df.columns:\n",
    "            df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        if \"tenure\" in df.columns:\n",
    "            df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        if \"SeniorCitizen\" in df.columns:\n",
    "            df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=[c for c in [\"MonthlyCharges\", \"tenure\"] if c in df.columns], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in dataset_name.lower():\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    # object -> category (solo baja cardinalidad)\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if col != class_col and df[col].nunique(dropna=True) < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _stable_row_hash(df: pd.DataFrame) -> np.ndarray:\n",
    "    return pd.util.hash_pandas_object(df, index=False).astype(\"uint64\").to_numpy()\n",
    "\n",
    "# =======================\n",
    "# üì• CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name: str, class_col: str):\n",
    "    \"\"\"\n",
    "    Lee TODO el pool (train con num_partitions=1) para fijar:\n",
    "    - cat_features (categorical cols)\n",
    "    - num_features\n",
    "    - categories_global (OHE categories_ en el orden de cat_features)\n",
    "    - onehot_columns (nombres finales onehot)\n",
    "    \"\"\"\n",
    "    fds_tmp = FederatedDataset(\n",
    "        dataset=flower_dataset_name,\n",
    "        partitioners={\"train\": IidPartitioner(num_partitions=1)}\n",
    "    )\n",
    "    df_all = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "    df_all = preprocess_df(df_all, flower_dataset_name, class_col)\n",
    "\n",
    "    # asegurar category dtype\n",
    "    for col in df_all.select_dtypes(include=[\"object\"]).columns:\n",
    "        if col != class_col and df_all[col].nunique(dropna=True) < 50:\n",
    "            df_all[col] = df_all[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [c for c in df_all.columns if df_all[c].dtype.name == \"category\" and c != class_col]\n",
    "    num_features = [c for c in df_all.columns if df_all[c].dtype.kind in \"fi\" and c != class_col]\n",
    "\n",
    "    if len(cat_features) > 0:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        ohe.fit(df_all[cat_features])\n",
    "        categories_global = ohe.categories_\n",
    "        onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    else:\n",
    "        categories_global = []\n",
    "        onehot_columns = []\n",
    "\n",
    "    return cat_features, num_features, categories_global, onehot_columns, df_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üì• CARGA GENERAL + TEST GLOBAL SIN FUGA\n",
    "# ============================================\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      X_train, y_train,\n",
    "      X_test_local, y_test_local,\n",
    "      X_test_global, y_test_global,\n",
    "      tabular_dataset, feature_names_out, label_encoder,\n",
    "      num_transformer, numeric_features,\n",
    "      encoder (ColumnTransformerEnc), preprocessor\n",
    "    \"\"\"\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "    global GLOBAL_TEST_IDX, GLOBAL_TEST_HASHES\n",
    "\n",
    "    # 1) Info global OHE + df_all pool\n",
    "    cat_features, num_features, categories_global, onehot_columns, df_all = get_global_onehot_info(\n",
    "        flower_dataset_name, class_col\n",
    "    )\n",
    "\n",
    "    # 2) LabelEncoder global (clases estables)\n",
    "    if not UNIQUE_LABELS:\n",
    "        le_global = LabelEncoder()\n",
    "        le_global.fit(df_all[class_col])\n",
    "        UNIQUE_LABELS[:] = le_global.classes_.tolist()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "\n",
    "    y_all = label_encoder.transform(df_all[class_col])\n",
    "\n",
    "    # 3) Definir TEST GLOBAL una sola vez (idx + hashes estables)\n",
    "    if GLOBAL_TEST_IDX is None:\n",
    "        idx = np.arange(len(df_all))\n",
    "        _, GLOBAL_TEST_IDX = train_test_split(\n",
    "            idx,\n",
    "            test_size=0.2,\n",
    "            random_state=SEED,\n",
    "            stratify=y_all if len(np.unique(y_all)) > 1 else None\n",
    "        )\n",
    "        row_hash_all = _stable_row_hash(df_all)\n",
    "        GLOBAL_TEST_HASHES = set(row_hash_all[GLOBAL_TEST_IDX].tolist())\n",
    "\n",
    "    # 4) Crear/usar FederatedDataset particionado (por filas)\n",
    "    if fds is None:\n",
    "        if NON_IID:\n",
    "            partitioner = DirichletPartitioner(\n",
    "                num_partitions=num_partitions,\n",
    "                alpha=NON_IID_ALPHA,\n",
    "                partition_by=class_col,\n",
    "            )\n",
    "        else:\n",
    "            partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "\n",
    "        fds = FederatedDataset(\n",
    "            dataset=flower_dataset_name,\n",
    "            partitioners={\"train\": partitioner},\n",
    "        )\n",
    "\n",
    "    df_client = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "    df_client = preprocess_df(df_client, flower_dataset_name, class_col)\n",
    "\n",
    "    # 5) Eliminar filas que est√©n en el TEST GLOBAL (sin fuga)\n",
    "    row_hash_client = _stable_row_hash(df_client)\n",
    "    keep_mask = ~np.isin(row_hash_client, np.fromiter(GLOBAL_TEST_HASHES, dtype=\"uint64\"))\n",
    "    df_client = df_client.loc[keep_mask].copy()\n",
    "\n",
    "    # 6) TabularDataset/descriptor (cliente) para LORE\n",
    "    tabular_dataset = TabularDataset(df_client.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    # Asegurar distinct_values local (por si viene vac√≠o)\n",
    "    for col, info in descriptor.get(\"categorical\", {}).items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(df_client[col].dropna().unique())\n",
    "\n",
    "    # 7) X/y cliente (sin onehot a√∫n)\n",
    "    y = label_encoder.transform(df_client[class_col])\n",
    "    X_raw = df_client.drop(columns=[class_col])\n",
    "\n",
    "    numeric_features = list(descriptor.get(\"numeric\", {}).keys())\n",
    "    categorical_features = list(descriptor.get(\"categorical\", {}).keys())\n",
    "\n",
    "    # Ojo: aqu√≠ mantenemos el mismo orden que usa el descriptor del cliente\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    num_idx = list(range(len(numeric_features)))\n",
    "    cat_idx = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    # 8) Preprocessor con categor√≠as globales (dim estable)\n",
    "    transformers = [(\"num\", \"passthrough\", num_idx)]\n",
    "    if len(categorical_features) > 0:\n",
    "        # IMPORTANT√çSIMO: categories_global est√° en el orden de cat_features (global)\n",
    "        # pero aqu√≠ categorical_features puede venir en otro orden -> reordenamos categories_global\n",
    "        cat_to_pos = {c: i for i, c in enumerate(cat_features)}\n",
    "        cats_ordered = [categories_global[cat_to_pos[c]] for c in categorical_features]\n",
    "\n",
    "        transformers.append((\n",
    "            \"cat\",\n",
    "            OneHotEncoder(\n",
    "                sparse_output=False,\n",
    "                handle_unknown=\"ignore\",\n",
    "                categories=cats_ordered\n",
    "            ),\n",
    "            cat_idx\n",
    "        ))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "\n",
    "    # 9) Split local + FIT SOLO con train local\n",
    "    X_train_raw, X_test_local_raw, y_train, y_test_local = train_test_split(\n",
    "        X_raw[FEATURES], y,\n",
    "        test_size=0.3,\n",
    "        random_state=SEED,\n",
    "        stratify=y if len(np.unique(y)) > 1 else None\n",
    "    )\n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train_raw.to_numpy())\n",
    "    X_test_local = preprocessor.transform(X_test_local_raw.to_numpy())\n",
    "\n",
    "    # 10) Construir test global REAL (mismo preprocessor del cliente)\n",
    "    df_global = df_all.iloc[GLOBAL_TEST_IDX].copy()\n",
    "    df_global = preprocess_df(df_global, flower_dataset_name, class_col)\n",
    "\n",
    "    X_test_global = preprocessor.transform(df_global.drop(columns=[class_col])[FEATURES].to_numpy())\n",
    "    y_test_global = label_encoder.transform(df_global[class_col])\n",
    "\n",
    "    # 11) Feature names finales (num + onehot) para NN/servidor\n",
    "    feature_names_out = []\n",
    "    feature_names_out += list(numeric_features)\n",
    "    if len(categorical_features) > 0:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features).tolist()\n",
    "        feature_names_out += cat_names\n",
    "\n",
    "    FEATURES[:] = feature_names_out  # ahora s√≠: columnas finales (onehot)\n",
    "\n",
    "    # 12) Encoder LORE con distinct_values globales (para reglas legibles)\n",
    "    descriptor_global = descriptor.copy()\n",
    "    if \"categorical\" in descriptor_global and len(categorical_features) > 0:\n",
    "        # cats_ordered ya est√° en orden de categorical_features\n",
    "        for i, col in enumerate(categorical_features):\n",
    "            if col in descriptor_global[\"categorical\"]:\n",
    "                descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(cats_ordered[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    num_transformer = preprocessor.named_transformers_[\"num\"] if \"num\" in preprocessor.named_transformers_ else None\n",
    "\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test_local, y_test_local,\n",
    "        X_test_global, y_test_global,\n",
    "        tabular_dataset, feature_names_out, label_encoder,\n",
    "        num_transformer, numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ‚úÖ DATASET\n",
    "# =======================\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "# CLASS_COLUMN = \"HeartDisease\"\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "CLASS_COLUMN = \"diagnosis\"\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "# CLASS_COLUMN = \"Outcome\"\n",
    "\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c49bf6",
   "metadata": {},
   "source": [
    "### HOLDOUT DEL SERVIDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:32:09,905 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-12-23 10:32:10,380 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-12-23 10:32:10,507 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 616\n",
      "2025-12-23 10:32:10,647 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2025-12-23 10:32:10,663 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-12-23 10:32:11,047 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2025-12-23 10:32:11,163 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2025-12-23 10:32:11,295 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 616\n",
      "2025-12-23 10:32:11,414 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-12-23 10:32:11,414 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-12-23 10:32:11,633 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2025-12-23 10:32:11,767 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 616\n",
      "2025-12-23 10:32:11,999 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e?recursive=False&expand=False HTTP/11\" 200 207\n",
      "2025-12-23 10:32:12,132 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-12-23 10:32:12,249 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-12-23 10:32:12,416 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 616\n",
      "2025-12-23 10:32:12,750 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-12-23 10:32:12,750 filelock     DEBUG    Attempting to acquire lock 2422543521344 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:12,750 filelock     DEBUG    Lock 2422543521344 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:12,750 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-12-23 10:32:12,750 filelock     DEBUG    Attempting to release lock 2422543521344 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:12,766 filelock     DEBUG    Lock 2422543521344 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:12,783 filelock     DEBUG    Attempting to acquire lock 2422540069408 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:12,797 filelock     DEBUG    Lock 2422540069408 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:12,799 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-12-23 10:32:12,800 filelock     DEBUG    Attempting to release lock 2422540069408 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:12,800 filelock     DEBUG    Lock 2422540069408 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:13,016 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-12-23 10:32:13,234 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 616\n",
      "2025-12-23 10:32:13,367 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2025-12-23 10:32:13,491 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2025-12-23 10:32:13,601 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2025-12-23 10:32:13,918 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-12-23 10:32:14,101 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2025-12-23 10:32:14,235 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-12-23 10:32:14,341 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-12-23 10:32:14,519 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 616\n",
      "2025-12-23 10:32:14,970 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-12-23 10:32:14,970 filelock     DEBUG    Attempting to acquire lock 2422413822480 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:14,970 filelock     DEBUG    Lock 2422413822480 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:14,970 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-12-23 10:32:14,985 filelock     DEBUG    Attempting to release lock 2422413822480 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:14,987 filelock     DEBUG    Lock 2422413822480 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-12-23 10:32:14,989 filelock     DEBUG    Attempting to acquire lock 2422552223552 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:14,990 filelock     DEBUG    Lock 2422552223552 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:14,990 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-12-23 10:32:14,992 filelock     DEBUG    Attempting to release lock 2422552223552 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-12-23 10:32:14,993 filelock     DEBUG    Lock 2422552223552 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ TRAIN (primeras filas):\n",
      "         0      1       2      3        4        5        6        7       8   \\\n",
      "0    16.240  18.77  108.80  805.1  0.10660  0.18020  0.19480  0.09052  0.1876   \n",
      "1    11.270  15.50   73.38  392.0  0.08365  0.11140  0.10070  0.02757  0.1810   \n",
      "2     9.029  17.33   58.79  250.5  0.10660  0.14130  0.31300  0.04375  0.2111   \n",
      "3    15.460  11.89  102.50  736.9  0.12570  0.15550  0.20320  0.10970  0.1966   \n",
      "4    12.230  19.56   78.54  461.0  0.09586  0.08087  0.04187  0.04107  0.1979   \n",
      "..      ...    ...     ...    ...      ...      ...      ...      ...     ...   \n",
      "156  14.580  13.66   94.29  658.8  0.09832  0.08918  0.08222  0.04349  0.1739   \n",
      "157  17.300  17.08  113.00  928.2  0.10080  0.10410  0.12660  0.08353  0.1813   \n",
      "158  14.530  19.34   94.25  659.7  0.08388  0.07800  0.08817  0.02925  0.1473   \n",
      "159  14.680  20.13   94.74  684.5  0.09867  0.07200  0.07395  0.05259  0.1586   \n",
      "160  15.500  21.08  102.90  803.1  0.11200  0.15710  0.15220  0.08481  0.2085   \n",
      "\n",
      "          9   ...     20     21      22      23      24      25      26  \\\n",
      "0    0.06684  ...  18.55  25.09  126.90  1031.0  0.1365  0.4706  0.5026   \n",
      "1    0.07252  ...  12.04  18.93   79.73   450.0  0.1102  0.2809  0.3021   \n",
      "2    0.08046  ...  10.31  22.65   65.50   324.7  0.1482  0.4365  1.2520   \n",
      "3    0.07069  ...  18.79  17.04  125.00  1102.0  0.1531  0.3583  0.5830   \n",
      "4    0.06013  ...  14.44  28.36   92.15   638.4  0.1429  0.2042  0.1377   \n",
      "..       ...  ...    ...    ...     ...     ...     ...     ...     ...   \n",
      "156  0.05640  ...  16.76  17.24  108.50   862.0  0.1223  0.1928  0.2492   \n",
      "157  0.05613  ...  19.85  25.09  130.90  1222.0  0.1416  0.2405  0.3378   \n",
      "158  0.05746  ...  16.30  28.39  108.10   830.5  0.1089  0.2649  0.3779   \n",
      "159  0.05922  ...  19.07  30.88  123.40  1138.0  0.1464  0.1871  0.2914   \n",
      "160  0.06864  ...  23.17  27.65  157.10  1748.0  0.1517  0.4002  0.4211   \n",
      "\n",
      "          27      28       29  \n",
      "0    0.17320  0.2770  0.10630  \n",
      "1    0.08272  0.2157  0.10430  \n",
      "2    0.17500  0.4228  0.11750  \n",
      "3    0.18270  0.3216  0.10100  \n",
      "4    0.10800  0.2668  0.08174  \n",
      "..       ...     ...      ...  \n",
      "156  0.09186  0.2626  0.07048  \n",
      "157  0.18570  0.3138  0.08113  \n",
      "158  0.09594  0.2471  0.07463  \n",
      "159  0.16090  0.3029  0.08216  \n",
      "160  0.21340  0.3003  0.10480  \n",
      "\n",
      "[161 rows x 30 columns]\n",
      "[1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
      " 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0\n",
      " 0 1 1 0 1 1 0 0 0 1 0 1 1]\n",
      "(161, 30)\n",
      "\n",
      "üß™ TEST LOCAL (primeras filas):\n",
      "       0      1       2       3        4        5        6        7       8   \\\n",
      "0   13.81  23.75   91.56   597.8  0.13230  0.17680  0.15580  0.09176  0.2251   \n",
      "1   21.71  17.25  140.90  1546.0  0.09384  0.08562  0.11680  0.08465  0.1717   \n",
      "2   10.20  17.48   65.05   321.2  0.08054  0.05907  0.05774  0.01071  0.1964   \n",
      "3   11.20  29.37   70.67   386.0  0.07449  0.03558  0.00000  0.00000  0.1060   \n",
      "4   16.16  21.54  106.20   809.8  0.10080  0.12840  0.10430  0.05613  0.2160   \n",
      "..    ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
      "64  12.75  16.70   82.51   493.8  0.11250  0.11170  0.03880  0.02995  0.2120   \n",
      "65  14.86  23.21  100.40   671.4  0.10440  0.19800  0.16970  0.08878  0.1737   \n",
      "66  11.84  18.70   77.93   440.6  0.11090  0.15160  0.12180  0.05182  0.2301   \n",
      "67  12.34  14.95   78.29   469.1  0.08682  0.04571  0.02109  0.02054  0.1571   \n",
      "68  16.65  21.38  110.00   904.6  0.11210  0.14570  0.15250  0.09170  0.1995   \n",
      "\n",
      "         9   ...     20     21      22      23       24       25       26  \\\n",
      "0   0.07421  ...  19.20  41.85  128.50  1153.0  0.22260  0.52090  0.46460   \n",
      "1   0.05054  ...  30.75  26.44  199.50  3143.0  0.13630  0.16280  0.28610   \n",
      "2   0.06315  ...  11.48  24.47   75.40   403.7  0.09527  0.13970  0.19250   \n",
      "3   0.05502  ...  11.92  38.30   75.19   439.6  0.09267  0.05494  0.00000   \n",
      "4   0.05891  ...  19.47  31.68  129.70  1175.0  0.13950  0.30550  0.29920   \n",
      "..      ...  ...    ...    ...     ...     ...      ...      ...      ...   \n",
      "64  0.06623  ...  14.45  21.74   93.63   624.1  0.14750  0.19790  0.14230   \n",
      "65  0.06672  ...  16.08  27.78  118.60   784.7  0.13160  0.46480  0.45890   \n",
      "66  0.07799  ...  16.82  28.12  119.40   888.7  0.16370  0.57750  0.69560   \n",
      "67  0.05708  ...  13.18  16.85   84.11   533.1  0.10480  0.06744  0.04921   \n",
      "68  0.06330  ...  26.46  31.56  177.00  2215.0  0.18050  0.35780  0.46950   \n",
      "\n",
      "         27      28       29  \n",
      "0   0.20130  0.4432  0.10860  \n",
      "1   0.18200  0.2510  0.06494  \n",
      "2   0.03571  0.2868  0.07809  \n",
      "3   0.00000  0.1566  0.05905  \n",
      "4   0.13120  0.3480  0.07619  \n",
      "..      ...     ...      ...  \n",
      "64  0.08045  0.3071  0.08557  \n",
      "65  0.17270  0.3000  0.08701  \n",
      "66  0.15460  0.4761  0.14020  \n",
      "67  0.04793  0.2298  0.05974  \n",
      "68  0.20950  0.3613  0.09564  \n",
      "\n",
      "[69 rows x 30 columns]\n",
      "[1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1]\n",
      "\n",
      "üåç TEST GLOBAL (primeras filas):\n",
      "        0      1       2       3        4        5         6         7   \\\n",
      "0    10.90  12.96   68.69   366.8  0.07515  0.03718  0.003090  0.006588   \n",
      "1    13.98  19.62   91.12   599.5  0.10600  0.11330  0.112600  0.064630   \n",
      "2    12.56  19.07   81.92   485.8  0.08760  0.10380  0.103000  0.043910   \n",
      "3    23.21  26.97  153.50  1670.0  0.09509  0.16820  0.195000  0.123700   \n",
      "4    13.50  12.71   85.69   566.2  0.07376  0.03614  0.002758  0.004419   \n",
      "..     ...    ...     ...     ...      ...      ...       ...       ...   \n",
      "109  14.97  16.95   96.22   685.9  0.09855  0.07885  0.026020  0.037810   \n",
      "110  11.54  10.72   73.73   409.1  0.08597  0.05969  0.013670  0.008907   \n",
      "111  12.19  13.29   79.08   455.8  0.10660  0.09509  0.028550  0.028820   \n",
      "112  18.31  20.58  120.80  1052.0  0.10680  0.12480  0.156900  0.094510   \n",
      "113  18.01  20.56  118.40  1007.0  0.10010  0.12890  0.117000  0.077620   \n",
      "\n",
      "         8        9   ...     20     21      22      23       24       25  \\\n",
      "0    0.1442  0.05743  ...  12.36  18.20   78.07   470.0  0.11710  0.08294   \n",
      "1    0.1669  0.06544  ...  17.04  30.80  113.90   869.3  0.16130  0.35680   \n",
      "2    0.1533  0.06184  ...  13.37  22.43   89.02   547.4  0.10960  0.20020   \n",
      "3    0.1909  0.06309  ...  31.01  34.51  206.00  2944.0  0.14810  0.41260   \n",
      "4    0.1365  0.05335  ...  14.97  16.94   95.48   698.7  0.09023  0.05836   \n",
      "..      ...      ...  ...    ...    ...     ...     ...      ...      ...   \n",
      "109  0.1780  0.05650  ...  16.11  23.00  104.60   793.7  0.12160  0.16370   \n",
      "110  0.1833  0.06100  ...  12.34  12.87   81.23   467.8  0.10920  0.16260   \n",
      "111  0.1880  0.06471  ...  13.34  17.81   91.38   545.2  0.14270  0.25850   \n",
      "112  0.1860  0.05941  ...  21.86  26.20  142.20  1493.0  0.14920  0.25360   \n",
      "113  0.2116  0.06077  ...  21.53  26.06  143.40  1426.0  0.13090  0.23270   \n",
      "\n",
      "          26       27      28       29  \n",
      "0    0.01854  0.03953  0.2738  0.07685  \n",
      "1    0.40690  0.18270  0.3179  0.10550  \n",
      "2    0.23880  0.09265  0.2121  0.07188  \n",
      "3    0.58200  0.25930  0.3103  0.08677  \n",
      "4    0.01379  0.02210  0.2267  0.06192  \n",
      "..       ...      ...     ...      ...  \n",
      "109  0.06648  0.08485  0.2404  0.06428  \n",
      "110  0.08324  0.04715  0.3390  0.07434  \n",
      "111  0.09915  0.08187  0.3469  0.09241  \n",
      "112  0.37590  0.15100  0.3074  0.07863  \n",
      "113  0.25440  0.14890  0.3251  0.07625  \n",
      "\n",
      "[114 rows x 30 columns]\n",
      "[0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 1]\n",
      "üîé Overlap TRAIN vs TEST GLOBAL: 0\n",
      "üîé Overlap TEST LOCAL vs TEST GLOBAL: 0\n",
      "['radiusMEAN', 'textureMEAN', 'perimeterMEAN', 'areaMEAN', 'smoothnessMEAN', 'compactnessMEAN', 'concavityMEAN', 'concave pointsMEAN', 'symmetryMEAN', 'fractaldimensionMEAN', 'radiusSE', 'textureSE', 'perimeterSE', 'areaSE', 'smoothnessSE', 'compactnessSE', 'concavitySE', 'concave pointsSE', 'symmetrySE', 'fractalDimensionSE', 'radiusWORST', 'textureWORST', 'perimeterWORST', 'areaWORST', 'smoothnessWORST', 'compactnessWORST', 'concavityWORST', 'concavePointsWORST', 'symmetryWORST', 'fractalDimensionWORST']\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train,\n",
    " X_test_local, y_test_local,\n",
    " X_test_global, y_test_global,\n",
    " dataset, feature_names, label_encoder,\n",
    " scaler, numeric_features, encoder, preprocessor) = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nüì¶ TRAIN (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nüß™ TEST LOCAL (primeras filas):\")\n",
    "print(pd.DataFrame(X_test_local))\n",
    "\n",
    "print(y_test_local)\n",
    "\n",
    "print(\"\\nüåç TEST GLOBAL (primeras filas):\")\n",
    "print(pd.DataFrame(X_test_global))\n",
    "\n",
    "print(y_test_global)\n",
    "\n",
    "def overlap_check(A, B):\n",
    "    hA = pd.util.hash_pandas_object(pd.DataFrame(A), index=False)\n",
    "    hB = pd.util.hash_pandas_object(pd.DataFrame(B), index=False)\n",
    "    return np.intersect1d(hA.values, hB.values).size\n",
    "\n",
    "\n",
    "print(\"üîé Overlap TRAIN vs TEST GLOBAL:\",\n",
    "      overlap_check(X_train, X_test_global))\n",
    "\n",
    "print(\"üîé Overlap TEST LOCAL vs TEST GLOBAL:\",\n",
    "      overlap_check(X_test_local, X_test_global))\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# üåº CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import EnsembleDecisionTreeSurrogate, SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model, num_idx, mean, scale):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.mean = np.asarray(mean, dtype=np.float32)\n",
    "        self.scale = np.asarray(scale, dtype=np.float32)\n",
    "        self.scale_safe = np.where(self.scale == 0, 1.0, self.scale)\n",
    "\n",
    "    def _scale_internally(self, X):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        Xs = X.copy()\n",
    "        # soporta [n, d] o [d]\n",
    "        if Xs.ndim == 1:\n",
    "            Xs = Xs[None, :]\n",
    "        Xs[:, self.num_idx] = (Xs[:, self.num_idx] - self.mean) / self.scale_safe\n",
    "        return Xs\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return probs.cpu().numpy()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient, ClientUtilsMixin):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, X_test_global, y_test_global, X_train_nn, X_test_nn, scaler_nn_mean, scaler_nn_scale, num_idx, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.nn_model_local = copy.deepcopy(nn_model)\n",
    "        self.nn_model_global = nn_model  # la que Flower sincroniza\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_test_global = X_test_global\n",
    "        self.y_test_global = y_test_global\n",
    "        self.X_train_nn = X_train_nn\n",
    "        self.X_test_nn  = X_test_nn\n",
    "        self.scaler_nn_mean = np.asarray(scaler_nn_mean, dtype=np.float32)\n",
    "        self.scaler_nn_scale = np.where(np.asarray(scaler_nn_scale, np.float32)==0, 1.0, np.asarray(scaler_nn_scale, np.float32))\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=1e-3):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train_nn, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ‚úÖ Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "            # 1Ô∏è‚É£ Recibir GLOBAL\n",
    "            set_model_params(\n",
    "                self.tree_model,\n",
    "                self.nn_model_global,\n",
    "                {\"tree\": [\n",
    "                    self.tree_model.get_params()[\"max_depth\"],\n",
    "                    self.tree_model.get_params()[\"min_samples_split\"],\n",
    "                    self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "                ], \"nn\": parameters}\n",
    "            )\n",
    "\n",
    "            round_number = config.get(\"server_round\", 1)\n",
    "\n",
    "            # 2Ô∏è‚É£ Entrenar LOCAL SOLO EN LA PRIMERA RONDA\n",
    "            if round_number <= NUM_TRAIN_ROUNDS:\n",
    "                self.tree_model.fit(self.X_train, self.y_train)\n",
    "                self._train_nn()   # NN local\n",
    "\n",
    "            # 3Ô∏è‚É£ Enviar SIEMPRE el LOCAL\n",
    "            nn_weights = get_model_parameters(\n",
    "                self.tree_model,\n",
    "                self.nn_model_local\n",
    "            )[\"nn\"]\n",
    "\n",
    "            return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model_global, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        explain_only = config.get(\"explain_only\", False)\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ‚ùå Error al recibir SuperTree: {e}\")\n",
    "\n",
    "\n",
    "        # üîπ CASO 1: rondas de entrenamiento (1..NUM_TRAIN_ROUNDS)\n",
    "        if not explain_only:\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "            supertree = SuperTree()\n",
    "            root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "\n",
    "            root_node = supertree.prune_redundant_leaves_local(root_node)\n",
    "\n",
    "\n",
    "            self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features,\n",
    "                                scaler=None, unique_labels=UNIQUE_LABELS, encoder=self.encoder)\n",
    "            tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "\n",
    "            # En rondas de entrenamiento NO explicas todo el test (si no quieres)\n",
    "            if self.received_supertree is not None and round_number == NUM_TRAIN_ROUNDS:\n",
    "                pass\n",
    "\n",
    "            return 0.0, len(self.X_test), {\n",
    "                f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "                f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "                f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "                f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "                f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "            }\n",
    "\n",
    "         # üîπ CASO 2: ronda final (solo explicaci√≥n con Supertree final)\n",
    "        else:\n",
    "            print(f\"[CLIENTE {self.client_id}] üîç Ronda final: solo explicaciones\")\n",
    "            # aqu√≠ NO entrenamos self.tree_model ni mandamos tree_ensemble_*\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            y_pred_tree_local = self.tree_model.predict(self.X_test)\n",
    "\n",
    "            self.local_metrics = {\n",
    "                \"acc_local_tree\": accuracy_score(self.y_test, y_pred_tree_local),\n",
    "                \"prec_local_tree\": precision_score(self.y_test, y_pred_tree_local, average=\"weighted\"),\n",
    "                \"rec_local_tree\": recall_score(self.y_test, y_pred_tree_local, average=\"weighted\"),\n",
    "                \"f1_local_tree\": f1_score(self.y_test, y_pred_tree_local, average=\"weighted\"),\n",
    "            }\n",
    "\n",
    "            # Usamos el SuperTree final recibido + LORE + mergedTree\n",
    "            if self.received_supertree is not None:\n",
    "                \n",
    "                self.explain_all_test_instances(config)\n",
    "                # self.explain_all_test_instances(config, only_idx=0)\n",
    "\n",
    "            # Puedes devolver m√©tricas dummy o medias de lo que has calculado en explain_all_test_instances\n",
    "            return 0.0, len(self.X_test), {}\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def _explain_one_instance(self, num_row, config, save_trees=False):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "        \n",
    "\n",
    "        # Wrapper que escala SOLO para la NN (espacio NN)\n",
    "        bb_local = TorchNNWrapper(\n",
    "            model=self.nn_model_local,\n",
    "            num_idx=self.num_idx,\n",
    "            mean=self.scaler_nn_mean,\n",
    "            scale=self.scaler_nn_scale\n",
    "        )\n",
    "\n",
    "        bb_global = TorchNNWrapper(\n",
    "            model=self.nn_model_global,\n",
    "            num_idx=self.num_idx,\n",
    "            mean=self.scaler_nn_mean,\n",
    "            scale=self.scaler_nn_scale\n",
    "        )\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            None,                 # <-- sin scaler (en crudo)\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Aseg√∫rate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        row = np.asarray(self.X_test[num_row], dtype=np.float32)\n",
    "\n",
    "        probs_local = bb_local.predict_proba(row[None, :])\n",
    "        pred_class_idx_local = int(probs_local.argmax(axis=1)[0])\n",
    "        pred_class_local = self.label_encoder.inverse_transform([pred_class_idx_local])[0]\n",
    "\n",
    "        probs_global = bb_global.predict_proba(row[None, :])\n",
    "        pred_class_idx_global = int(probs_global.argmax(axis=1)[0])\n",
    "        pred_class_global = self.label_encoder.inverse_transform([pred_class_idx_global])[0]\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")\n",
    "\n",
    "\n",
    "        bbox_for_Z = sklearn_classifier_bbox.sklearnBBox(bb_global)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox_for_Z, local_tabular_dataset)\n",
    "\n",
    "\n",
    "        # Explicaci√≥n LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "\n",
    "        # print(\"Instancia a explicar (decodificada):\")\n",
    "        # print(x_instance)\n",
    "        # print(\"ü§ñ NN local pred:\", pred_class_local)\n",
    "        # print(\"üåç NN global pred:\", pred_class_global)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "\n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS,\n",
    "                                                    client_id=self.client_id, round_number=round_number)\n",
    "        \n",
    "\n",
    "        lore_tree_global = explanation[\"merged_tree\"]\n",
    "\n",
    "\n",
    "        \n",
    "        if save_trees:\n",
    "            self.save_lore_tree_image(lore_tree_global.root,round_number,self.feature_names,self.numeric_features,UNIQUE_LABELS,self.encoder,folder=\"lore_tree_global\")\n",
    "\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # \n",
    "        # # MERGED TREE\n",
    "\n",
    "        # merged_tree = SuperTree()\n",
    "        # merged_tree.mergeDecisionTrees(\n",
    "        #     roots=[lore_tree.root, self.received_supertree],\n",
    "        #     num_classes=len(self.unique_labels),\n",
    "        #     feature_names=self.feature_names,\n",
    "        #     categorical_features=list(self.global_mapping.keys()), \n",
    "        #     global_mapping=self.global_mapping\n",
    "        # )\n",
    "        \n",
    "        # merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "\n",
    "        # if save_trees:\n",
    "        #     self.save_mergedTree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sint√©tico generado alrededor del punto a explicar.\n",
    "        y_bb_global = explanation[\"neighborhood_Yb\"] # predicciones del modelo BBOX (red neuronal) sobre Z (el vecindario).\n",
    "        y_bb_local  = bb_local.predict(Z)\n",
    "\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "        # 1Ô∏è‚É£ Crear surrogate NUEVO (no el del LORE global)\n",
    "        surrogate_local = EnsembleDecisionTreeSurrogate(n_estimators=1)\n",
    "\n",
    "        y_bb_local_enc = self.encoder.encode_target_class(y_bb_local.reshape(-1, 1), categories_global=UNIQUE_LABELS).squeeze()\n",
    "\n",
    "        # print(\"Distribuci√≥n BB global en Z:\", np.bincount(y_bb_global))\n",
    "        # print(\"Distribuci√≥n BB local en Z:\", np.bincount(y_bb_local_enc))\n",
    "\n",
    "        # 2Ô∏è‚É£ Entrenar con MISMO Z pero etiquetas LOCALES\n",
    "        surrogate_local.train(\n",
    "            Z,\n",
    "            y_bb_local_enc,\n",
    "            features=self.feature_names\n",
    "        )\n",
    "\n",
    "        # 3Ô∏è‚É£ Obtener LoreTree LOCAL\n",
    "        lore_tree_local = surrogate_local.get_single_supertree(\n",
    "            num_classes=len(UNIQUE_LABELS),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()),\n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        if save_trees:\n",
    "            self.save_lore_tree_image(lore_tree_local.root,round_number,self.feature_names,self.numeric_features,UNIQUE_LABELS,self.encoder,folder=\"lore_tree_local\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # tree_str = self.tree_to_str(merged_tree.root,self.feature_names,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        lore_tree_local_str = self.tree_to_str(lore_tree_local.root, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        lore_tree_global_str = self.tree_to_str(lore_tree_global.root, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        supertree_str = self.tree_to_str(self.received_supertree, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # # rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "        rules_lore_local = self.extract_rules_from_str(lore_tree_local_str, target_class_label=pred_class_local)\n",
    "        rules_lore_global = self.extract_rules_from_str(lore_tree_global_str, target_class_label=pred_class_global)\n",
    "        rules_supertree_global = self.extract_rules_from_str(supertree_str, target_class_label=pred_class_global)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"‚àß\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 ‚àß ‚â§ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|‚â§|‚â•)\\s*([-\\d\\.]+)\\s*‚àß\\s*([><]=?|‚â§|‚â•)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # Eval√∫a las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('‚â§','<=').replace('‚â•','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('‚â§','<=').replace('‚â•','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu c√≥digo tal cual ...\n",
    "                if \"‚â§\" in cond:\n",
    "                    var, val = cond.split(\"‚â§\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"‚â•\" in cond:\n",
    "                    var, val = cond.replace(\"‚â•\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"‚â†\" in cond:\n",
    "                    var, val = cond.split(\"‚â†\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "\n",
    "        \n",
    "        regla_factual_lore_local = None\n",
    "        for r in rules_lore_local:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_lore_local = r\n",
    "                break\n",
    "\n",
    "        regla_factual_lore_global = None\n",
    "        for r in rules_lore_global:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_lore_global = r\n",
    "                break\n",
    "\n",
    "        \n",
    "        regla_factual_supertree_global = None\n",
    "        for r in rules_supertree_global:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_supertree_global = r\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # # # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "\n",
    "\n",
    "        # Extraer 1 contrafactual tipo LORE por cada clase distinta a la predicha\n",
    "        cf_rules_LORE_local_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class_local:\n",
    "                rules_clase = self.extract_rules_from_str(lore_tree_local_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_LORE_local_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "\n",
    "        cf_rules_LORE_global_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class_global:\n",
    "                rules_clase = self.extract_rules_from_str(lore_tree_global_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_LORE_global_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "\n",
    "\n",
    "        cf_rules_Supertree_global_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class_global:\n",
    "                rules_clase = self.extract_rules_from_str(supertree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_Supertree_global_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================================================================================================================================================================================\n",
    "        # üìè M√âTRICAS DE EXPLICABILIDAD LOCAL (vecindario Z)\n",
    "        # \n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ========================================================================================================================================================================================================\n",
    "        mask_same_class_local = (y_bb_local == pred_class_idx_local)\n",
    "        mask_diff_class_local = (y_bb_local != pred_class_idx_local)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class_local]\n",
    "        Z_minus = dfZ[mask_diff_class_local]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette_local = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette_local = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        mask_same_class_global = (y_bb_global == pred_class_idx_global)\n",
    "        mask_diff_class_global = (y_bb_global != pred_class_idx_global)\n",
    "        Z_plus_global = dfZ[mask_same_class_global]\n",
    "        Z_minus_global = dfZ[mask_diff_class_global]\n",
    "\n",
    "        a_global = pairwise_distances([x], Z_plus_global).mean() if len(Z_plus_global) > 0 else 0.0\n",
    "        b_global = pairwise_distances([x], Z_minus_global).mean() if len(Z_minus_global) > 0 else 0.0\n",
    "\n",
    "        silhouette_global = 0.0\n",
    "        if (a_global + b_global) > 0:\n",
    "            silhouette_global = (b_global - a_global) / max(a_global, b_global)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # ==================================================================================================================\n",
    "        # M√âTRICAS DE COMPARATIVA DE PRECISION DE LOS √ÅRBOLES CON EL TEST (QUE TAN BUENOS SON LOS ARBOLES QUE HEMOS GENERADO)\n",
    "        # ==================================================================================================================\n",
    "\n",
    "        y_true = self.y_test\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred_supertree = self.received_supertree.predict(self.X_test)\n",
    "\n",
    "        Xg = self.X_test_global\n",
    "        if Xg.ndim == 1:\n",
    "            Xg = Xg.reshape(1, -1)\n",
    "\n",
    "        y_pred_superTree_globalTest = self.received_supertree.predict(self.X_test_global)\n",
    "        y_pred_localTree_globalTest = self.tree_model.predict(self.X_test_global)\n",
    "        \n",
    "\n",
    "        # Accuracy, precision, Recall, F1\n",
    "        acc_supertree = accuracy_score(y_true, y_pred_supertree)\n",
    "        prec_supertree       = precision_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "        rec_super  = recall_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "        f1_super   = f1_score(y_true, y_pred_supertree, average=\"weighted\")\n",
    "\n",
    "        acc_super_globalTest = accuracy_score(self.y_test_global, y_pred_superTree_globalTest)\n",
    "        prec_super_globalTest = precision_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "        rec_super_globalTest = recall_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "        f1_super_globalTest = f1_score(self.y_test_global, y_pred_superTree_globalTest, average=\"weighted\")\n",
    "\n",
    "        acc_localTree_globalTest = accuracy_score(self.y_test_global, y_pred_localTree_globalTest)\n",
    "        prec_localTree_globalTest = precision_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "        rec_localTree_globalTest = recall_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "        f1_localTree_globalTest = f1_score(self.y_test_global, y_pred_localTree_globalTest, average=\"weighted\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ================= CSV por cliente =================\n",
    "        row = {\n",
    "            \"round\": int(round_number),\n",
    "            \"dataset\": DATASET_NAME,\n",
    "            \"client_id\": int(self.client_id),\n",
    "            \"bbox_pred_class_global\": str(pred_class_global),\n",
    "            \"bbox_pred_class_local\": str(pred_class_local),\n",
    "\n",
    "            # Vecindario\n",
    "            \"silhouette_global\": float(silhouette_global),\n",
    "            \"silhouette_local\": float(silhouette_local),\n",
    "\n",
    "            # ================= M√©tricas de como de buenos son los √°rboles =================\n",
    "            \"acc_super\": float(acc_supertree),\n",
    "            \"prec_super\": float(prec_supertree),\n",
    "            \"rec_super\": float(rec_super),\n",
    "            \"f1_super\": float(f1_super),\n",
    "\n",
    "            \"acc_super_globalTest\": float(acc_super_globalTest),\n",
    "            \"prec_super_globalTest\": float(prec_super_globalTest),\n",
    "            \"rec_super_globalTest\": float(rec_super_globalTest),\n",
    "            \"f1_super_globalTest\": float(f1_super_globalTest),\n",
    "\n",
    "            # üîπ m√©tricas LOCALES (guardadas antes)\n",
    "            \"acc_local_tree\": self.local_metrics[\"acc_local_tree\"],\n",
    "            \"prec_local_tree\": self.local_metrics[\"prec_local_tree\"],\n",
    "            \"rec_local_tree\": self.local_metrics[\"rec_local_tree\"],\n",
    "            \"f1_local_tree\": self.local_metrics[\"f1_local_tree\"],\n",
    "\n",
    "            \"acc_localTree_global\": float(acc_localTree_globalTest),\n",
    "            \"prec_localTree_global\": float(prec_localTree_globalTest),\n",
    "            \"rec_localTree_global\": float(rec_localTree_globalTest),\n",
    "            \"f1_localTree_global\": float(f1_localTree_globalTest),\n",
    "\n",
    "            \n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # (Opcional) M√©tricas contrafactuales por clase en columnas ‚Äúanchas‚Äù\n",
    "\n",
    "        # for cl in self.unique_labels:\n",
    "        #     row[f\"cf_cov_lore_{cl}_TEST\"]  = self._to_float(coverage_cf_lore.get(cl,0))\n",
    "        #     row[f\"cf_comp_lore_{cl}_TEST\"] = int(comp_cf_lore_simpl.get(cl,0))\n",
    "        #     row[f\"cf_prec_lore_{cl}_TEST\"] = self._to_float(precision_cf_lore.get(cl,0))\n",
    "\n",
    "\n",
    "        # Guardar\n",
    "        self._append_client_csv(row, filename=\"Balanced\")\n",
    "\n",
    "        return row\n",
    "\n",
    "    # ======================================================================\n",
    "    # Bucle sobre todo el test\n",
    "    # ======================================================================\n",
    "    def explain_all_test_instances(self, config, only_idx=None):\n",
    "        results = []\n",
    "\n",
    "        # Si only_idx es None ‚Üí explicamos TODO el test\n",
    "        # Si only_idx es un entero ‚Üí explicamos solo esa instancia\n",
    "        if only_idx is None:\n",
    "            indices = range(len(self.X_test))\n",
    "            desc_text = f\"Cliente {self.client_id} explicando test completo\"\n",
    "            save_trees_flag = False      \n",
    "\n",
    "        else:\n",
    "            indices = [only_idx]\n",
    "            desc_text = f\"Cliente {self.client_id} explicando instancia {only_idx}\"\n",
    "            save_trees_flag = True\n",
    "\n",
    "\n",
    "        for i in tqdm(indices, desc=desc_text):\n",
    "            try:\n",
    "\n",
    "                row = self._explain_one_instance(i, config, save_trees=save_trees_flag)\n",
    "                results.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[Cliente {self.client_id}] ‚ö†Ô∏è Error en instancia {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "        # üî• Leer el CSV incremental REAL\n",
    "        full_path = f\"results/metrics_Balanced_cliente_{self.client_id}.csv\"\n",
    "        df = pd.read_csv(full_path)\n",
    "\n",
    "        # üî• Guardar CSV de medias en formato metric,mean\n",
    "        mean_metrics = df.mean(numeric_only=True)\n",
    "        mean_df = mean_metrics.to_frame(name=\"mean\")\n",
    "        mean_df.to_csv(\n",
    "            f\"results/metrics_cliente_{self.client_id}_balanced_mean.csv\",\n",
    "            index_label=\"metric\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "\n",
    "    (X_train, y_train,\n",
    "     X_test_local, y_test_local,\n",
    "     X_test_global, y_test_global,\n",
    "     dataset, feature_names, label_encoder,\n",
    "     scaler, numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=NUM_CLIENTS)\n",
    "    \n",
    "    tree_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, random_state=42)\n",
    "\n",
    "    num_idx = list(range(len(numeric_features)))\n",
    "\n",
    "    scaler_nn = StandardScaler().fit(X_train[:, num_idx])\n",
    "\n",
    "    def scale_for_nn(X):\n",
    "        Xs = X.copy().astype(np.float32)\n",
    "        Xs[:, num_idx] = scaler_nn.transform(Xs[:, num_idx])\n",
    "        return Xs\n",
    "    \n",
    "    X_train_nn = scale_for_nn(X_train)\n",
    "    X_test_nn  = scale_for_nn(X_test_local)\n",
    "\n",
    "    # ‚úÖ SIEMPRE MISMO N√öMERO DE CLASES GLOBAL\n",
    "    n_clases_global = len(UNIQUE_LABELS)  # o len(label_encoder.classes_)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = n_clases_global\n",
    "\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test_local,\n",
    "                        y_test=y_test_local,\n",
    "                        X_test_global=X_test_global,\n",
    "                        y_test_global=y_test_global,\n",
    "                        X_train_nn=X_train_nn, \n",
    "                        X_test_nn=X_test_nn,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor,         \n",
    "                        scaler_nn_mean=scaler_nn.mean_,  \n",
    "                        scaler_nn_scale=scaler_nn.scale_,\n",
    "                        num_idx=num_idx).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# üì¶ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan din√°micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# üß† UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net est√° en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    sums: Dict[str, float] = {}\n",
    "    counts: Dict[str, int] = {}\n",
    "\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                sums[k] = sums.get(k, 0.0) + n * float(v)\n",
    "                counts[k] = counts.get(k, 0) + n\n",
    "\n",
    "    return {k: sums[k] / counts[k] for k in sums}\n",
    "\n",
    "# ============================\n",
    "# üöÄ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se carg√≥ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "        # ============================\n",
    "        # üîπ Ronda final: NO fusionar nada\n",
    "        # ============================\n",
    "        if server_round > NUM_TRAIN_ROUNDS:\n",
    "            return aggregated_metrics\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] üå≤ Generando SuperTree - Ronda {server_round}\")\n",
    "            from collections import defaultdict\n",
    "\n",
    "            tree_nodes = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            feature_names = None\n",
    "            numeric_features = None\n",
    "            class_names = None\n",
    "\n",
    "            # 1) recolectar mapeos categ√≥ricos y metadatos\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                # distinct_values_* para global_mapping\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"distinct_values_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        enc = json.loads(v)\n",
    "                        client_encoders[cid] = enc\n",
    "                        for feat, d in enc.items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            # 2) recolectar √°rboles y dem√°s metadatos por cliente\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"tree_ensemble_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(v)\n",
    "\n",
    "                        # lee estos una sola vez (son iguales por cliente)\n",
    "                        if feature_names is None and f\"encoded_feature_names_{cid}\" in metrics:\n",
    "                            feature_names = json.loads(metrics[f\"encoded_feature_names_{cid}\"])\n",
    "                        if numeric_features is None and f\"numeric_features_{cid}\" in metrics:\n",
    "                            numeric_features = json.loads(metrics[f\"numeric_features_{cid}\"])\n",
    "                        if class_names is None and f\"unique_labels_{cid}\" in metrics:\n",
    "                            class_names = json.loads(metrics[f\"unique_labels_{cid}\"])\n",
    "\n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "                            tree_nodes.append(root)\n",
    "\n",
    "            if not tree_nodes:\n",
    "                return aggregated_metrics\n",
    "\n",
    "            # 3) fusionar\n",
    "            st = SuperTree()\n",
    "            st.mergeDecisionTrees(\n",
    "                roots=tree_nodes,\n",
    "                num_classes=len(class_names),\n",
    "                feature_names=feature_names,\n",
    "                categorical_features=list(global_mapping.keys()),\n",
    "                global_mapping=global_mapping,\n",
    "            )\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            st.prune_redundant_leaves_full()\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            # st.merge_equal_class_leaves()\n",
    "\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # 4) guardar/emitir\n",
    "            save_supertree_plot(\n",
    "                root_node=st.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=class_names,\n",
    "                numeric_features=numeric_features,\n",
    "                global_mapping=global_mapping,   # sin scaler\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(st.root.to_dict())\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ‚ùå Error en SuperTree: {e}\")\n",
    "\n",
    "        return aggregated_metrics\n",
    "\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# üß© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "\n",
    "            # Siempre mandamos el √∫ltimo SuperTree disponible\n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "\n",
    "            # Ronda final: modo solo explicaci√≥n\n",
    "            if server_round == NUM_SERVER_ROUNDS:\n",
    "                ins.config[\"explain_only\"] = True\n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUM√âRICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real  = left\n",
    "            right_real = right\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} ‚â§ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} ‚àà ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features,\n",
    "                scaler=None,  # ya no se usa\n",
    "                global_mapping=global_mapping, depth=depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEG√ìRICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEG√ìRICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACI√ìN] Error interpretando categ√≥rica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACI√ìN] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACI√ìN] Nombres num√©ricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACI√ìN] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACI√ìN] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\",\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0]); node_id[0] += 1\n",
    "\n",
    "        # etiqueta\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            label = f\"class: {class_names[class_index]}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            label = fname.split(\"_\", 1)[0] if \"_\" in fname else fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent: dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # OneHot\n",
    "            if \"_\" in fname:\n",
    "                _, val = fname.split(\"_\", 1)\n",
    "                add_node(node.children[0], curr, f'‚â† \"{val.strip()}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val.strip()}\"')\n",
    "            # Num√©rica\n",
    "            elif fname in numeric_features:\n",
    "                thr = node.intervals[0] if node.intervals else node.thresh\n",
    "                add_node(node.children[0], curr, f\"‚â§ {thr:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thr:.2f}\")\n",
    "            # Categ√≥rica ordinal\n",
    "            elif fname in global_mapping:\n",
    "                vals = global_mapping[fname]\n",
    "                val = vals[node.intervals[0]] if node.intervals else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'‚â† \"{val}\"')\n",
    "            else:\n",
    "                for ch in node.children:\n",
    "                    add_node(ch, curr, \"?\")\n",
    "\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"pdf\", cleanup=True)\n",
    "    return f\"{filename}.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# üîß INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:32:19,875\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-12-23 10:32:23,693 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n",
      "[CLIENTE 1] üîç Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cliente 1 explicando test completo:   0%|          | 0/69 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n",
      "[CLIENTE 2] üîç Ronda final: solo explicaciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cliente 2 explicando test completo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [05:21<00:00,  4.73s/it]\n",
      "Cliente 1 explicando test completo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69/69 [06:22<00:00,  5.55s/it]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 397.81s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar a√∫n m√°s\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesi√≥n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ab03c",
   "metadata": {},
   "source": [
    "### BALANCED METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a663325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voy a usar estos ficheros:\n",
      "  - metrics_cliente_1_balanced_mean.csv\n",
      "  - metrics_cliente_2_balanced_mean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc_localTree_global</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acc_local_tree</td>\n",
       "      <td>0.941603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acc_super</td>\n",
       "      <td>0.956095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acc_super_globalTest</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>client_id</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f1_localTree_global</td>\n",
       "      <td>0.916927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f1_local_tree</td>\n",
       "      <td>0.942010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f1_super</td>\n",
       "      <td>0.956095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>f1_super_globalTest</td>\n",
       "      <td>0.928529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prec_localTree_global</td>\n",
       "      <td>0.917849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prec_local_tree</td>\n",
       "      <td>0.944077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prec_super</td>\n",
       "      <td>0.956095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prec_super_globalTest</td>\n",
       "      <td>0.933086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rec_localTree_global</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rec_local_tree</td>\n",
       "      <td>0.941603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rec_super</td>\n",
       "      <td>0.956095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rec_super_globalTest</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>round</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>silhouette_global</td>\n",
       "      <td>0.470036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>silhouette_local</td>\n",
       "      <td>-0.604418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   metric      mean\n",
       "0    acc_localTree_global  0.916667\n",
       "1          acc_local_tree  0.941603\n",
       "2               acc_super  0.956095\n",
       "3    acc_super_globalTest  0.929825\n",
       "4               client_id  1.500000\n",
       "5     f1_localTree_global  0.916927\n",
       "6           f1_local_tree  0.942010\n",
       "7                f1_super  0.956095\n",
       "8     f1_super_globalTest  0.928529\n",
       "9   prec_localTree_global  0.917849\n",
       "10        prec_local_tree  0.944077\n",
       "11             prec_super  0.956095\n",
       "12  prec_super_globalTest  0.933086\n",
       "13   rec_localTree_global  0.916667\n",
       "14         rec_local_tree  0.941603\n",
       "15              rec_super  0.956095\n",
       "16   rec_super_globalTest  0.929825\n",
       "17                  round  3.000000\n",
       "18      silhouette_global  0.470036\n",
       "19       silhouette_local -0.604418"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Promedios globales guardados en: results\\metrics_Balanced_global.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üìä Promedio global a partir de los *balanced_mean*\n",
    "# ==========================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "csv_dir = Path(\"results\")\n",
    "\n",
    "files = sorted(csv_dir.glob(\"metrics_cliente_*_balanced_mean.csv\"))\n",
    "print(\"Voy a usar estos ficheros:\")\n",
    "for f in files:\n",
    "    print(\"  -\", f.name)\n",
    "\n",
    "# Leer cada fichero y dejar columnas [metric, value_clienteX]\n",
    "dfs = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)          # aqu√≠ s√≠ usamos la cabecera \",0\"\n",
    "    # df.columns suele ser algo como [\"Unnamed: 0\", \"0\"]\n",
    "    df = df.rename(columns={df.columns[0]: \"metric\", df.columns[1]: f\"value_{f.stem}\"})\n",
    "    dfs.append(df)\n",
    "\n",
    "# Unir por nombre de m√©trica (outer join para no perder nada)\n",
    "merged = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged = merged.merge(df, on=\"metric\", how=\"outer\")\n",
    "\n",
    "# Calcular la media entre clientes (macro-media)\n",
    "value_cols = [c for c in merged.columns if c.startswith(\"value_\")]\n",
    "merged[\"mean\"] = merged[value_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "means_df = merged[[\"metric\", \"mean\"]].copy()\n",
    "\n",
    "# ==========================================\n",
    "# üìâ Colapsar clases B/M en m√©tricas CF (TEST y Z)\n",
    "# ==========================================\n",
    "\n",
    "collapse_patterns = {\n",
    "    # --------- TEST ----------\n",
    "    \"cf_cov_merged_TEST\":   r\"^cf_cov_merged_[^_]+_TEST$\",\n",
    "    \"cf_comp_merged_TEST\":  r\"^cf_comp_merged_[^_]+_TEST$\",\n",
    "    \"cf_prec_merged_TEST\": r\"^cf_prec_merged_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_lore_TEST\":     r\"^cf_cov_lore_[^_]+_TEST$\",\n",
    "    \"cf_comp_lore_TEST\":    r\"^cf_comp_lore_[^_]+_TEST$\",\n",
    "    \"cf_prec_lore_TEST\":   r\"^cf_prec_lore_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_super_TEST\":    r\"^cf_cov_super_[^_]+_TEST$\",\n",
    "    \"cf_comp_super_TEST\":   r\"^cf_comp_super_[^_]+_TEST$\",\n",
    "    \"cf_prec_super_TEST\":  r\"^cf_prec_super_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_local_local_TEST\":   r\"^cf_cov_local_local_[^_]+_TEST$\",\n",
    "    \"cf_comp_local_local_TEST\":  r\"^cf_comp_local_local_[^_]+_TEST$\",\n",
    "    \"cf_prec_local_local_TEST\":  r\"^cf_prec_local_local_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_local_super_TEST\":   r\"^cf_cov_local_super_[^_]+_TEST$\",\n",
    "    \"cf_comp_local_super_TEST\":  r\"^cf_comp_local_super_[^_]+_TEST$\",\n",
    "    \"cf_prec_local_super_TEST\":  r\"^cf_prec_local_super_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_localZ_TEST\":   r\"^cf_cov_localZ_[^_]+_TEST$\",\n",
    "    \"cf_comp_localZ_TEST\":  r\"^cf_comp_localZ_[^_]+_TEST$\",\n",
    "    \"cf_prec_localZ_TEST\":  r\"^cf_prec_localZ_[^_]+_TEST$\",\n",
    "\n",
    "    \"cf_cov_superZ_TEST\":   r\"^cf_cov_superZ_[^_]+_TEST$\",\n",
    "    \"cf_comp_superZ_TEST\":  r\"^cf_comp_superZ_[^_]+_TEST$\",\n",
    "    \"cf_prec_superZ_TEST\":  r\"^cf_prec_superZ_[^_]+_TEST$\",\n",
    "\n",
    "    # --------- Z ----------\n",
    "    \"cf_cov_merged_Z\":   r\"^cf_cov_merged_[^_]+_Z$\",\n",
    "    \"cf_prec_merged_Z\":  r\"^cf_prec_merged_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_lore_Z\":     r\"^cf_cov_lore_[^_]+_Z$\",\n",
    "    \"cf_prec_lore_Z\":    r\"^cf_prec_lore_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_super_Z\":    r\"^cf_cov_super_[^_]+_Z$\",\n",
    "    \"cf_prec_super_Z\":   r\"^cf_prec_super_[^_]+_Z$\",  # ojo: revisa este patr√≥n si hiciera falta\n",
    "\n",
    "    \"cf_cov_local_local_Z\":   r\"^cf_cov_local_local_[^_]+_Z$\",\n",
    "    \"cf_prec_local_local_Z\":  r\"^cf_prec_local_local_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_local_super_Z\":   r\"^cf_cov_local_super_[^_]+_Z$\",\n",
    "    \"cf_prec_local_super_Z\":  r\"^cf_prec_local_super_[^_]+_Z$\",\n",
    "    \n",
    "    \"cf_cov_localZ_Z\":   r\"^cf_cov_localZ_[^_]+_Z$\",\n",
    "    \"cf_prec_localZ_Z\":  r\"^cf_prec_localZ_[^_]+_Z$\",\n",
    "\n",
    "    \"cf_cov_superZ_Z\":   r\"^cf_cov_superZ_[^_]+_Z$\",\n",
    "    \"cf_prec_superZ_Z\":  r\"^cf_prec_superZ_[^_]+_Z$\",\n",
    "}\n",
    "\n",
    "rows_new = []\n",
    "\n",
    "for new_name, pattern in collapse_patterns.items():\n",
    "    mask = means_df[\"metric\"].str.match(pattern)\n",
    "    sub = means_df[mask]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    new_mean = sub[\"mean\"].mean()\n",
    "    rows_new.append({\"metric\": new_name, \"mean\": new_mean})\n",
    "\n",
    "# A√±adir las filas colapsadas\n",
    "if rows_new:\n",
    "    means_df = pd.concat([means_df, pd.DataFrame(rows_new)], ignore_index=True)\n",
    "\n",
    "# Eliminar m√©tricas CF espec√≠ficas por clase (Yes, No, 0, 1, B, M, etc.), tanto TEST como Z\n",
    "pattern_drop = r\"^cf_(cov|prec|comp)_\" \\\n",
    "               r\"(merged|lore|super|local_local|local_super|localZ|superZ)_\" \\\n",
    "               r\"[^_]+_(TEST|Z)$\"\n",
    "\n",
    "means_df = means_df[~means_df[\"metric\"].str.match(pattern_drop)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# üíæ Guardar resultado final\n",
    "# ==========================================\n",
    "\n",
    "display(means_df.head(30))\n",
    "\n",
    "out_path = csv_dir / \"metrics_Balanced_global.csv\"\n",
    "means_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Promedios globales guardados en: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
