{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68391f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 13:15:24,084\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-11-11 13:15:27,595 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.piping.pipe(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-11-11 13:15:27,595 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.rendering.render(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-11-11 13:15:27,606 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.unflattening.unflatten(['stagger', 'fanout', 'chain', 'encoding'])\n",
      "2025-11-11 13:15:27,607 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.viewing.view(['quiet'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.quote(['is_html_string', 'is_valid_id', 'dot_keywords', 'endswith_odd_number_of_backslashes', 'escape_unescaped_quotes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.a_list(['kwargs', 'attributes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.attr_list(['kwargs', 'attributes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.clear(['keep_attrs'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.__iter__(['subgraph'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.node(['_attributes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.edge(['_attributes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.attr(['_attributes'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.subgraph(['name', 'comment', 'graph_attr', 'node_attr', 'edge_attr', 'body'])\n",
      "2025-11-11 13:15:27,612 graphviz._tools DEBUG    deprecate positional args: graphviz.piping.Pipe._pipe_legacy(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-11-11 13:15:27,623 graphviz._tools DEBUG    deprecate positional args: graphviz.saving.Save.save(['directory'])\n",
      "2025-11-11 13:15:27,623 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.render(['directory', 'view', 'cleanup', 'format', 'renderer', 'formatter', 'neato_no_op', 'quiet', 'quiet_view'])\n",
      "2025-11-11 13:15:27,624 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.view(['directory', 'cleanup', 'quiet', 'quiet_view'])\n",
      "2025-11-11 13:15:27,625 graphviz._tools DEBUG    deprecate positional args: graphviz.unflattening.Unflatten.unflatten(['stagger', 'fanout', 'chain'])\n",
      "2025-11-11 13:15:27,626 graphviz._tools DEBUG    deprecate positional args: graphviz.graphs.BaseGraph.__init__(['comment', 'filename', 'directory', 'format', 'engine', 'encoding', 'graph_attr', 'node_attr', 'edge_attr', 'body', 'strict'])\n",
      "2025-11-11 13:15:27,627 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.from_file(['directory', 'format', 'engine', 'encoding', 'renderer', 'formatter'])\n",
      "2025-11-11 13:15:27,629 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.__init__(['filename', 'directory', 'format', 'engine', 'encoding'])\n",
      "2025-11-11 13:15:27,629 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.save(['directory'])\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# üì¶ IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "from lore_sa.client_utils import ClientUtilsMixin\n",
    "\n",
    "# Otros\n",
    "from pathlib import Path\n",
    "from filelock import FileLock  # pip install filelock\n",
    "import pandas as pd, os\n",
    "from graphviz import Digraph\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ‚öôÔ∏è VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "SEED = 42\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# üßπ Borrar TODOS los CSV individuales de clientes\n",
    "# ==============================================\n",
    "\n",
    "csv_dir = Path(\"results\")\n",
    "all_csvs = list(csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Solo borrar si hay alguno\n",
    "if all_csvs:\n",
    "    for f in all_csvs:\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass  # Ignora errores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# üîß UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# üì• CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name, class_col):\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_tmp = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "    df = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado est√°ndar\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "    \n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado espec√≠fico para el dataset de c√°ncer de mama\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].nunique() < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [col for col in df.select_dtypes(include=\"category\").columns if col != class_col]\n",
    "    num_features = [col for col in df.columns if df[col].dtype.kind in \"fi\" and col != class_col]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    ohe.fit(df[cat_features])\n",
    "    categories_global = ohe.categories_\n",
    "    onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    return cat_features, num_features, categories_global, onehot_columns\n",
    "\n",
    "\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    # Saca info global siempre al principio\n",
    "    cat_features, num_features, categories_global, onehot_columns = get_global_onehot_info(flower_dataset_name, class_col)\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado espec√≠fico por dataset\n",
    "    if \"adult\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset['SeniorCitizen'] = dataset['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado espec√≠fico para el dataset de c√°ncer de mama\n",
    "        dataset.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataset[class_col])\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "    dataset[class_col] = label_encoder.transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", \"passthrough\", numeric_indices),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", categories=categories_global), categorical_indices)\n",
    "    ])\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    # Reconstrucci√≥n del DataFrame\n",
    "    num_out = X_encoded[:, :len(numeric_features)]\n",
    "    cat_out = X_encoded[:, len(numeric_features):]\n",
    "    if categorical_features:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = numeric_features\n",
    "\n",
    "    X_df = pd.DataFrame(num_out, columns=num_names)\n",
    "    if len(cat_names) > 0:\n",
    "        X_cat_df = pd.DataFrame(cat_out, columns=cat_names)\n",
    "        X_full = pd.concat([X_df.reset_index(drop=True), X_cat_df.reset_index(drop=True)], axis=1)\n",
    "        for col in onehot_columns:\n",
    "            if col not in X_cat_df.columns:\n",
    "                X_full[col] = 0\n",
    "    else:\n",
    "        X_full = X_df\n",
    "\n",
    "    # Rellenar columnas onehot que falten y ordenar\n",
    "    final_columns = num_names + list(cat_names)\n",
    "    X_full = X_full[final_columns]\n",
    "    FEATURES[:] = final_columns\n",
    "\n",
    "    split_idx = int(0.7 * len(X_full))\n",
    "\n",
    "        # --- ¬°Construye el descriptor global! ---\n",
    "    descriptor_global = descriptor.copy()\n",
    "    for i, col in enumerate(cat_features):\n",
    "        if col in descriptor_global[\"categorical\"]:\n",
    "            descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(categories_global[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    return (\n",
    "        X_full.iloc[:split_idx].to_numpy(), y[:split_idx],\n",
    "        X_full.iloc[split_idx:].to_numpy(), y[split_idx:],\n",
    "        tabular_dataset, final_columns, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# Los resultados de las m√©tricas no son muy buenos aqui\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "CLASS_COLUMN = \"HeartDisease\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "# CLASS_COLUMN = \"diagnosis\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "# CLASS_COLUMN = \"Outcome\" \n",
    "\n",
    "\n",
    " \n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c49bf6",
   "metadata": {},
   "source": [
    "### HOLDOUT DEL SERVIDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 13:15:27,662 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-11-11 13:15:27,880 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-11-11 13:15:28,010 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease HTTP/11\" 200 613\n",
      "2025-11-11 13:15:28,147 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/HeartDisease.py HTTP/11\" 404 0\n",
      "2025-11-11 13:15:28,150 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-11-11 13:15:28,497 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/HeartDisease/pablopalacios23/HeartDisease.py HTTP/11\" 404 0\n",
      "2025-11-11 13:15:28,625 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/README.md HTTP/11\" 404 0\n",
      "2025-11-11 13:15:28,774 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/revision/1a8d6e23c2479cd551c95cb4c2872ab8d2661602 HTTP/11\" 200 613\n",
      "2025-11-11 13:15:28,907 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-11-11 13:15:28,907 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-11-11 13:15:29,124 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/HeartDisease HTTP/11\" 200 None\n",
      "2025-11-11 13:15:29,258 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/revision/1a8d6e23c2479cd551c95cb4c2872ab8d2661602 HTTP/11\" 200 613\n",
      "2025-11-11 13:15:29,385 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/tree/1a8d6e23c2479cd551c95cb4c2872ab8d2661602?recursive=False&expand=False HTTP/11\" 200 204\n",
      "2025-11-11 13:15:29,511 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/tree/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-11-11 13:15:29,641 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-11-11 13:15:29,799 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/revision/1a8d6e23c2479cd551c95cb4c2872ab8d2661602 HTTP/11\" 200 613\n",
      "2025-11-11 13:15:30,143 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-11-11 13:15:30,143 filelock     DEBUG    Attempting to acquire lock 1708700977776 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:30,143 filelock     DEBUG    Lock 1708700977776 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:30,143 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___heart_disease/default/0.0.0/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_info.json\n",
      "2025-11-11 13:15:30,143 filelock     DEBUG    Attempting to release lock 1708700977776 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:30,143 filelock     DEBUG    Lock 1708700977776 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:30,176 filelock     DEBUG    Attempting to acquire lock 1708700978880 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:30,176 filelock     DEBUG    Lock 1708700978880 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:30,176 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___heart_disease/default/0.0.0/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_info.json\n",
      "2025-11-11 13:15:30,176 filelock     DEBUG    Attempting to release lock 1708700978880 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:30,176 filelock     DEBUG    Lock 1708700978880 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:30,376 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-11-11 13:15:30,510 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease HTTP/11\" 200 613\n",
      "2025-11-11 13:15:30,644 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/HeartDisease.py HTTP/11\" 404 0\n",
      "2025-11-11 13:15:30,762 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/HeartDisease/pablopalacios23/HeartDisease.py HTTP/11\" 404 0\n",
      "2025-11-11 13:15:30,891 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/README.md HTTP/11\" 404 0\n",
      "2025-11-11 13:15:31,011 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-11-11 13:15:31,144 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/HeartDisease HTTP/11\" 200 None\n",
      "2025-11-11 13:15:31,278 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/tree/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-11-11 13:15:31,394 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-11-11 13:15:31,647 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/HeartDisease/revision/1a8d6e23c2479cd551c95cb4c2872ab8d2661602 HTTP/11\" 200 613\n",
      "2025-11-11 13:15:31,779 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/HeartDisease/resolve/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-11-11 13:15:31,779 filelock     DEBUG    Attempting to acquire lock 1708708377856 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Lock 1708708377856 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:31,784 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___heart_disease/default/0.0.0/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_info.json\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Attempting to release lock 1708708377856 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Lock 1708708377856 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___heart_disease_default_0.0.0_1a8d6e23c2479cd551c95cb4c2872ab8d2661602.lock\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Attempting to acquire lock 1708709022752 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Lock 1708709022752 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:31,784 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___heart_disease/default/0.0.0/1a8d6e23c2479cd551c95cb4c2872ab8d2661602/dataset_info.json\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Attempting to release lock 1708709022752 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n",
      "2025-11-11 13:15:31,784 filelock     DEBUG    Lock 1708709022752 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___heart_disease\\default\\0.0.0\\1a8d6e23c2479cd551c95cb4c2872ab8d2661602_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ X_train (primeras filas):\n",
      "   0    1    2  3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
      "0  44  118  242  0  149  0.3  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
      "1  46  140  311  0  120  1.8  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "2  42  115  211  0  137  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0   \n",
      "3  74  138    0  0  116  0.2  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
      "4  45  130  236  0  144  0.1  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
      "5  58  130  230  0  150  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0   \n",
      "6  72  160    0  0  114  1.6  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
      "7  55  172  260  0   73  2.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "8  63  133    0  0  120  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0   \n",
      "9  74  145    0  1  123  1.3  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
      "\n",
      "    15   16   17   18   19  \n",
      "0  1.0  0.0  0.0  1.0  0.0  \n",
      "1  0.0  1.0  0.0  1.0  0.0  \n",
      "2  1.0  0.0  0.0  0.0  1.0  \n",
      "3  1.0  0.0  0.0  0.0  1.0  \n",
      "4  1.0  0.0  0.0  0.0  1.0  \n",
      "5  1.0  0.0  0.0  0.0  1.0  \n",
      "6  1.0  0.0  0.0  1.0  0.0  \n",
      "7  1.0  0.0  0.0  1.0  0.0  \n",
      "8  0.0  1.0  0.0  1.0  0.0  \n",
      "9  1.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "üéØ y_train (primeros valores):\n",
      "[0 1 0 0 0 0 0 1 1 1]\n",
      "\n",
      "üì¶ X_test (primeras filas):\n",
      "   0    1    2  3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
      "0  41  120  237  1  138  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "1  36  112  340  0  184  1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
      "2  69  140    0  1  118  2.5  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0   \n",
      "3  55  140  229  0  110  0.5  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "4  58  135  222  0  100  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "\n",
      "    15   16   17   18   19  \n",
      "0  0.0  1.0  0.0  1.0  0.0  \n",
      "1  1.0  0.0  0.0  1.0  0.0  \n",
      "2  1.0  0.0  1.0  0.0  0.0  \n",
      "3  0.0  1.0  0.0  1.0  0.0  \n",
      "4  1.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "üéØ y_test (primeros valores):\n",
      "[1 0 1 0 0]\n",
      "['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak', 'Sex_F', 'Sex_M', 'ChestPainType_ASY', 'ChestPainType_ATA', 'ChestPainType_NAP', 'ChestPainType_TA', 'RestingECG_LVH', 'RestingECG_Normal', 'RestingECG_ST', 'ExerciseAngina_N', 'ExerciseAngina_Y', 'STSlope_Down', 'STSlope_Flat', 'STSlope_Up']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\nüì¶ X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\nüéØ y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nüì¶ X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\nüéØ y_test (primeros valores):\")\n",
    "print(y_test)\n",
    "\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# üåº CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model, num_idx, mean, scale):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.mean = np.asarray(mean, dtype=np.float32)\n",
    "        self.scale = np.asarray(scale, dtype=np.float32)\n",
    "        self.scale_safe = np.where(self.scale == 0, 1.0, self.scale)\n",
    "\n",
    "    def _scale_internally(self, X):\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        Xs = X.copy()\n",
    "        # soporta [n, d] o [d]\n",
    "        if Xs.ndim == 1:\n",
    "            Xs = Xs[None, :]\n",
    "        Xs[:, self.num_idx] = (Xs[:, self.num_idx] - self.mean) / self.scale_safe\n",
    "        return Xs\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            return logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xs = self._scale_internally(X)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return probs.cpu().numpy()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient, ClientUtilsMixin):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, X_train_nn, X_test_nn, scaler_nn_mean, scaler_nn_scale, num_idx, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_train_nn = X_train_nn\n",
    "        self.X_test_nn  = X_test_nn\n",
    "        self.scaler_nn_mean = np.asarray(scaler_nn_mean, dtype=np.float32)\n",
    "        self.scaler_nn_scale = np.where(np.asarray(scaler_nn_scale, np.float32)==0, 1.0, np.asarray(scaler_nn_scale, np.float32))\n",
    "        self.num_idx = np.asarray(num_idx, dtype=int)\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=1e-3):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train_nn, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ‚úÖ Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self._train_nn()\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                \n",
    "                # print(\"supertree_dict\")\n",
    "                # print(\"supertree_dict:\", supertree_dict)\n",
    "                # print(\"type:\", type(supertree_dict))\n",
    "                # print(\"dir(supertree_dict):\", dir(supertree_dict))\n",
    "                # print(\"\\n\")\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ‚ùå Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "        \n",
    "        root_node = supertree.prune_redundant_leaves_local(root_node)\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(export_text(self.tree_model, feature_names=FEATURES))\n",
    "        # print(\"root_node:\", root_node)\n",
    "        # print(\"type:\", type(root_node))\n",
    "        # print(dir(root_node))\n",
    "        # print(\"\\n\")\n",
    "        # print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "        \n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features,scaler=None, unique_labels=UNIQUE_LABELS, encoder=self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None and config.get(\"server_round\", 0) == NUM_SERVER_ROUNDS:\n",
    "            self.explain_all_test_instances(config)\n",
    "\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def _explain_one_instance(self, num_row, config, save_trees=False):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "        \n",
    "\n",
    "        # Wrapper que escala SOLO para la NN (espacio NN)\n",
    "        nn_wrapper = TorchNNWrapper(\n",
    "            self.nn_model,\n",
    "            num_idx=self.num_idx,\n",
    "            mean=self.scaler_nn_mean,\n",
    "            scale=self.scaler_nn_scale,\n",
    "        )\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            None,                 # <-- sin scaler (en crudo)\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Aseg√∫rate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        row = np.asarray(self.X_test[num_row], dtype=np.float32)\n",
    "        probs = nn_wrapper.predict_proba(row[None, :])\n",
    "        pred_class_idx = int(probs.argmax(axis=1)[0])\n",
    "        pred_class = self.label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")\n",
    "\n",
    "\n",
    "        # Explicabilidad local y la vecindad es generada del train (local_tabular_dataset)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "        \n",
    "        # Explicaci√≥n LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS,\n",
    "                                                     client_id=self.client_id, round_number=round_number)\n",
    "\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        \n",
    "        if save_trees:\n",
    "            self.save_lore_tree_image(lore_tree.root,round_number,self.feature_names,self.numeric_features,UNIQUE_LABELS,self.encoder,folder=\"LoreTree\")\n",
    "\n",
    "        merged_tree = SuperTree()\n",
    "        merged_tree.mergeDecisionTrees(\n",
    "            roots=[lore_tree.root, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()), \n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "        if save_trees:\n",
    "            self.save_mergedTree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "\n",
    "        tree_str = self.tree_to_str(merged_tree.root,self.feature_names,numeric_features=self.numeric_features,scaler=None, global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        lore_tree_str = self.tree_to_str(lore_tree.root, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "        supertree_str = self.tree_to_str(self.received_supertree, self.feature_names, numeric_features=self.numeric_features,scaler=None,global_mapping=self.global_mapping,unique_labels=self.unique_labels)\n",
    "           \n",
    "        rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "        rules_lore = self.extract_rules_from_str(lore_tree_str, target_class_label=pred_class)\n",
    "        rules_supertree = self.extract_rules_from_str(supertree_str, target_class_label=pred_class)\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"‚àß\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 ‚àß ‚â§ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|‚â§|‚â•)\\s*([-\\d\\.]+)\\s*‚àß\\s*([><]=?|‚â§|‚â•)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # Eval√∫a las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('‚â§','<=').replace('‚â•','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('‚â§','<=').replace('‚â•','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu c√≥digo tal cual ...\n",
    "                if \"‚â§\" in cond:\n",
    "                    var, val = cond.split(\"‚â§\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"‚â•\" in cond:\n",
    "                    var, val = cond.replace(\"‚â•\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"‚â†\" in cond:\n",
    "                    var, val = cond.split(\"‚â†\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "        # Buscar la regla factual (la que cubre la instancia)\n",
    "        regla_factual = None\n",
    "        for regla in rules:\n",
    "            if cumple_regla(decoded, regla):\n",
    "                regla_factual = regla\n",
    "                break\n",
    "\n",
    "        # 3) Regla factual de LORE: la que cubre la instancia decodificada 'decoded'\n",
    "        regla_factual_lore = None\n",
    "        for r in rules_lore:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_lore = r\n",
    "                break\n",
    "\n",
    "        regla_factual_supertree = None\n",
    "        for r in rules_supertree:\n",
    "            if cumple_regla(decoded, r):\n",
    "                regla_factual_supertree = r\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "        cf_rules_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "\n",
    "\n",
    "        # Extraer 1 contrafactual tipo LORE por cada clase distinta a la predicha\n",
    "        cf_rules_LORE_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(lore_tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_LORE_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "\n",
    "        cf_rules_Supertree_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(supertree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la m√°s sencilla (menos condiciones)\n",
    "                    cf_rules_Supertree_por_clase[clase] = min(rules_clase, key=len)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # ========================================================================================================================================================================================================\n",
    "        # üìè M√âTRICAS DE COMPARATIVA DE PRECISION DE LOS √ÅRBOLES\n",
    "        # ========================================================================================================================================================================================================\n",
    "\n",
    "        y_true = self.y_test\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred_lore = lore_tree.root.predict(self.X_test)\n",
    "        y_pred_merged = merged_tree.root.predict(self.X_test)\n",
    "        y_pred_super = self.received_supertree.predict(self.X_test)\n",
    "\n",
    "        # Accuracy\n",
    "        acc_lore = accuracy_score(y_true, y_pred_lore)\n",
    "        acc_merged = accuracy_score(y_true, y_pred_merged)\n",
    "        acc_super = accuracy_score(y_true, y_pred_super)\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        prec_lore = precision_score(y_true, y_pred_lore, average=\"weighted\")\n",
    "        rec_lore  = recall_score(y_true, y_pred_lore, average=\"weighted\")\n",
    "        f1_lore   = f1_score(y_true, y_pred_lore, average=\"weighted\")\n",
    "\n",
    "        prec_merged = precision_score(y_true, y_pred_merged, average=\"weighted\")\n",
    "        rec_merged  = recall_score(y_true, y_pred_merged, average=\"weighted\")\n",
    "        f1_merged   = f1_score(y_true, y_pred_merged, average=\"weighted\")\n",
    "\n",
    "        prec_super = precision_score(y_true, y_pred_super, average=\"weighted\")\n",
    "        rec_super  = recall_score(y_true, y_pred_super, average=\"weighted\")\n",
    "        f1_super   = f1_score(y_true, y_pred_super, average=\"weighted\")\n",
    "\n",
    "        # AUC (solo si es binario)\n",
    "        try:\n",
    "            auc_lore = roc_auc_score(y_true, y_pred_lore)\n",
    "            auc_merged = roc_auc_score(y_true, y_pred_merged)\n",
    "            auc_super = roc_auc_score(y_true, y_pred_super)\n",
    "        except:\n",
    "            auc_lore = auc_merged = auc_super = None\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================================================================================================================================================================================\n",
    "        # üìè M√âTRICAS DE EXPLICABILIDAD LOCAL (vecindario Z)\n",
    "        # ========================================================================================================================================================================================================\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sint√©tico generado alrededor del punto a explicar.\n",
    "        y_bb = explanation[\"neighborhood_Yb\"] # predicciones del modelo BBOX (red neuronal) sobre Z (el vecindario).\n",
    "\n",
    "        y_surrogate_preds = explanation[\"surrogate_preds\"]  # predicciones del modelo interpretable (arbol - LORE Tree) sobre Z (el vecindario).\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        # ==============================================================================================\n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ==============================================================================================\n",
    "\n",
    "        mask_same_class = (y_bb == pred_class_idx)\n",
    "        mask_diff_class = (y_bb != pred_class_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "        # Fidelity: Porcentaje de veces que el modelo interpretable (LORE tree) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Un valor alto de fidelity significa que el √°rbol surrogate est√° imitando bien a la red neuronal para esa instancia.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Fidelity merged tree: Porcentaje de veces que el modelo interpretable (√Årbol combinado LORE + global) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Comparar BBOX(Z) vs MergedTree(Z).\n",
    "\n",
    "        # Mide cu√°nto consigue imitar al BBOX en el entorno de la instancia explicada el √°rbol fusionado.\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "\n",
    "        fidelity_Lore = accuracy_score(y_bb, y_surrogate_preds)\n",
    "\n",
    "\n",
    "        y_mt_Z = merged_tree.root.predict(Z)\n",
    "        fidelity_merged_Z = accuracy_score(y_bb, y_mt_Z)\n",
    "\n",
    "        \n",
    "        y_super_Z = self.received_supertree.predict(Z)\n",
    "        fidelity_super_Z = accuracy_score(y_bb, y_super_Z)\n",
    "\n",
    "\n",
    "\n",
    "        # ====================================================================================================================================================================================================================================================\n",
    "        # Coverage (merged tree):\n",
    "        #   Proporci√≥n de instancias del vecindario Z que satisfacen la REGRA FACTUAL del √°rbol fusionado\n",
    "        #   (regla_factual extra√≠da de `tree_str` del merged_tree para la clase predicha `pred_class`).\n",
    "\n",
    "\n",
    "        # Precision (merged tree factual):\n",
    "        #   Condicionada a las instancias de Z que cumplen la regla factual del merged tree,\n",
    "        #   proporci√≥n en la que el BBOX (y_bb) predice la MISMA clase que la de la regla factual        \n",
    "\n",
    "        # NOTAS:\n",
    "\n",
    "        # Z es el vecindario sint√©tico generado por LORE alrededor de la instancia x que estamos explicando.\n",
    "\n",
    "        # - `regla_factual` se obtiene DE LAS REGLAS DEL MERGED TREE (no del LORE local),\n",
    "        #   filtrando por la clase predicha `pred_class` y eligiendo la que CUBRE x.\n",
    "\n",
    "        # - Si no existe `regla_factual` (ninguna regla del merged cubre x), definimos:\n",
    "        #     coverage_merged = 0.0\n",
    "        #     precision_merged = 0.0   # (o \"N/A\" si prefieres no definir precisi√≥n sin cobertura)\n",
    "\n",
    "\n",
    "\n",
    "        # Coverage/Precision LORE en el vecindario Z (comparando SIEMPRE con el BBOX)\n",
    "\n",
    "\n",
    "\n",
    "        # Coverage/Precision Supertree en el vecindario Z (comparando SIEMPRE con el BBOX)\n",
    "\n",
    "        # =====================================================================================================================================================================================================================================================\n",
    "\n",
    "        support_merged = 0\n",
    "        support_lore   = 0\n",
    "        support_supertree = 0\n",
    "\n",
    "        # Decodifica cada fila del vecindario a un formato legible\n",
    "        dfZ_decoded = dfZ.apply(lambda row: self.decode_onehot_instance(\n",
    "            row.values, self.numeric_features, self.encoder, self.scaler, self.feature_names\n",
    "        ), axis=1)\n",
    "\n",
    "        cf_rules_por_clase_simplify = self._simplify_rules_by_class(cf_rules_por_clase, mode='loose')\n",
    "\n",
    "        regla_factual_simplify = None\n",
    "        \n",
    "        if regla_factual:\n",
    "            regla_factual_simplify = self._simplify_rule(regla_factual, mode='loose')\n",
    "            cumplen_regla = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual), axis=1)\n",
    "            coverage_merged = cumplen_regla.mean()\n",
    "            support_merged = cumplen_regla.sum()   # üëà n√∫mero absoluto\n",
    "\n",
    "            \n",
    "            covered_target_match = (y_bb[cumplen_regla.values] == pred_class_idx)\n",
    "\n",
    "            if cumplen_regla.sum() > 0:\n",
    "                precision_merged  = covered_target_match.sum() / cumplen_regla.sum()\n",
    "            else:\n",
    "                precision_merged  = 0.0\n",
    "        else:\n",
    "            coverage_merged = \"None\" # \"Ninguna regla factual cubre la instancia. No hay una regla en el √°rbol que explique la predicci√≥n sobre esa muestra\"\n",
    "            precision_merged = \"None\"  # \"Si no hay regla factual, no se puede calcular la precisi√≥n (n√∫mero de aciertos entre las instancias cubiertas)\"\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cf_rules_LORE_por_clase_simplify = self._simplify_rules_by_class(cf_rules_LORE_por_clase, mode='loose')\n",
    "\n",
    "        regla_factual_LORE_simplify = None\n",
    "\n",
    "        if regla_factual_lore:\n",
    "\n",
    "            regla_factual_LORE_simplify = self._simplify_rule(regla_factual_lore, mode='loose')\n",
    "            cumplen_regla_lore = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual_lore), axis=1)\n",
    "            coverage_lore = cumplen_regla_lore.mean()\n",
    "            support_lore = cumplen_regla_lore.sum()\n",
    "\n",
    "            precision_lore = (y_bb[cumplen_regla_lore.values] == pred_class_idx).mean() if cumplen_regla_lore.sum() > 0 else 0.0\n",
    "        else:\n",
    "            coverage_lore =  \"None\" # \"Ninguna regla factual cubre la instancia. No hay una regla en el √°rbol que explique la predicci√≥n sobre esa muestra\"\n",
    "            precision_lore = \"None\"  # \"Si no hay regla factual, no se puede calcular la precisi√≥n (n√∫mero de aciertos entre las instancias cubiertas)\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cf_rules_Supertree_por_clase_simplify = self._simplify_rules_by_class(cf_rules_Supertree_por_clase, mode='loose')\n",
    "        regla_factual_Supertree_simplify = None\n",
    "\n",
    "        if regla_factual_supertree:\n",
    "            \n",
    "            regla_factual_Supertree_simplify = self._simplify_rule(regla_factual_supertree, mode='loose')\n",
    "            cumplen_regla_supertree = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual_supertree), axis=1)\n",
    "            coverage_supertree = cumplen_regla_supertree.mean()\n",
    "            support_supertree = cumplen_regla_supertree.sum()\n",
    "\n",
    "            precision_supertree = (y_bb[cumplen_regla_supertree.values] == pred_class_idx).mean() if cumplen_regla_supertree.sum() > 0 else 0.0\n",
    "        else:\n",
    "            coverage_supertree =  \"None\" # \"Ninguna regla factual cubre la instancia. No hay una regla en el √°rbol que explique la predicci√≥n sobre esa muestra\"\n",
    "            precision_supertree = \"None\"  # \"Si no hay regla factual, no se puede calcular la precisi√≥n (n√∫mero de aciertos entre las instancias cubiertas)\"\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        # ======================================== HIT ========================================        \n",
    "        # BBOX predice una clase para tu instancia ùë•, por ejemplo \"No\".\n",
    "\n",
    "        # extract_rules_from_str(..., target_class_label=pred_class) te devuelve solo las reglas de esa clase \"No\" en el surrogate.\n",
    "\n",
    "        # Si alguna de esas reglas cubre x ‚Üí\n",
    "\n",
    "        # Existe regla factual ‚úÖ\n",
    "        # Esa regla ya es de la misma clase que el BBOX ‚úÖ\n",
    "        # Entonces HIT = 1.\n",
    "\n",
    "        # Si ninguna regla de esa clase cubre x ‚Üí HIT = 0.\n",
    "        # ============================================================================================\n",
    "\n",
    "        hit_merged = int(regla_factual is not None)              # factual del merged\n",
    "        hit_lore   = int(regla_factual_lore is not None)         # factual del LORE\n",
    "        hit_supertree = int(regla_factual_supertree is not None) # factual del supertree\n",
    "\n",
    "\n",
    "        # ============================================================================================\n",
    "        # Metricas de los √°rboles\n",
    "        # ============================================================================================\n",
    "\n",
    "        depth_merged_edges = self.tree_depth_edges(merged_tree.root)\n",
    "        nodes_merged = self.count_nodes(merged_tree.root)\n",
    "        leaves_merged = self.count_leaves(merged_tree.root)\n",
    "\n",
    "        depth_lore_edges = self.tree_depth_edges(lore_tree.root)\n",
    "        nodes_lore = self.count_nodes(lore_tree.root)\n",
    "        leaves_lore = self.count_leaves(lore_tree.root)\n",
    "\n",
    "        depth_supertree_edges = self.tree_depth_edges(self.received_supertree)\n",
    "        nodes_supertree = self.count_nodes(self.received_supertree)\n",
    "        leaves_supertree = self.count_leaves(self.received_supertree)\n",
    "\n",
    "        # ============================================================================================================================================\n",
    "        # Complejidad de las reglas (n√∫mero de condiciones)\n",
    "        # ============================================================================================================================================\n",
    "\n",
    "        def rule_complexity(regla):\n",
    "            return len(regla) if regla else 0\n",
    "        \n",
    "        comp_factual_merged_simpl = rule_complexity(regla_factual_simplify)\n",
    "\n",
    "        comp_cf_merged_simpl = {cl: rule_complexity(r) for cl, r in cf_rules_por_clase_simplify.items()}\n",
    "\n",
    "        comp_factual_lore_simpl = rule_complexity(regla_factual_LORE_simplify)\n",
    "\n",
    "        comp_cf_lore_simpl = {cl: rule_complexity(r) for cl, r in cf_rules_LORE_por_clase_simplify.items()}\n",
    "\n",
    "        comp_factual_supertree_simpl = rule_complexity(regla_factual_Supertree_simplify)\n",
    "\n",
    "        comp_cf_supertree_simpl = {cl: rule_complexity(r) for cl, r in cf_rules_Supertree_por_clase_simplify.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ============================================================================================================================================\n",
    "\n",
    "        # Coverage / Support / Precision de CONTRAFACTUALES (por clase)\n",
    "\n",
    "        # ============================================================================================================================================\n",
    "\n",
    "\n",
    "        # --- Merged tree ---\n",
    "        coverage_cf_merged = {}\n",
    "        support_cf_merged  = {}\n",
    "        precision_cf_merged = {}\n",
    "\n",
    "        for clase, regla_cf in cf_rules_por_clase_simplify.items():\n",
    "            # 'clase' es la etiqueta objetivo del contrafactual (p.ej. \"Yes\" si la predicha factual fue \"No\")\n",
    "            if regla_cf:\n",
    "                mask_cf = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_cf), axis=1)\n",
    "                cov = float(mask_cf.mean())\n",
    "                sup = int(mask_cf.sum())\n",
    "                # precisi√≥n: de las Z que cumplen el contrafactual, ¬øcu√°ntas el BBOX las clasifica como 'clase'?\n",
    "                if sup > 0:\n",
    "                    clase_idx = self.label_encoder.transform([clase])[0]\n",
    "                    prec = float((y_bb[mask_cf.values] == clase_idx).mean())\n",
    "                else:\n",
    "                    prec = None\n",
    "            else:\n",
    "                cov, sup, prec = None, 0, None\n",
    "\n",
    "            coverage_cf_merged[clase]  = cov\n",
    "            support_cf_merged[clase]   = sup\n",
    "            precision_cf_merged[clase] = prec\n",
    "\n",
    "        # --- LORE tree ---\n",
    "        coverage_cf_lore = {}\n",
    "        support_cf_lore  = {}\n",
    "        precision_cf_lore = {}\n",
    "\n",
    "        for clase, regla_cf in cf_rules_LORE_por_clase_simplify.items():\n",
    "            if regla_cf:\n",
    "                mask_cf = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_cf), axis=1)\n",
    "                cov = float(mask_cf.mean())\n",
    "                sup = int(mask_cf.sum())\n",
    "                if sup > 0:\n",
    "                    clase_idx = self.label_encoder.transform([clase])[0]\n",
    "                    prec = float((y_bb[mask_cf.values] == clase_idx).mean())\n",
    "                else:\n",
    "                    prec = None\n",
    "            else:\n",
    "                cov, sup, prec = None, 0, None\n",
    "\n",
    "            coverage_cf_lore[clase]  = cov\n",
    "            support_cf_lore[clase]   = sup\n",
    "            precision_cf_lore[clase] = prec\n",
    "\n",
    "        # --- Supertree ---\n",
    "        coverage_cf_supertree = {}\n",
    "        support_cf_supertree  = {}\n",
    "        precision_cf_supertree = {}\n",
    "\n",
    "        for clase, regla_cf in cf_rules_Supertree_por_clase_simplify.items():\n",
    "            if regla_cf:\n",
    "                mask_cf = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_cf), axis=1)\n",
    "                cov = float(mask_cf.mean())\n",
    "                sup = int(mask_cf.sum())\n",
    "                if sup > 0:\n",
    "                    clase_idx = self.label_encoder.transform([clase])[0]\n",
    "                    prec = float((y_bb[mask_cf.values] == clase_idx).mean())\n",
    "                else:\n",
    "                    prec = None\n",
    "            else:\n",
    "                cov, sup, prec = None, 0, None\n",
    "\n",
    "            coverage_cf_supertree[clase]  = cov\n",
    "            support_cf_supertree[clase]   = sup\n",
    "            precision_cf_supertree[clase] = prec\n",
    "\n",
    "\n",
    "        # ================= CSV por cliente =================\n",
    "        row = {\n",
    "            \"round\": int(round_number),\n",
    "            \"dataset\": DATASET_NAME,\n",
    "            \"client_id\": int(self.client_id),\n",
    "            \"bbox_pred_class\": str(pred_class),\n",
    "\n",
    "            # Vecindario\n",
    "            \"silhouette\": float(silhouette),\n",
    "\n",
    "            # Fidelity local (por si quieres mantenerlo)\n",
    "            \"fidelity_lore\": float(fidelity_Lore),\n",
    "            \"fidelity_merged\": float(fidelity_merged_Z),\n",
    "            \"fidelity_super\": float(fidelity_super_Z),\n",
    "\n",
    "            # ================= M√©tricas de test =================\n",
    "            \"acc_lore\": float(acc_lore),\n",
    "            \"acc_merged\": float(acc_merged),\n",
    "            \"acc_super\": float(acc_super),\n",
    "\n",
    "            \"prec_lore\": float(prec_lore),\n",
    "            \"prec_merged\": float(prec_merged),\n",
    "            \"prec_super\": float(prec_super),\n",
    "\n",
    "            \"rec_lore\": float(rec_lore),\n",
    "            \"rec_merged\": float(rec_merged),\n",
    "            \"rec_super\": float(rec_super),\n",
    "\n",
    "            \"f1_lore\": float(f1_lore),\n",
    "            \"f1_merged\": float(f1_merged),\n",
    "            \"f1_super\": float(f1_super),\n",
    "\n",
    "            \"auc_lore\": float(auc_lore) if auc_lore is not None else None,\n",
    "            \"auc_merged\": float(auc_merged) if auc_merged is not None else None,\n",
    "            \"auc_super\": float(auc_super) if auc_super is not None else None,\n",
    "\n",
    "            # Factual (merged / lore / supertree)\n",
    "            \"coverage_factual_merged\": self._to_float(coverage_merged),\n",
    "            \"precision_factual_merged\": self._to_float(precision_merged),\n",
    "            \"hit_factual_merged\": int(hit_merged),\n",
    "            \"support_factual_merged\": int(support_merged),\n",
    "            \"complexity_factual_merged\": int(comp_factual_merged_simpl),\n",
    "\n",
    "            \"coverage_factual_lore\": self._to_float(coverage_lore),\n",
    "            \"precision_factual_lore\": self._to_float(precision_lore),\n",
    "            \"hit_factual_lore\": int(hit_lore),\n",
    "            \"support_factual_lore\": int(support_lore),\n",
    "            \"complexity_factual_lore\": int(comp_factual_lore_simpl),\n",
    "\n",
    "            \"coverage_factual_super\": self._to_float(coverage_supertree),\n",
    "            \"precision_factual_super\": self._to_float(precision_supertree),\n",
    "            \"hit_factual_super\": int(hit_supertree),\n",
    "            \"support_factual_super\": int(support_supertree),\n",
    "            \"complexity_factual_super\": int(comp_factual_supertree_simpl),\n",
    "\n",
    "            # ================= Estructura =================\n",
    "            \"depth_edges_merged\": int(depth_merged_edges),\n",
    "            \"nodes_merged\": int(nodes_merged),\n",
    "            \"leaves_merged\": int(leaves_merged),\n",
    "            \"depth_edges_lore\": int(depth_lore_edges),\n",
    "            \"nodes_lore\": int(nodes_lore),\n",
    "            \"leaves_lore\": int(leaves_lore),\n",
    "            \"depth_edges_super\": int(depth_supertree_edges),\n",
    "            \"nodes_super\": int(nodes_supertree),\n",
    "            \"leaves_super\": int(leaves_supertree),\n",
    "        }\n",
    "\n",
    "        # (Opcional) M√©tricas contrafactuales por clase en columnas ‚Äúanchas‚Äù\n",
    "        for cl in sorted(coverage_cf_merged.keys()):\n",
    "            row[f\"cf_cov_merged_{cl}\"]  = self._to_float(coverage_cf_merged.get(cl))\n",
    "            row[f\"cf_prec_merged_{cl}\"] = self._to_float(precision_cf_merged.get(cl))\n",
    "            row[f\"cf_sup_merged_{cl}\"]  = int(support_cf_merged.get(cl, 0))\n",
    "            row[f\"cf_comp_merged_{cl}\"] = int(comp_cf_merged_simpl.get(cl) or 0)\n",
    "\n",
    "        for cl in sorted(coverage_cf_lore.keys()):\n",
    "            row[f\"cf_cov_lore_{cl}\"]  = self._to_float(coverage_cf_lore.get(cl))\n",
    "            row[f\"cf_prec_lore_{cl}\"] = self._to_float(precision_cf_lore.get(cl))\n",
    "            row[f\"cf_sup_lore_{cl}\"]  = int(support_cf_lore.get(cl, 0))\n",
    "            row[f\"cf_comp_lore_{cl}\"] = int(comp_cf_lore_simpl.get(cl) or 0)\n",
    "\n",
    "        for cl in sorted(coverage_cf_supertree.keys()):\n",
    "            row[f\"cf_cov_super_{cl}\"]  = self._to_float(coverage_cf_supertree.get(cl))\n",
    "            row[f\"cf_prec_super_{cl}\"] = self._to_float(precision_cf_supertree.get(cl))\n",
    "            row[f\"cf_sup_super_{cl}\"]  = int(support_cf_supertree.get(cl, 0))\n",
    "            row[f\"cf_comp_super_{cl}\"] = int(comp_cf_supertree_simpl.get(cl) or 0)\n",
    "\n",
    "        # Guardar\n",
    "        self._append_client_csv(row, filename=\"Balanced\")\n",
    "\n",
    "        return row\n",
    "\n",
    "    # ======================================================================\n",
    "    # Bucle sobre todo el test\n",
    "    # ======================================================================\n",
    "    def explain_all_test_instances(self, config):\n",
    "        results = []\n",
    "\n",
    "        for i in tqdm(\n",
    "            range(len(self.X_test)),\n",
    "            desc=f\"Cliente {self.client_id} explicando test\",\n",
    "        ):\n",
    "            try:\n",
    "                row = self._explain_one_instance(i, config)\n",
    "                results.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"[Cliente {self.client_id}] ‚ö†Ô∏è Error en instancia {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        mean_metrics = df.mean(numeric_only=True)\n",
    "        mean_metrics.to_csv(f\"results/metrics_cliente_{self.client_id}_balanced_mean.csv\")\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=NUM_CLIENTS)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "\n",
    "    num_idx = list(range(len(numeric_features)))\n",
    "\n",
    "    scaler_nn = StandardScaler().fit(X_train[:, num_idx])\n",
    "\n",
    "    def scale_for_nn(X):\n",
    "        Xs = X.copy().astype(np.float32)\n",
    "        Xs[:, num_idx] = scaler_nn.transform(Xs[:, num_idx])\n",
    "        return Xs\n",
    "    \n",
    "    X_train_nn = scale_for_nn(X_train)\n",
    "    X_test_nn  = scale_for_nn(X_test)\n",
    "\n",
    "    # 3) modelo NN (sin datos en el ctor)\n",
    "    input_dim, output_dim = X_train.shape[1], len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        X_train_nn=X_train_nn, \n",
    "                        X_test_nn=X_test_nn,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor,         \n",
    "                        scaler_nn_mean=scaler_nn.mean_,  \n",
    "                        scaler_nn_scale=scaler_nn.scale_,\n",
    "                        num_idx=num_idx).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# üì¶ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan din√°micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# üß† UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net est√° en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# üöÄ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se carg√≥ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] üå≤ Generando SuperTree - Ronda {server_round}\")\n",
    "            from collections import defaultdict\n",
    "\n",
    "            tree_nodes = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            feature_names = None\n",
    "            numeric_features = None\n",
    "            class_names = None\n",
    "\n",
    "            # 1) recolectar mapeos categ√≥ricos y metadatos\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                # distinct_values_* para global_mapping\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"distinct_values_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        enc = json.loads(v)\n",
    "                        client_encoders[cid] = enc\n",
    "                        for feat, d in enc.items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            # 2) recolectar √°rboles y dem√°s metadatos por cliente\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for k, v in metrics.items():\n",
    "                    if k.startswith(\"tree_ensemble_\"):\n",
    "                        cid = k.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(v)\n",
    "\n",
    "                        # lee estos una sola vez (son iguales por cliente)\n",
    "                        if feature_names is None and f\"encoded_feature_names_{cid}\" in metrics:\n",
    "                            feature_names = json.loads(metrics[f\"encoded_feature_names_{cid}\"])\n",
    "                        if numeric_features is None and f\"numeric_features_{cid}\" in metrics:\n",
    "                            numeric_features = json.loads(metrics[f\"numeric_features_{cid}\"])\n",
    "                        if class_names is None and f\"unique_labels_{cid}\" in metrics:\n",
    "                            class_names = json.loads(metrics[f\"unique_labels_{cid}\"])\n",
    "\n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "                            tree_nodes.append(root)\n",
    "\n",
    "            if not tree_nodes:\n",
    "                print(\"[SERVIDOR] ‚ö†Ô∏è No se recibieron √°rboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "\n",
    "            # 3) fusionar\n",
    "            st = SuperTree()\n",
    "            st.mergeDecisionTrees(\n",
    "                roots=tree_nodes,\n",
    "                num_classes=len(class_names),\n",
    "                feature_names=feature_names,\n",
    "                categorical_features=list(global_mapping.keys()),\n",
    "                global_mapping=global_mapping,\n",
    "            )\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            st.prune_redundant_leaves_full()\n",
    "            # print(st)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            # st.merge_equal_class_leaves()\n",
    "\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # 4) guardar/emitir\n",
    "            save_supertree_plot(\n",
    "                root_node=st.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=class_names,\n",
    "                numeric_features=numeric_features,\n",
    "                global_mapping=global_mapping,   # sin scaler\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(st.root.to_dict())\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ‚ùå Error en SuperTree: {e}\")\n",
    "\n",
    "        return aggregated_metrics\n",
    "\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# üß© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                # NO enviar global_scaler\n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUM√âRICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real  = left\n",
    "            right_real = right\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} ‚â§ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} ‚àà ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features,\n",
    "                scaler=None,  # ya no se usa\n",
    "                global_mapping=global_mapping, depth=depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEG√ìRICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEG√ìRICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACI√ìN] Error interpretando categ√≥rica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACI√ìN] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACI√ìN] Nombres num√©ricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACI√ìN] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACI√ìN] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\",\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0]); node_id[0] += 1\n",
    "\n",
    "        # etiqueta\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            label = f\"class: {class_names[class_index]}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            label = fname.split(\"_\", 1)[0] if \"_\" in fname else fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent: dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # OneHot\n",
    "            if \"_\" in fname:\n",
    "                _, val = fname.split(\"_\", 1)\n",
    "                add_node(node.children[0], curr, f'‚â† \"{val.strip()}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val.strip()}\"')\n",
    "            # Num√©rica\n",
    "            elif fname in numeric_features:\n",
    "                thr = node.intervals[0] if node.intervals else node.thresh\n",
    "                add_node(node.children[0], curr, f\"‚â§ {thr:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thr:.2f}\")\n",
    "            # Categ√≥rica ordinal\n",
    "            elif fname in global_mapping:\n",
    "                vals = global_mapping[fname]\n",
    "                val = vals[node.intervals[0]] if node.intervals else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'‚â† \"{val}\"')\n",
    "            else:\n",
    "                for ch in node.children:\n",
    "                    add_node(ch, curr, \"?\")\n",
    "\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"pdf\", cleanup=True)\n",
    "    return f\"{filename}.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# üîß INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 13:15:36,526\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-11 13:15:40,290 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 3] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 1\n",
      "[CLIENTE 4] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 3] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cliente 1 explicando test:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Cliente 1 explicando test:  20%|‚ñà‚ñà        | 1/5 [00:46<03:06, 46.67s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Cliente 1 explicando test:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:31<02:16, 45.62s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Cliente 2 explicando test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:07<00:00, 25.44s/it]\n",
      "\n",
      "Cliente 4 explicando test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:26<00:00, 29.37s/it]\n",
      "\n",
      "Cliente 1 explicando test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:56<00:00, 35.40s/it]\n",
      "\n",
      "Cliente 3 explicando test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:57<00:00, 35.40s/it]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 188.79s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar a√∫n m√°s\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesi√≥n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ab03c",
   "metadata": {},
   "source": [
    "### BALANCED METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a663325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han cargado 4 clientes (20 filas totales).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.014378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fidelity_lore</td>\n",
       "      <td>0.956870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fidelity_merged</td>\n",
       "      <td>0.532890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fidelity_super</td>\n",
       "      <td>0.581746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acc_lore</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acc_merged</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acc_super</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prec_lore</td>\n",
       "      <td>0.443833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prec_merged</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prec_super</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rec_lore</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rec_merged</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rec_super</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>f1_lore</td>\n",
       "      <td>0.466905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f1_merged</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>f1_super</td>\n",
       "      <td>0.736905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>auc_lore</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>auc_merged</td>\n",
       "      <td>0.620833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>auc_super</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>coverage_factual_merged</td>\n",
       "      <td>0.503052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     metric      mean\n",
       "0                silhouette  0.014378\n",
       "1             fidelity_lore  0.956870\n",
       "2           fidelity_merged  0.532890\n",
       "3            fidelity_super  0.581746\n",
       "4                  acc_lore  0.560000\n",
       "5                acc_merged  0.650000\n",
       "6                 acc_super  0.750000\n",
       "7                 prec_lore  0.443833\n",
       "8               prec_merged  0.633000\n",
       "9                prec_super  0.812500\n",
       "10                 rec_lore  0.560000\n",
       "11               rec_merged  0.650000\n",
       "12                rec_super  0.750000\n",
       "13                  f1_lore  0.466905\n",
       "14                f1_merged  0.590000\n",
       "15                 f1_super  0.736905\n",
       "16                 auc_lore  0.525000\n",
       "17               auc_merged  0.620833\n",
       "18                auc_super  0.750000\n",
       "19  coverage_factual_merged  0.503052"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Promedios guardados en: results\\metrics_Balanced_global.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# üìä Promedio global de m√©tricas por cliente (gen√©rico para cualquier clase)\n",
    "# ==============================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "csv_dir = Path(\"results\")\n",
    "csv_files = sorted(csv_dir.glob(\"metrics_Balanced_cliente_*.csv\"))\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Se han cargado {len(csv_files)} clientes ({len(df_all)} filas totales).\")\n",
    "\n",
    "# --- Crear medias por tipo contrafactual (promedio entre clases) ---\n",
    "cf_groups = {\n",
    "    \"cf_cov_merged\":  r\"^cf_cov_merged_\",\n",
    "    \"cf_cov_lore\":    r\"^cf_cov_lore_\",\n",
    "    \"cf_cov_super\":   r\"^cf_cov_super_\",\n",
    "    \"cf_prec_merged\": r\"^cf_prec_merged_\",\n",
    "    \"cf_prec_lore\":   r\"^cf_prec_lore_\",\n",
    "    \"cf_prec_super\":  r\"^cf_prec_super_\",\n",
    "    \"cf_sup_merged\":  r\"^cf_sup_merged_\",\n",
    "    \"cf_sup_lore\":    r\"^cf_sup_lore_\",\n",
    "    \"cf_sup_super\":   r\"^cf_sup_super_\",\n",
    "    \"cf_comp_merged\": r\"^cf_comp_merged_\",\n",
    "    \"cf_comp_lore\":   r\"^cf_comp_lore_\",\n",
    "    \"cf_comp_super\":  r\"^cf_comp_super_\",\n",
    "}\n",
    "\n",
    "for new_col, pattern in cf_groups.items():\n",
    "    cols = [c for c in df_all.columns if re.match(pattern, c)]\n",
    "    if cols:\n",
    "        # Media por fila entre clases (Yes/No/‚Ä¶)\n",
    "        df_all[new_col] = df_all[cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# --- Eliminar columnas de clases espec√≠ficas (Yes, No, etc.) ---\n",
    "df_all = df_all[[c for c in df_all.columns\n",
    "                 if not re.match(r\"^cf_(cov|prec|sup|comp)_(merged|lore|super)_[^_]+$\", c)]]\n",
    "\n",
    "# --- Medias globales (solo columnas num√©ricas limpias) ---\n",
    "numeric_cols = [\n",
    "    c for c in df_all.select_dtypes(include=[\"number\"]).columns\n",
    "    if c not in [\"round\", \"client_id\"]\n",
    "]\n",
    "means = df_all[numeric_cols].mean().to_frame(\"mean\").reset_index()\n",
    "means.rename(columns={\"index\": \"metric\"}, inplace=True)\n",
    "\n",
    "# Mostrar y guardar\n",
    "display(means.head(20))\n",
    "out_path = csv_dir / \"metrics_Balanced_global.csv\"\n",
    "means.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Promedios guardados en: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
