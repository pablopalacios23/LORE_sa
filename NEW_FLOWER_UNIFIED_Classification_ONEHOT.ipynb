{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68391f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ðŸ“¦ IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "# Otros\n",
    "from graphviz import Digraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# âš™ï¸ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "SEED = 42\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "\n",
    "# =======================\n",
    "# ðŸ”§ UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ðŸ“¥ CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name, class_col):\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_tmp = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "    df = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado estÃ¡ndar\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "    \n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado especÃ­fico para el dataset de cÃ¡ncer de mama\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].nunique() < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [col for col in df.select_dtypes(include=\"category\").columns if col != class_col]\n",
    "    num_features = [col for col in df.columns if df[col].dtype.kind in \"fi\" and col != class_col]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    ohe.fit(df[cat_features])\n",
    "    categories_global = ohe.categories_\n",
    "    onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    return cat_features, num_features, categories_global, onehot_columns\n",
    "\n",
    "\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    # Saca info global siempre al principio\n",
    "    cat_features, num_features, categories_global, onehot_columns = get_global_onehot_info(flower_dataset_name, class_col)\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado especÃ­fico por dataset\n",
    "    if \"adult\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset['SeniorCitizen'] = dataset['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado especÃ­fico para el dataset de cÃ¡ncer de mama\n",
    "        dataset.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataset[class_col])\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "    dataset[class_col] = label_encoder.transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", categories=categories_global), categorical_indices)\n",
    "    ])\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    # ReconstrucciÃ³n del DataFrame\n",
    "    num_out = X_encoded[:, :len(numeric_features)]\n",
    "    cat_out = X_encoded[:, len(numeric_features):]\n",
    "    if categorical_features:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = numeric_features\n",
    "\n",
    "    X_df = pd.DataFrame(num_out, columns=num_names)\n",
    "    if len(cat_names) > 0:\n",
    "        X_cat_df = pd.DataFrame(cat_out, columns=cat_names)\n",
    "        X_full = pd.concat([X_df.reset_index(drop=True), X_cat_df.reset_index(drop=True)], axis=1)\n",
    "        for col in onehot_columns:\n",
    "            if col not in X_cat_df.columns:\n",
    "                X_full[col] = 0\n",
    "    else:\n",
    "        X_full = X_df\n",
    "\n",
    "    # Rellenar columnas onehot que falten y ordenar\n",
    "    final_columns = num_names + list(cat_names)\n",
    "    X_full = X_full[final_columns]\n",
    "    FEATURES[:] = final_columns\n",
    "\n",
    "    split_idx = int(0.7 * len(X_full))\n",
    "\n",
    "        # --- Â¡Construye el descriptor global! ---\n",
    "    descriptor_global = descriptor.copy()\n",
    "    for i, col in enumerate(cat_features):\n",
    "        if col in descriptor_global[\"categorical\"]:\n",
    "            descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(categories_global[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    return (\n",
    "        X_full.iloc[:split_idx].to_numpy(), y[:split_idx],\n",
    "        X_full.iloc[split_idx:].to_numpy(), y[split_idx:],\n",
    "        tabular_dataset, final_columns, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# Los resultados de las mÃ©tricas no son muy buenos aqui\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "# CLASS_COLUMN = \"HeartDisease\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "# CLASS_COLUMN = \"diagnosis\" \n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "CLASS_COLUMN = \"Outcome\" \n",
    "\n",
    "\n",
    " \n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ X_train (primeras filas):\n",
      "            0         1         2         3         4         5         6  \\\n",
      "0   -0.841932 -1.027929 -0.028872 -1.280439 -0.642156 -1.663338  0.397108   \n",
      "1   -0.235362  0.171296  1.346238  0.900689  1.408755  3.052301  1.137405   \n",
      "2    1.887633 -0.488278  0.744628  1.025325 -0.642156  1.641228  1.877701   \n",
      "3   -0.841932  2.150016 -0.372650  0.152874  6.587307 -0.228144 -0.256435   \n",
      "4    0.067923 -0.548239 -0.028872  1.087643 -0.642156  0.097489 -0.988056   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "129 -0.841932 -0.967968 -0.114816  0.526782 -0.642156 -0.650260 -0.392349   \n",
      "130  0.977778 -0.338375  1.002461 -0.158716 -0.642156 -1.120618 -0.727796   \n",
      "131  1.887633 -0.758104 -0.286705 -1.280439 -0.642156 -0.734683 -0.924437   \n",
      "132 -1.145217 -0.188472 -0.157789 -1.280439 -0.642156 -0.891469  0.501212   \n",
      "133  0.067923  0.081354 -0.028872 -1.280439 -0.642156 -0.288446  0.642910   \n",
      "\n",
      "            7  \n",
      "0   -0.520733  \n",
      "1   -0.959631  \n",
      "2    0.444841  \n",
      "3    2.288210  \n",
      "4    0.005943  \n",
      "..        ...  \n",
      "129 -0.169616  \n",
      "130  1.322636  \n",
      "131 -0.169616  \n",
      "132 -0.169616  \n",
      "133  0.093723  \n",
      "\n",
      "[134 rows x 8 columns]\n",
      "\n",
      "ðŸŽ¯ y_train (primeros valores):\n",
      "[0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1\n",
      " 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "ðŸ“¦ X_test (primeras filas):\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.977778 -1.657522  0.400850 -1.280439 -0.642156  0.073368 -0.276677   \n",
      "1  -0.235362 -0.128511 -0.802371 -0.657260  0.084208 -0.300506  0.402892   \n",
      "2  -0.538647  0.860850  0.057072  1.087643  2.434211 -0.481413 -0.432834   \n",
      "3   0.674493 -3.516321 -0.028872  1.274597 -0.642156  0.845237  0.694962   \n",
      "4  -0.538647 -1.027929 -0.157789  0.464464 -0.078155  0.579907  0.411567   \n",
      "5   0.067923 -0.997949  0.916516  0.152874 -0.163610  0.905540 -0.947571   \n",
      "6   0.067923  0.321199  0.057072 -1.280439 -0.642156  0.278396 -0.531154   \n",
      "7   0.371208 -0.608201  0.314906  0.402146 -0.642156  0.435182 -0.314271   \n",
      "8  -0.538647 -1.477639 -0.286705 -0.470306 -0.513974 -1.434190 -0.664176   \n",
      "9  -1.145217 -1.177832  0.830572  0.526782 -0.300337  0.591968 -0.152331   \n",
      "10  0.977778  1.850210  1.131377  0.651417 -0.642156  0.266335 -0.933112   \n",
      "11  0.977778  0.201276  0.057072  0.776053  1.195119 -0.782925 -0.941788   \n",
      "12  2.190918  0.081354  0.486794  1.025325  0.639664  1.243232  0.862685   \n",
      "13  1.887633 -1.477639  1.604072  0.152874 -0.223428  0.423121 -0.583206   \n",
      "14 -0.235362  1.940152  0.228961 -1.280439 -0.642156 -0.179902 -0.409699   \n",
      "15 -1.145217 -0.518259  0.830572  2.458638  0.297845  1.785953  1.374531   \n",
      "16  0.067923  0.441121 -2.950981 -1.280439 -0.642156  0.109549 -0.534046   \n",
      "17 -0.235362  0.141315  0.400850 -1.280439 -0.642156 -1.084436 -0.672852   \n",
      "18  0.977778 -0.518259 -2.950981 -1.280439 -0.642156 -0.240204 -0.007742   \n",
      "19 -0.841932 -1.117871  0.228961 -0.594942 -0.129428 -0.240204  0.116605   \n",
      "20  0.977778 -0.698142 -0.200761  0.277510  0.032936  0.157791  0.726771   \n",
      "21 -0.538647 -0.758104  0.314906 -0.034080 -0.642156 -0.939711  3.502884   \n",
      "22 -1.145217 -0.728123 -0.372650  0.277510  0.144027 -0.396990  0.131064   \n",
      "23 -0.841932  1.520423  0.830572  0.526782 -0.642156  0.362819  1.209699   \n",
      "24 -0.841932  0.051373  0.744628  1.149961  1.237846  1.641228  0.929196   \n",
      "25 -0.538647  0.501082  0.057072 -1.280439 -0.642156 -0.372869  0.159982   \n",
      "26 -1.145217 -0.008588 -0.114816  0.651417  0.964391 -0.143720  0.018285   \n",
      "27 -1.145217  1.490443 -2.950981 -1.280439 -0.642156  0.037186  1.018842   \n",
      "28  0.674493  1.970133  1.088405 -1.280439 -0.642156  1.062326  2.817531   \n",
      "29  0.371208  1.220617  0.658683  1.274597  1.152391  0.893479 -0.265110   \n",
      "30  1.887633  0.141315  0.400850  0.651417 -0.642156 -0.529655  0.073228   \n",
      "31  0.371208  1.130675  0.658683  1.461551  4.015122  0.809056  0.382649   \n",
      "32  0.977778 -0.248433  0.486794  0.651417 -0.642156  0.471363  1.851675   \n",
      "33  0.067923  0.411140 -0.028872  0.028238  0.776391  0.133670 -0.944679   \n",
      "34  2.494203 -0.338375  0.486794 -1.280439 -0.642156 -1.012074 -1.011190   \n",
      "35 -0.841932 -3.516321  0.228961 -0.034080 -0.445610 -0.517595 -0.542722   \n",
      "36  0.977778 -0.368356 -2.950981 -1.280439 -0.642156 -3.858342 -0.525371   \n",
      "37  2.797488 -1.237794 -0.372650 -1.280439 -0.642156  0.097489 -0.886844   \n",
      "38 -1.145217  0.231257  1.174350 -1.280439 -0.642156 -1.144739 -0.649718   \n",
      "39 -0.841932 -0.308394 -0.802371 -0.096398 -0.642156 -0.445232 -0.883952   \n",
      "40 -0.841932 -0.008588 -0.372650  0.152874  0.263663  0.218093 -0.059794   \n",
      "41  0.977778  0.471102  0.658683 -1.280439 -0.642156  0.989963  0.605317   \n",
      "42 -0.841932 -0.878026 -0.286705  0.215192 -0.266156 -0.252265 -0.187032   \n",
      "43  0.674493  0.201276  0.143017 -1.280439 -0.642156 -0.529655 -0.343188   \n",
      "44 -0.841932 -1.207813 -0.544538  0.589099 -0.163610  0.157791  2.210256   \n",
      "45 -0.538647 -0.518259  0.057072  1.960095 -0.155065  1.026144  0.550373   \n",
      "46  0.674493  2.329900  0.057072 -1.280439 -0.642156 -0.131660 -0.458860   \n",
      "47  0.371208 -0.068549  1.260294 -1.280439 -0.642156  2.521641 -0.802982   \n",
      "48  0.067923 -1.237794 -0.286705 -1.280439 -0.642156  0.242214 -0.276677   \n",
      "49  0.674493  1.340540 -0.286705 -1.280439 -0.642156 -0.927650 -0.892627   \n",
      "50  0.371208  0.890830  0.271933 -1.280439 -0.642156 -0.252265 -0.152331   \n",
      "51 -0.538647 -0.848046  0.916516  0.589099 -0.642156  0.181912 -0.562964   \n",
      "52 -0.841932 -0.608201 -0.200761 -0.096398  0.058572 -1.663338 -0.542722   \n",
      "53  0.371208 -0.338375  0.572739  0.589099 -0.642156  0.905540 -0.580315   \n",
      "54  1.887633 -0.068549 -2.950981 -1.280439 -0.642156 -3.858342 -0.652609   \n",
      "55 -0.841932  1.400501  0.572739  1.399233 -0.069610  0.097489 -0.421267   \n",
      "56 -0.841932 -0.188472 -0.286705 -0.470306  0.913118 -0.963832 -1.008299   \n",
      "57  2.190918  0.561044  0.658683  0.900689  0.468754 -0.445232 -0.655501   \n",
      "\n",
      "           7  \n",
      "0   0.708179  \n",
      "1  -0.696292  \n",
      "2  -0.345174  \n",
      "3   0.708179  \n",
      "4  -0.784072  \n",
      "5  -0.696292  \n",
      "6  -0.784072  \n",
      "7   1.673753  \n",
      "8  -0.871851  \n",
      "9  -1.047410  \n",
      "10  2.375989  \n",
      "11  0.357061  \n",
      "12  1.322636  \n",
      "13  1.234856  \n",
      "14 -0.345174  \n",
      "15 -0.169616  \n",
      "16 -0.871851  \n",
      "17  0.620400  \n",
      "18 -0.081836  \n",
      "19 -0.959631  \n",
      "20  0.708179  \n",
      "21 -0.432954  \n",
      "22 -0.959631  \n",
      "23  1.673753  \n",
      "24 -0.345174  \n",
      "25 -0.871851  \n",
      "26 -0.959631  \n",
      "27 -0.257395  \n",
      "28  1.059297  \n",
      "29 -0.345174  \n",
      "30  1.059297  \n",
      "31  0.093723  \n",
      "32  0.883738  \n",
      "33 -0.432954  \n",
      "34  0.971518  \n",
      "35 -1.047410  \n",
      "36 -0.784072  \n",
      "37  0.708179  \n",
      "38 -1.047410  \n",
      "39 -0.345174  \n",
      "40 -0.520733  \n",
      "41  0.357061  \n",
      "42 -0.871851  \n",
      "43 -0.345174  \n",
      "44 -0.784072  \n",
      "45 -0.696292  \n",
      "46 -0.169616  \n",
      "47 -0.432954  \n",
      "48 -0.696292  \n",
      "49  1.498194  \n",
      "50 -0.432954  \n",
      "51  0.795959  \n",
      "52 -1.047410  \n",
      "53  0.444841  \n",
      "54 -0.257395  \n",
      "55  1.498194  \n",
      "56 -0.871851  \n",
      "57  0.795959  \n",
      "\n",
      "ðŸŽ¯ y_test (primeros valores):\n",
      "[0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1]\n",
      "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=3, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\nðŸ“¦ X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\nðŸŽ¯ y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nðŸ“¦ X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\nðŸŽ¯ y_test (primeros valores):\")\n",
    "print(y_test)\n",
    "\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸŒ¼ CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] âœ… Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self._train_nn()\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                \n",
    "                # print(\"supertree_dict\")\n",
    "                # print(\"supertree_dict:\", supertree_dict)\n",
    "                # print(\"type:\", type(supertree_dict))\n",
    "                # print(\"dir(supertree_dict):\", dir(supertree_dict))\n",
    "                # print(\"\\n\")\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "                self.global_scaler = json.loads(config[\"global_scaler\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] âŒ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(export_text(self.tree_model, feature_names=FEATURES))\n",
    "        # print(\"root_node:\", root_node)\n",
    "        # print(\"type:\", type(root_node))\n",
    "        # print(dir(root_node))\n",
    "        # print(\"\\n\")\n",
    "        # print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "        \n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None and config.get(\"server_round\", 0) == NUM_SERVER_ROUNDS:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"scaler_mean_{self.client_id}\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            f\"scaler_std_{self.client_id}\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"encoder_descriptor_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "\n",
    "        \n",
    "    \n",
    "        num_row = 0\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            self.scaler,\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "        # print(f\"\\n[CLIENTE {self.client_id}] ðŸ§ª Instancia a explicar (decodificada):\")\n",
    "        # print(decoded)\n",
    "        # print(f\"[CLIENTE {self.client_id}] ðŸ§ª Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "\n",
    "        # AsegÃºrate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        x_tensor = torch.tensor(self.X_test[num_row], dtype=torch.float32).unsqueeze(0)  # shape: [1, n_features]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.nn_model(x_tensor)   # shape: [1, n_classes]\n",
    "            probs = torch.softmax(logits, dim=1).numpy()\n",
    "            pred_class_idx = int(probs.argmax(axis=1)[0])\n",
    "\n",
    "        # Si tienes un label_encoder:\n",
    "        pred_class = self.label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        # Ahora crea el TabularDataset legible\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")    \n",
    "\n",
    "        # Explicabilidad local y la vecindad es generada del train (local_tabular_dataset)\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        \n",
    "        # ExplicaciÃ³n LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        \n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        \n",
    "        # self.print_tree_readable(node=lore_tree.root,feature_names=self.feature_names,class_names=UNIQUE_LABELS,  numeric_features=self.numeric_features,scaler=self.scaler,encoder=self.encoder)\n",
    "        # print('\\n')\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        self.save_lore_tree_image(lore_tree.root, round_number, self.feature_names, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder, folder=\"LoreTree\")\n",
    "\n",
    "\n",
    "        merged_tree = SuperTree()\n",
    "        merged_tree.mergeDecisionTrees(\n",
    "            roots=[lore_tree.root, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()), \n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "        merged_tree.merge_equal_class_leaves()\n",
    "\n",
    "        self.save_supertree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=self.scaler,global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "        \n",
    "        tree_str = self.tree_to_str(merged_tree.root, self.feature_names, numeric_features=self.numeric_features, scaler=self.scaler, global_mapping=self.global_mapping, unique_labels=self.unique_labels)\n",
    "\n",
    "        rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"âˆ§\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 âˆ§ â‰¤ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)\\s*âˆ§\\s*([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # EvalÃºa las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('â‰¤','<=').replace('â‰¥','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('â‰¤','<=').replace('â‰¥','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu cÃ³digo tal cual ...\n",
    "                if \"â‰¤\" in cond:\n",
    "                    var, val = cond.split(\"â‰¤\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"â‰¥\" in cond:\n",
    "                    var, val = cond.replace(\"â‰¥\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"â‰ \" in cond:\n",
    "                    var, val = cond.split(\"â‰ \")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "        # Buscar la regla factual (la que cubre la instancia)\n",
    "        regla_factual = None\n",
    "        for regla in rules:\n",
    "            if cumple_regla(decoded, regla):\n",
    "                regla_factual = regla\n",
    "                break\n",
    "\n",
    "        \n",
    "\n",
    "        # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "        cf_rules_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la mÃ¡s sencilla (menos condiciones)\n",
    "                    cf_rules_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================\n",
    "        # ðŸ“ MÃ‰TRICAS DE EXPLICACIÃ“N tipo LORE \n",
    "        # ========================================\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sintÃ©tico generado alrededor del punto a explicar.\n",
    "        y_bb = explanation[\"neighborhood_Yb\"] # predicciones del modelo BBOX (red neuronal) sobre Z (el vecindario).\n",
    "\n",
    "        y_surrogate_preds = explanation[\"surrogate_preds\"]  # predicciones del modelo interpretable (arbol) sobre Z (el vecindario).\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        # ==============================================================================================\n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ==============================================================================================\n",
    "\n",
    "        mask_same_class = (y_bb == pred_class_idx)\n",
    "        mask_diff_class = (y_bb != pred_class_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "        # Fidelity: Porcentaje de veces que el modelo interpretable (LORE tree) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Un valor alto de fidelity significa que el Ã¡rbol surrogate estÃ¡ imitando bien a la red neuronal para esa instancia.\n",
    "        # ===========================================================================================================================================================\n",
    "\n",
    "        fidelity = accuracy_score(y_bb, y_surrogate_preds)\n",
    "\n",
    "\n",
    "        # ====================================================================================================================================================================================================================================================\n",
    "        # Coverage: mide cuÃ¡ntas instancias del vecindario ð‘ (generado alrededor de la instancia a explicar) cumplen la regla factual ð‘. Es decir, calcula la proporciÃ³n de instancias en las que la regla es aplicable.\n",
    "\n",
    "        # PrecisiÃ³n: proporciÃ³n de las instancias del vecindario que cumplen la regla factual (es decir, de las instancias que cumplen el coverage) ademÃ¡s tienen que cumplir que el modelo black-box (tu red neuronal) predice la clase de la regla factual.\n",
    "        # =====================================================================================================================================================================================================================================================\n",
    "\n",
    "        # Decodifica cada fila del vecindario a un formato legible\n",
    "        dfZ_decoded = dfZ.apply(lambda row: self.decode_onehot_instance(\n",
    "            row.values, self.numeric_features, self.encoder, self.scaler, self.feature_names\n",
    "        ), axis=1)\n",
    "\n",
    "        cf_rules_por_clase_simplify = self._simplify_rules_by_class(cf_rules_por_clase, mode='loose')\n",
    "        \n",
    "        if regla_factual:\n",
    "            regla_factual_simplify = self._simplify_rule(regla_factual, mode='loose')\n",
    "            cumplen_regla = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual), axis=1)\n",
    "            coverage = cumplen_regla.mean()\n",
    "\n",
    "            \n",
    "            covered_target_match = (y_bb[cumplen_regla.values] == pred_class_idx)\n",
    "\n",
    "            if cumplen_regla.sum() > 0:\n",
    "                precision = covered_target_match.sum() / cumplen_regla.sum()\n",
    "            else:\n",
    "                precision = 0.0\n",
    "        else:\n",
    "            coverage = \"Ninguna regla factual cubre la instancia. No hay una regla en el Ã¡rbol que explique la predicciÃ³n sobre esa muestra\"\n",
    "            precision = \"Si no hay regla factual, no se puede calcular la precisiÃ³n (nÃºmero de aciertos entre las instancias cubiertas)\"\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"\\n[CLIENTE {self.client_id}] ðŸ§ª Instancia a explicar (decodificada):\\n{decoded}\\n\"\n",
    "            f\"ðŸ§ª Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\\n\"\n",
    "            f\"ðŸ§ª Clase predicha: {repr(pred_class)}\\n\"\n",
    "            # f\"{'Regla factual: ' + str(regla_factual) if regla_factual else 'Ninguna regla cubre la instancia. No hay explicaciÃ³n factual disponible para esta predicciÃ³n.'}\\n\"\n",
    "            f\"{'Regla factual simplificada: ' + str(regla_factual_simplify) if regla_factual_simplify else 'Ninguna regla cubre la instancia. No hay explicaciÃ³n factual disponible para esta predicciÃ³n.'}\\n\"\n",
    "            # f\"Contrafactuales por clase: {cf_rules_por_clase}\\n\"\n",
    "            f\"Contrafactuales simplificado: {cf_rules_por_clase_simplify}\\n\"\n",
    "            f\"MÃ©tricas explicabilidad:\\n\"\n",
    "            f\"  - Silhouette: {silhouette:.3f}\\n\"\n",
    "            f\"  - Fidelity:   {fidelity:.3f}\\n\"\n",
    "            f\"  - Coverage:   {coverage}\\n\"\n",
    "            f\"  - Precision:  {precision}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def extract_rules_from_str(self, tree_str, target_class_label, exclude=False):\n",
    "        target_class_label = target_class_label.strip()\n",
    "        lines = tree_str.strip().split(\"\\n\")\n",
    "        path = []\n",
    "        rules = []\n",
    "\n",
    "        def recurse(idx, indent_level):\n",
    "            seen = set()\n",
    "            while idx < len(lines):\n",
    "                line = lines[idx]\n",
    "                current_indent = len(line) - len(line.lstrip())\n",
    "                if current_indent < indent_level:\n",
    "                    return idx\n",
    "                if \"â®•\" in line:\n",
    "                    import re\n",
    "                    m = re.search(r'class = \"([^\"]+)\"', line)\n",
    "                    leaf_class = m.group(1).strip() if m else None\n",
    "                    condition = (leaf_class == target_class_label)\n",
    "                    if exclude:\n",
    "                        condition = not condition  # cambia la lÃ³gica\n",
    "                    if condition:\n",
    "                        cleaned = []\n",
    "                        for cond in path:\n",
    "                            if cond not in seen:\n",
    "                                cleaned.append(cond)\n",
    "                                seen.add(cond)\n",
    "                        rules.append(cleaned)\n",
    "                    return idx + 1\n",
    "                elif \"if\" in line:\n",
    "                    condition = line.strip()[3:]\n",
    "                    path.append(condition)\n",
    "                    idx = recurse(idx + 1, current_indent + 2)\n",
    "                    path.pop()\n",
    "                else:\n",
    "                    idx += 1\n",
    "            return idx\n",
    "\n",
    "        recurse(0, 0)\n",
    "        return rules\n",
    "    \n",
    "\n",
    "    def _simplify_rule(self, regla, mode='tight'):\n",
    "        \"\"\"\n",
    "        mode:\n",
    "        - 'tight' (por defecto): mantiene la regla equivalente (lb = max lowers, ub = min uppers).\n",
    "        - 'loose': presenta una banda mÃ¡s ancha (lb = min lowers, ub = max uppers).\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        # Estado por variable\n",
    "        bounds = {}   # var -> dict(lb, lb_inc, ub, ub_inc)\n",
    "        cat_eq = {}   # var -> set(values)\n",
    "        cat_neq = {}  # var -> set(values)\n",
    "        others = []   # condiciones que dejamos tal cual (p.ej. ya venÃ­an como \"a âˆ§ b\")\n",
    "\n",
    "        def ensure_num(var):\n",
    "            if var not in bounds:\n",
    "                bounds[var] = {\"lb\": None, \"lb_inc\": False, \"ub\": None, \"ub_inc\": True}\n",
    "\n",
    "        # Estrategia de agregaciÃ³n segÃºn modo\n",
    "        if mode == 'tight':\n",
    "            # mÃ¡s restrictivo: lb = mÃ¡ximo, ub = mÃ­nimo\n",
    "            def upd_lower(d, v, inc):\n",
    "                if d[\"lb\"] is None or (v > d[\"lb\"]) or (v == d[\"lb\"] and d[\"lb_inc\"] and not inc):\n",
    "                    d[\"lb\"] = v; d[\"lb_inc\"] = inc\n",
    "            def upd_upper(d, v, inc):\n",
    "                if d[\"ub\"] is None or (v < d[\"ub\"]) or (v == d[\"ub\"] and d[\"ub_inc\"] and not inc):\n",
    "                    d[\"ub\"] = v; d[\"ub_inc\"] = inc\n",
    "        else:  # 'loose'\n",
    "            # menos restrictivo: lb = mÃ­nimo, ub = mÃ¡ximo\n",
    "            def upd_lower(d, v, inc):\n",
    "                if d[\"lb\"] is None or (v < d[\"lb\"]) or (v == d[\"lb\"] and not d[\"lb_inc\"] and inc):\n",
    "                    d[\"lb\"] = v; d[\"lb_inc\"] = inc\n",
    "            def upd_upper(d, v, inc):\n",
    "                if d[\"ub\"] is None or (v > d[\"ub\"]) or (v == d[\"ub\"] and not d[\"ub_inc\"] and inc):\n",
    "                    d[\"ub\"] = v; d[\"ub_inc\"] = inc\n",
    "\n",
    "        # Regex\n",
    "        r_le  = re.compile(r'^\\s*(.+?)\\s*â‰¤\\s*([\\-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "        r_lt  = re.compile(r'^\\s*(.+?)\\s*<\\s*([\\-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "        r_ge  = re.compile(r'^\\s*(.+?)\\s*â‰¥\\s*([\\-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "        r_ge2 = re.compile(r'^\\s*(.+?)\\s*>=\\s*([\\-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "        r_gt  = re.compile(r'^\\s*(.+?)\\s*>\\s*([\\-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "        r_eq  = re.compile(r'^\\s*(.+?)\\s*=\\s*\"?([^\"]+?)\"?\\s*$')\n",
    "        r_neq = re.compile(r'^\\s*(.+?)\\s*â‰ \\s*\"?([^\"]+?)\"?\\s*$')\n",
    "\n",
    "        simplified_out = []\n",
    "\n",
    "        for cond in regla:\n",
    "            c = cond.strip()\n",
    "            # deja intervalos ya compactos tal cual\n",
    "            if \"âˆ§\" in c:\n",
    "                others.append(c); continue\n",
    "\n",
    "            m = r_le.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), float(m.group(2))\n",
    "                if var in self.numeric_features:\n",
    "                    ensure_num(var); upd_upper(bounds[var], val, inc=True); continue\n",
    "\n",
    "            m = r_lt.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), float(m.group(2))\n",
    "                if var in self.numeric_features:\n",
    "                    ensure_num(var); upd_upper(bounds[var], val, inc=False); continue\n",
    "\n",
    "            m = r_ge.match(c) or r_ge2.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), float(m.group(2))\n",
    "                if var in self.numeric_features:\n",
    "                    ensure_num(var); upd_lower(bounds[var], val, inc=True); continue\n",
    "\n",
    "            m = r_gt.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), float(m.group(2))\n",
    "                if var in self.numeric_features:\n",
    "                    ensure_num(var); upd_lower(bounds[var], val, inc=False); continue\n",
    "\n",
    "            m = r_eq.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), m.group(2).strip()\n",
    "                if var not in self.numeric_features:\n",
    "                    cat_eq.setdefault(var, set()).add(val); continue\n",
    "\n",
    "            m = r_neq.match(c)\n",
    "            if m:\n",
    "                var, val = m.group(1).strip(), m.group(2).strip()\n",
    "                if var not in self.numeric_features:\n",
    "                    cat_neq.setdefault(var, set()).add(val); continue\n",
    "\n",
    "            # si no encaja, se mantiene\n",
    "            others.append(c)\n",
    "\n",
    "        # construir numÃ©ricas\n",
    "        for var, d in bounds.items():\n",
    "            lb, li = d[\"lb\"], d[\"lb_inc\"]\n",
    "            ub, ui = d[\"ub\"], d[\"ub_inc\"]\n",
    "\n",
    "            # evitar intervalos imposibles\n",
    "            if lb is not None and ub is not None:\n",
    "                if (lb > ub) or (lb == ub and (not li or not ui)):\n",
    "                    continue\n",
    "\n",
    "            if lb is not None and ub is not None:\n",
    "                op_lb = \"â‰¥\" if li else \">\"\n",
    "                op_ub = \"â‰¤\" if ui else \"<\"\n",
    "                simplified_out.append(f\"{var} {op_lb} {lb:.2f} âˆ§ {op_ub} {ub:.2f}\")\n",
    "            elif lb is not None:\n",
    "                op_lb = \"â‰¥\" if li else \">\"\n",
    "                simplified_out.append(f\"{var} {op_lb} {lb:.2f}\")\n",
    "            elif ub is not None:\n",
    "                op_ub = \"â‰¤\" if ui else \"<\"\n",
    "                simplified_out.append(f\"{var} {op_ub} {ub:.2f}\")\n",
    "\n",
    "        # categÃ³ricas (dedupe)\n",
    "        for var, vals in cat_eq.items():\n",
    "            for v in sorted(vals):\n",
    "                simplified_out.append(f'{var} = \"{v}\"')\n",
    "        for var, vals in cat_neq.items():\n",
    "            for v in sorted(vals):\n",
    "                simplified_out.append(f'{var} â‰  \"{v}\"')\n",
    "\n",
    "        simplified_out.extend(others)\n",
    "\n",
    "        def _key(c):\n",
    "            var = c.split()[0]\n",
    "            return (0 if var in self.numeric_features else 1, var)\n",
    "        simplified_out.sort(key=_key)\n",
    "        return simplified_out\n",
    "\n",
    "\n",
    "    def _simplify_rules_by_class(self, cf_rules_por_clase, mode='tight'):\n",
    "        return {clase: self._simplify_rule(regla, mode=mode)\n",
    "                for clase, regla in cf_rules_por_clase.items()}\n",
    "    \n",
    "\n",
    "\n",
    "    def decode_onehot_instance(self, X_row, numeric_features, encoder, scaler, feature_names):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        x_named = pd.Series(X_row, index=feature_names)\n",
    "        data = {}\n",
    "\n",
    "        # NumÃ©ricas\n",
    "        for i, col in enumerate(numeric_features):\n",
    "            if col in x_named:\n",
    "                val = x_named[col]\n",
    "                idx = numeric_features.index(col)\n",
    "                mean = scaler.mean_[idx]\n",
    "                std = scaler.scale_[idx]\n",
    "                data[col] = val * std + mean\n",
    "            else:\n",
    "                data[col] = None\n",
    "\n",
    "        # CategÃ³ricas\n",
    "        cat_map = encoder.dataset_descriptor[\"categorical\"]\n",
    "        for cat in cat_map:\n",
    "            onehot_names = [c for c in feature_names if c.startswith(cat + \"_\")]\n",
    "            val_found = None\n",
    "            for c in onehot_names:\n",
    "                if c in x_named and x_named[c] == 1:\n",
    "                    val_found = c[len(cat) + 1 :]\n",
    "                    break\n",
    "            if val_found is not None:\n",
    "                data[cat] = val_found.strip()\n",
    "            else:\n",
    "                data[cat] = None  # O \"?\"\n",
    "\n",
    "        return pd.Series(data)\n",
    "    \n",
    "    def decode_Xtrain_to_df(self, X_test, numeric_features, encoder, scaler, feature_names):\n",
    "        # Lista de diccionarios para cada fila\n",
    "        decoded_rows = []\n",
    "        for x in X_test:\n",
    "            decoded = self.decode_onehot_instance(x, numeric_features, encoder, scaler, feature_names)\n",
    "            decoded_rows.append(decoded)\n",
    "        df = pd.DataFrame(decoded_rows)\n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def print_tree_readable(self, node, feature_names, class_names, numeric_features, scaler, encoder, depth=0):\n",
    "        indent = \"|   \" * depth\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_idx = int(np.argmax(node.labels))\n",
    "            print(f\"{indent}|--- class: {class_names[class_idx]}\")\n",
    "            return\n",
    "\n",
    "        feat_name = feature_names[node.feat]\n",
    "\n",
    "        # --- CASO NUMÃ‰RICA ---\n",
    "        if feat_name in numeric_features:\n",
    "            idx = numeric_features.index(feat_name)\n",
    "            threshold = node.thresh * scaler.scale_[idx] + scaler.mean_[idx]\n",
    "            print(f\"{indent}|--- {feat_name} <= {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            print(f\"{indent}|--- {feat_name} > {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            return\n",
    "\n",
    "        # --- CASO CATEGÃ“RICA ONE-HOT ---\n",
    "        if \"=\" in feat_name:\n",
    "            # Ejemplo: occupation= Adm-clerical\n",
    "            var, valor = feat_name.split(\"=\")\n",
    "            var = var.strip()\n",
    "            valor = valor.strip()\n",
    "            # Si threshold == 0.5, OneHot tÃ­pico: <= 0.5 (no es ese valor), > 0.5 (es ese valor)\n",
    "            if node.thresh == 0.5:\n",
    "                print(f\"{indent}|--- {var} == \\\"{valor}\\\"\")\n",
    "                self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "                print(f\"{indent}|--- {var} != \\\"{valor}\\\"\")\n",
    "                self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            else:\n",
    "                # Por si hay rarezas (poco frecuente)\n",
    "                print(f\"{indent}|--- {feat_name} <= {node.thresh:.2f}\")\n",
    "                self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "                print(f\"{indent}|--- {feat_name} > {node.thresh:.2f}\")\n",
    "                self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            return\n",
    "\n",
    "        # --- SI NO ENCAJA ---\n",
    "        print(f\"{indent}|--- {feat_name} <= {node.thresh:.2f}\")\n",
    "        self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "        print(f\"{indent}|--- {feat_name} > {node.thresh:.2f}\")\n",
    "        self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tree_to_str(self, node, feature_names, numeric_features=None, scaler=None, global_mapping=None, unique_labels=None, depth=0):\n",
    "        indent = \"  \" * depth\n",
    "        result = \"\"\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_idx = int(np.argmax(node.labels))\n",
    "            class_label = unique_labels[class_idx] if unique_labels is not None else str(class_idx)\n",
    "            result += f'{indent}â®• Leaf: class = \"{class_label.strip()}\" | {node.labels}\\n'\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "\n",
    "            # --- Split OneHot ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                for i, child in enumerate(node.children):\n",
    "                    cond = f'{var} {\"â‰ \" if i == 0 else \"=\"} \"{val}\"'\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "\n",
    "            # --- Split categÃ³rico ordinal ---\n",
    "            elif global_mapping and fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                for i, child in enumerate(node.children):\n",
    "                    val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                    val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                    cond = f'{fname} {\"â‰ \" if i == 0 else \"=\"} \"{val}\"'\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "\n",
    "            # --- Split numÃ©rico robusto ---\n",
    "            elif numeric_features and fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler.mean_[idx]\n",
    "                std = scaler.scale_[idx]\n",
    "                # bounds siempre de tamaÃ±o len(children)+1 si es correcto\n",
    "                bounds = [-np.inf] + list(getattr(node, \"intervals\", []))\n",
    "                for i, child in enumerate(node.children):\n",
    "                    left = bounds[i]\n",
    "                    # Si hay suficientes bounds, usa el siguiente, si no, pon np.inf\n",
    "                    if i + 1 < len(bounds):\n",
    "                        right = bounds[i + 1]\n",
    "                    else:\n",
    "                        right = np.inf\n",
    "                    left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "                    right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "                    if i == 0:\n",
    "                        cond = f\"{fname} â‰¤ {right_real:.2f}\"\n",
    "                    elif i == len(node.children) - 1:\n",
    "                        cond = f\"{fname} > {left_real:.2f}\"\n",
    "                    else:\n",
    "                        cond = f\"{fname} âˆˆ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "            else:\n",
    "                # Por si acaso, caso no detectado\n",
    "                for child in node.children:\n",
    "                    result += f\"{indent}if {fname} ?\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def save_supertree_plot(self, root_node, round_number, feature_names, class_names, numeric_features, scaler, global_mapping, folder=\"Supertree\"):\n",
    "\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1\n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = int(np.argmax(node.labels))\n",
    "                class_label = class_names[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                fname = feature_names[node.feat]\n",
    "                if \"_\" in fname:\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    label = var.strip()\n",
    "                else:\n",
    "                    label = fname\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Nodos hijos (solo binario)\n",
    "            if not node.is_leaf:\n",
    "                fname = feature_names[node.feat]\n",
    "                if \"_\" in fname:  # OneHotEncoder\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    left_label = f'â‰  \"{val}\"'\n",
    "                    right_label = f'= \"{val}\"'\n",
    "                    add_node(node.children[0], curr, left_label)\n",
    "                    add_node(node.children[1], curr, right_label)\n",
    "                elif fname in numeric_features:\n",
    "                    idx = numeric_features.index(fname)\n",
    "                    mean = scaler.mean_[idx]\n",
    "                    std = scaler.scale_[idx]\n",
    "                    threshold = node.intervals[0]\n",
    "                    thresh_real = threshold * std + mean if np.isfinite(threshold) else threshold\n",
    "                    add_node(node.children[0], curr, f\"â‰¤ {thresh_real:.2f}\")\n",
    "                    add_node(node.children[1], curr, f\"> {thresh_real:.2f}\")\n",
    "                elif fname in global_mapping:\n",
    "                    vals_cat = global_mapping[fname]\n",
    "                    val = vals_cat[node.intervals[0]] if node.intervals and len(node.intervals) > 0 else \"?\"\n",
    "                    add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                    add_node(node.children[1], curr, f'â‰  \"{val}\"')\n",
    "                else:\n",
    "                    for child in node.children:\n",
    "                        add_node(child, curr, \"?\")\n",
    "\n",
    "        folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        filename = f\"{folder_path}/LoreTree_cliente{self.client_id}_Supertree_ronda_{round_number}\"\n",
    "        add_node(root_node)\n",
    "        dot.render(filename, format=\"png\", cleanup=True)\n",
    "        return f\"{filename}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_lore_tree_image(self, root_node, round_number, feature_names, numeric_features, scaler, unique_labels, encoder, tree_type=\"LoreTree\", folder=\"LoreTree\"):\n",
    "\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            match = re.match(r\"([a-zA-Z0-9\\- ]+)\", feat)\n",
    "            return match.group(1).strip() if match else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1 \n",
    "\n",
    "            if node.is_leaf:\n",
    "                class_index = int(np.argmax(node.labels))\n",
    "                class_label = unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Ãrbol binario\n",
    "            if not node.is_leaf:\n",
    "                fname = feature_names[node.feat]\n",
    "                if \"_\" in fname or \"=\" in fname:\n",
    "                    if \"_\" in fname:\n",
    "                        var, val = fname.split(\"_\", 1)\n",
    "                    else:\n",
    "                        var, val = fname.split(\"=\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    left_label = f'â‰  \"{val}\"'\n",
    "                    right_label = f'= \"{val}\"'\n",
    "                elif base_name(fname) in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                    val_idx = int(node.thresh)\n",
    "                    vals_cat = encoder.dataset_descriptor[\"categorical\"][base_name(fname)][\"distinct_values\"]\n",
    "                    val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                    left_label = f'= \"{val}\"'\n",
    "                    right_label = f'â‰  \"{val}\"'\n",
    "                elif fname in numeric_features:\n",
    "                    idx = numeric_features.index(fname)\n",
    "                    mean = scaler.mean_[idx]\n",
    "                    std = scaler.scale_[idx]\n",
    "                    thresh = node.thresh * std + mean\n",
    "                    left_label = f\"<= {thresh:.2f}\"\n",
    "                    right_label = f\"> {thresh:.2f}\"\n",
    "                else:\n",
    "                    left_label = \"â‰¤ ?\"\n",
    "                    right_label = \"> ?\"\n",
    "\n",
    "                add_node(node.children[0], curr, left_label)\n",
    "                add_node(node.children[1], curr, right_label)\n",
    "\n",
    "        folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        filename = f\"{folder_path}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        add_node(root_node)\n",
    "        dot.render(filename, format=\"png\", cleanup=True)\n",
    "        return f\"{filename}.png\"\n",
    "            \n",
    "\n",
    "    \n",
    "    def _save_local_tree(self, root_node, round_number, feature_names, numeric_features, scaler, unique_labels, encoder, tree_type= \"LocalTree\"):\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            # Extrae solo el nombre de la variable, antes de '_' o '=' o espacios\n",
    "            match = re.match(r\"([a-zA-Z0-9\\- ]+)\", feat)\n",
    "            return match.group(1).strip() if match else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1 \n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = np.argmax(node.labels)\n",
    "                class_label = unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Ãrbol tipo SuperTree\n",
    "            if hasattr(node, \"children\") and node.children is not None and hasattr(node, \"intervals\"):\n",
    "                for i, child in enumerate(node.children):\n",
    "                    try:\n",
    "                        fname = feature_names[node.feat]\n",
    "                    except:\n",
    "                        fname = f\"X_{node.feat}\"\n",
    "\n",
    "                    original_feat = base_name(fname)\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val_idx = int(val_idx)\n",
    "                        vals_cat = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]\n",
    "                        val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                        edge = f'= \"{val}\"' if i == 0 else f'â‰  \"{val}\"'\n",
    "                    elif original_feat in numeric_features:\n",
    "                        idx = numeric_features.index(original_feat)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val = val * std + mean\n",
    "                        edge = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "                    else:\n",
    "                        edge = \"?\"\n",
    "\n",
    "                    add_node(child, curr, edge)\n",
    "\n",
    "            elif hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                except:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                # Si es OneHot\n",
    "                if \"_\" in fname:\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    # La split es: Si sex_ Male <= 0.5  (NO es Male)\n",
    "                    #              Si sex_ Male > 0.5   (SÃ es Male)\n",
    "                    left_label = f'â‰  \"{val}\"'   # <= 0.5 â†’ no es ese valor\n",
    "                    right_label = f'= \"{val}\"'  # > 0.5  â†’ sÃ­ es ese valor\n",
    "                else:\n",
    "                    original_feat = base_name(fname)\n",
    "\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = int(node.thresh)\n",
    "                        vals_cat = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]\n",
    "                        val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                        left_label = f'= \"{val}\"'\n",
    "                        right_label = f'â‰  \"{val}\"'\n",
    "\n",
    "                    elif fname in numeric_features:\n",
    "                        idx = numeric_features.index(fname)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        thresh = node.thresh * std + mean\n",
    "                        left_label = f\"<= {thresh:.2f}\"\n",
    "                        right_label = f\"> {thresh:.2f}\"\n",
    "                        \n",
    "                    else:\n",
    "                        left_label = \"â‰¤ ?\"\n",
    "                        right_label = \"> ?\"\n",
    "\n",
    "                if node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        add_node(root_node)\n",
    "        folder = f\"Ronda_{round_number}/{tree_type}_Cliente_{self.client_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        filepath = f\"{folder}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        dot.render(filepath, format=\"png\", cleanup=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=num_partitions)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ“¦ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# âš™ï¸ CONFIGURACIÃ“N GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinÃ¡micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ§  UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net estÃ¡ en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# ðŸš€ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargÃ³ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"distinct_values_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        client_encoders[client_id] = json.loads(value)\n",
    "                        for feat, d in client_encoders[client_id].items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            all_means = []\n",
    "            all_stds = []\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"tree_ensemble_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(value)\n",
    "                        local_encoder = client_encoders[client_id]\n",
    "                        feature_names = json.loads(metrics.get(f\"encoded_feature_names_{client_id}\"))\n",
    "                        numeric_features = json.loads(metrics.get(f\"numeric_features_{client_id}\"))\n",
    "                        unique_labels = json.loads(metrics.get(f\"unique_labels_{client_id}\"))\n",
    "                        scaler = {\n",
    "                            \"mean\": json.loads(metrics.get(f\"scaler_mean_{client_id}\")),\n",
    "                            \"std\": json.loads(metrics.get(f\"scaler_std_{client_id}\")),\n",
    "                        }\n",
    "\n",
    "                        # Guarda los scalers de cada cliente\n",
    "                        all_means.append(scaler[\"mean\"])\n",
    "                        all_stds.append(scaler[\"std\"])\n",
    "                        \n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "\n",
    "                            # print(\"Local tree del cliente\", client_id)\n",
    "                            # print(\"root:\", root)\n",
    "                            # print(\"type:\", type(root))\n",
    "                            # print(\"dir(root):\", dir(root))\n",
    "                            # print(\"\\n\")\n",
    "\n",
    "                            tree_dicts.append(root)\n",
    "\n",
    "                # Calcular el scaler promedio\n",
    "                global_mean = np.mean(np.stack(all_means), axis=0)\n",
    "                global_std = np.mean(np.stack(all_stds), axis=0)\n",
    "                global_scaler = {\"mean\": global_mean, \"std\": global_std}\n",
    "\n",
    "\n",
    "                            \n",
    "            # print(tree_dicts)\n",
    "            \n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] âš ï¸ No se recibieron Ã¡rboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "            \n",
    "            supertree = SuperTree()\n",
    "            roots = tree_dicts\n",
    "            \n",
    "            supertree.mergeDecisionTrees(roots, num_classes=len(UNIQUE_LABELS), feature_names=feature_names, categorical_features=list(global_mapping.keys()), global_mapping=global_mapping)\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            supertree.merge_equal_class_leaves()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"supertree.root.to_dict(): \", supertree.root.to_dict())\n",
    "            # print(\"type:\", type(supertree.root))\n",
    "            # print(\"dir(supertree.root): \", dir(supertree.root))\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] ðŸŒ³ SuperTree legible (nombre de variables):\")\n",
    "            # print_supertree_legible_fusionado(\n",
    "            #     supertree.root,\n",
    "            #     feature_names=feature_names,\n",
    "            #     class_names=UNIQUE_LABELS,\n",
    "            #     numeric_features=numeric_features,\n",
    "            #     scaler=global_scaler,  # <-- ahora el scaler promedio\n",
    "            #     global_mapping=global_mapping\n",
    "            # )\n",
    "            \n",
    "            \n",
    "\n",
    "            save_supertree_plot(\n",
    "                root_node=supertree.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=UNIQUE_LABELS,\n",
    "                numeric_features=numeric_features,\n",
    "                scaler=global_scaler,\n",
    "                global_mapping=global_mapping\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "      \n",
    "            global_scaler = {\n",
    "                \"mean\": global_mean.tolist(),\n",
    "                \"std\": global_std.tolist()\n",
    "            }\n",
    "\n",
    "            GLOBAL_SCALER_JSON = json.dumps(global_scaler)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] âŒ Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# ðŸ§© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            \n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                ins.config[\"global_scaler\"] = GLOBAL_SCALER_JSON\n",
    "                \n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUMÃ‰RICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        idx = numeric_features.index(feat_name)\n",
    "        mean = scaler[\"mean\"][idx]\n",
    "        std = scaler[\"std\"][idx]\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "\n",
    "        # Robusto: asegura que bounds tiene len(children)+1\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        if len(bounds) != len(children) + 1:\n",
    "            print(f\"[DEPURACIÃ“N] NUMÃ‰RICA '{feat_name}' mal construida\")\n",
    "            print(f\"    intervals: {intervals}\")\n",
    "            print(f\"    children: {len(children)}\")\n",
    "            print(f\"    bounds: {bounds}\")\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "            right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} â‰¤ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} âˆˆ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACIÃ“N] Error interpretando categÃ³rica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres numÃ©ricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACIÃ“N] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACIÃ“N] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,           # dict con mean y std\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\"\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        # Etiqueta del nodo\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            class_label = class_names[class_index]\n",
    "            label = f\"class: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            if \"_\" in fname:  # OneHotEncoder\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                label = var.strip()\n",
    "            else:\n",
    "                label = fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        # Nodos hijos (binario siempre)\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # --- Caso OneHotEncoder ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                # Solo dos hijos: [â‰ val, =val]\n",
    "                add_node(node.children[0], curr, f'â‰  \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val}\"')\n",
    "            # --- Caso numÃ©rica ---\n",
    "            elif fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler[\"mean\"][idx]\n",
    "                std = scaler[\"std\"][idx]\n",
    "                # Solo dos hijos y un threshold\n",
    "                threshold = node.intervals[0]\n",
    "                thresh_real = threshold * std + mean if np.isfinite(threshold) else threshold\n",
    "                add_node(node.children[0], curr, f\"â‰¤ {thresh_real:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thresh_real:.2f}\")\n",
    "            # --- Caso categÃ³rica ordinal ---\n",
    "            elif fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                # Binario: solo dos hijos, dividir por primer valor\n",
    "                val = vals_cat[node.intervals[0]] if node.intervals and len(node.intervals) > 0 else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'â‰  \"{val}\"')\n",
    "            else:\n",
    "                # Caso raro/desconocido\n",
    "                for child in node.children:\n",
    "                    add_node(child, curr, \"?\")\n",
    "\n",
    "    # --- Guardado ---\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "    return f\"{filename}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ”§ INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:33:59,600\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] âœ… Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] âœ… Red neuronal entrenada\n",
      "[CLIENTE 3] âœ… Red neuronal entrenada\n",
      "[CLIENTE 4] âœ… Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] âœ… Red neuronal entrenada\n",
      "[CLIENTE 1] âœ… Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] âœ… Red neuronal entrenada\n",
      "[CLIENTE 3] âœ… Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "\n",
      "[CLIENTE 2] ðŸ§ª Instancia a explicar (decodificada):\n",
      "Pregnancies                 4.440892e-16\n",
      "Glucose                     1.400000e+02\n",
      "BloodPressure               6.500000e+01\n",
      "SkinThickness               2.600000e+01\n",
      "Insulin                     1.300000e+02\n",
      "BMI                         4.260000e+01\n",
      "DiabetesPedigreeFunction    4.310000e-01\n",
      "Age                         2.400000e+01\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: Yes\n",
      "ðŸ§ª Clase predicha: 'No'\n",
      "Regla factual simplificada: ['Age â‰¤ 25.46', 'Pregnancies â‰¤ 3.78']\n",
      "Contrafactuales simplificado: {'Yes': ['Age > 25.46', 'BMI > 29.29', 'BloodPressure > 57.42', 'Glucose > 109.31']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.775\n",
      "  - Fidelity:   0.983\n",
      "  - Coverage:   0.4180602006688963\n",
      "  - Precision:  0.992\n",
      "\n",
      "\n",
      "[CLIENTE 4] ðŸ§ª Instancia a explicar (decodificada):\n",
      "Pregnancies                  7.000\n",
      "Glucose                     62.000\n",
      "BloodPressure               78.000\n",
      "SkinThickness                0.000\n",
      "Insulin                      0.000\n",
      "BMI                         32.600\n",
      "DiabetesPedigreeFunction     0.391\n",
      "Age                         41.000\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: No\n",
      "ðŸ§ª Clase predicha: 'No'\n",
      "Regla factual simplificada: ['Glucose â‰¤ 68.68']\n",
      "Contrafactuales simplificado: {'Yes': ['Age > 33.55', 'BMI > 9.16 âˆ§ â‰¤ 37.15', 'BloodPressure > 79.40', 'Glucose > 68.68 âˆ§ â‰¤ 102.65', 'Insulin > 211.14 âˆ§ â‰¤ 276.41', 'SkinThickness > 8.06']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.828\n",
      "  - Fidelity:   0.983\n",
      "  - Coverage:   0.46153846153846156\n",
      "  - Precision:  0.9855072463768116\n",
      "\n",
      "\n",
      "[CLIENTE 3] ðŸ§ª Instancia a explicar (decodificada):\n",
      "Pregnancies                  2.000\n",
      "Glucose                     96.000\n",
      "BloodPressure               68.000\n",
      "SkinThickness               13.000\n",
      "Insulin                     49.000\n",
      "BMI                         21.100\n",
      "DiabetesPedigreeFunction     0.647\n",
      "Age                         26.000\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: No\n",
      "ðŸ§ª Clase predicha: 'No'\n",
      "Regla factual simplificada: ['Glucose â‰¤ 106.50', 'Pregnancies â‰¤ 3.26']\n",
      "Contrafactuales simplificado: {'Yes': ['Age > 33.97', 'BMI â‰¤ 36.15', 'BloodPressure > 77.09', 'Glucose â‰¤ 106.50', 'Insulin > 224.00 âˆ§ â‰¤ 712.37', 'Pregnancies > 3.26', 'SkinThickness > 37.88']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.894\n",
      "  - Fidelity:   0.993\n",
      "  - Coverage:   0.451505016722408\n",
      "  - Precision:  1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLIENTE 1] ðŸ§ª Instancia a explicar (decodificada):\n",
      "Pregnancies                   6.000\n",
      "Glucose                     123.000\n",
      "BloodPressure                72.000\n",
      "SkinThickness                45.000\n",
      "Insulin                     230.000\n",
      "BMI                          33.600\n",
      "DiabetesPedigreeFunction      0.733\n",
      "Age                          34.000\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: No\n",
      "ðŸ§ª Clase predicha: 'Yes'\n",
      "Regla factual simplificada: ['Age > 29.99 âˆ§ â‰¤ 35.49', 'BMI > 32.82 âˆ§ â‰¤ 37.04', 'DiabetesPedigreeFunction > 0.68 âˆ§ â‰¤ 1.81', 'Glucose > 109.23 âˆ§ â‰¤ 135.65', 'SkinThickness > 41.02']\n",
      "Contrafactuales simplificado: {'No': ['BMI > 32.82', 'DiabetesPedigreeFunction > 1.81']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.833\n",
      "  - Fidelity:   0.977\n",
      "  - Coverage:   0.411371237458194\n",
      "  - Precision:  0.991869918699187\n",
      "\n",
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 70.61s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar aÃºn mÃ¡s\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesiÃ³n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
