{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68391f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 📦 IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "from lore_sa.client_utils import ClientUtilsMixin  \n",
    "\n",
    "# Otros\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ⚙️ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "SEED = 42\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "\n",
    "# =======================\n",
    "# 🔧 UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 📥 CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name, class_col):\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_tmp = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "    df = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado estándar\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "    \n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado específico para el dataset de cáncer de mama\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].nunique() < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [col for col in df.select_dtypes(include=\"category\").columns if col != class_col]\n",
    "    num_features = [col for col in df.columns if df[col].dtype.kind in \"fi\" and col != class_col]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    ohe.fit(df[cat_features])\n",
    "    categories_global = ohe.categories_\n",
    "    onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    return cat_features, num_features, categories_global, onehot_columns\n",
    "\n",
    "\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    # Saca info global siempre al principio\n",
    "    cat_features, num_features, categories_global, onehot_columns = get_global_onehot_info(flower_dataset_name, class_col)\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado específico por dataset\n",
    "    if \"adult\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset['SeniorCitizen'] = dataset['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado específico para el dataset de cáncer de mama\n",
    "        dataset.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataset[class_col])\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "    dataset[class_col] = label_encoder.transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", categories=categories_global), categorical_indices)\n",
    "    ])\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    # Reconstrucción del DataFrame\n",
    "    num_out = X_encoded[:, :len(numeric_features)]\n",
    "    cat_out = X_encoded[:, len(numeric_features):]\n",
    "    if categorical_features:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = numeric_features\n",
    "\n",
    "    X_df = pd.DataFrame(num_out, columns=num_names)\n",
    "    if len(cat_names) > 0:\n",
    "        X_cat_df = pd.DataFrame(cat_out, columns=cat_names)\n",
    "        X_full = pd.concat([X_df.reset_index(drop=True), X_cat_df.reset_index(drop=True)], axis=1)\n",
    "        for col in onehot_columns:\n",
    "            if col not in X_cat_df.columns:\n",
    "                X_full[col] = 0\n",
    "    else:\n",
    "        X_full = X_df\n",
    "\n",
    "    # Rellenar columnas onehot que falten y ordenar\n",
    "    final_columns = num_names + list(cat_names)\n",
    "    X_full = X_full[final_columns]\n",
    "    FEATURES[:] = final_columns\n",
    "\n",
    "    split_idx = int(0.7 * len(X_full))\n",
    "\n",
    "        # --- ¡Construye el descriptor global! ---\n",
    "    descriptor_global = descriptor.copy()\n",
    "    for i, col in enumerate(cat_features):\n",
    "        if col in descriptor_global[\"categorical\"]:\n",
    "            descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(categories_global[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    return (\n",
    "        X_full.iloc[:split_idx].to_numpy(), y[:split_idx],\n",
    "        X_full.iloc[split_idx:].to_numpy(), y[split_idx:],\n",
    "        tabular_dataset, final_columns, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# Los resultados de las métricas no son muy buenos aqui\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "# CLASS_COLUMN = \"HeartDisease\" \n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "CLASS_COLUMN = \"diagnosis\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "# CLASS_COLUMN = \"Outcome\" \n",
    "\n",
    "\n",
    " \n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 X_train (primeras filas):\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   2.099024  0.741360  2.136455  2.149266  0.527598  1.713305  2.013530   \n",
      "1  -0.559923 -1.356307 -0.540982 -0.561735  0.547262 -0.203641 -0.782879   \n",
      "2  -0.232293 -0.464616 -0.289432 -0.306538 -1.237521 -0.932314 -0.870036   \n",
      "3  -1.314013  0.692634 -1.306638 -1.049409  0.403063 -0.843833 -0.887364   \n",
      "4  -0.248539 -0.730174 -0.249734 -0.392132  0.625915  0.912587  0.359608   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "94 -1.376831 -1.217437 -1.322360 -1.076619 -1.379098 -0.348496 -0.377739   \n",
      "95  0.211768 -0.515778  0.231355  0.055386  0.037979  0.585802 -0.153638   \n",
      "96  1.709118 -0.264838  1.574397  1.737148 -0.885546 -0.518004 -0.028333   \n",
      "97  1.302965  1.450328  1.393596  1.150671 -0.487690  2.164307  1.572571   \n",
      "98  0.682905 -0.013897  0.646806  0.575554 -0.629266 -0.266131 -0.062989   \n",
      "\n",
      "          7         8         9   ...        20        21        22        23  \\\n",
      "0   2.545812  0.058015 -0.207988  ...  2.366036  0.114366  2.627625  2.441073   \n",
      "1  -0.574083  0.270806  0.277841  ... -0.602518 -1.201770 -0.465384 -0.592495   \n",
      "2  -0.936544 -0.603159 -0.852335  ... -0.314370 -0.560998 -0.394342 -0.379110   \n",
      "3  -0.992110  0.821784  0.468650  ... -1.118699  1.010459 -1.141476 -0.927635   \n",
      "4   0.481917  0.506396  0.910446  ... -0.148528 -0.315560 -0.307778 -0.466773   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "94 -0.754670  2.022535  0.999979  ... -1.292832 -0.967863 -1.235203 -1.016560   \n",
      "95  0.264543 -0.534761  0.862010  ...  0.106452 -0.770196  0.081460 -0.030263   \n",
      "96  0.489634  0.012416 -0.902239  ...  1.812542 -0.279321  1.547072  1.952253   \n",
      "97  1.246970  1.904740  0.089968  ...  1.625971  1.328374  2.102273  1.402105   \n",
      "98  0.376188  0.320204 -1.097451  ...  0.788473  0.196728  0.708300  0.696834   \n",
      "\n",
      "          24        25        26        27        28        29  \n",
      "0  -0.154042  0.930173  1.091214  2.055999 -0.255063  0.165904  \n",
      "1   0.463868  0.071635 -0.886179 -0.549170  0.937976  0.563782  \n",
      "2  -0.530364 -0.528681 -0.690514 -0.742947 -0.255063 -0.425843  \n",
      "3   0.923816 -0.847661 -1.084417 -1.262430 -0.040381  0.004844  \n",
      "4   0.970275  1.528507  1.144792  0.886878  1.033026  1.111982  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "94 -1.596142 -0.527360 -0.673180 -1.049760  0.631522  0.115796  \n",
      "95 -0.892747 -0.198474 -0.432080 -0.227339 -1.179340  0.150991  \n",
      "96 -0.414215 -0.403203 -0.137926  1.132329 -0.240313  0.361562  \n",
      "97 -0.372402  3.280585  2.397045  2.154502  2.906162  1.243217  \n",
      "98 -0.739431 -0.236118 -0.189403  0.509013 -0.141986 -0.637011  \n",
      "\n",
      "[99 rows x 30 columns]\n",
      "\n",
      "🎯 y_train (primeros valores):\n",
      "[1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1]\n",
      "\n",
      "📦 X_test (primeras filas):\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0  -0.197093 -1.928841 -0.193922 -0.288574  2.022018 -0.019992 -0.262779   \n",
      "1  -0.735922 -1.982440 -0.751263 -0.685106 -0.804926 -0.880143 -0.975297   \n",
      "2  -0.906506 -1.585321 -0.931671 -0.805044 -0.856707 -1.118638 -0.992238   \n",
      "3  -0.465154 -0.618103 -0.455691 -0.507578  1.150273  0.052627 -0.385498   \n",
      "4  -0.543677  0.872921 -0.576357 -0.533468 -1.070382 -1.027100 -0.930426   \n",
      "5  -0.156478 -0.615667 -0.199817 -0.245513 -0.359222 -0.634385 -0.925512   \n",
      "6   0.041184  0.212680  0.125233 -0.046058 -0.700055  1.182041  0.609183   \n",
      "7   3.750712 -0.094296  3.759739  4.835963  1.045402  0.876278  2.987263   \n",
      "8  -1.759427  1.384548 -1.765717 -1.287698 -2.990188 -1.187244 -1.152069   \n",
      "9   1.072812  0.003157  1.016270  0.947254 -0.051817  0.113780  0.309176   \n",
      "10  0.122414  0.665835  0.106366 -0.030472  1.012629  0.587713  0.519957   \n",
      "11  1.703702  0.487984  1.766990  1.689596  0.416172  1.302436  1.544122   \n",
      "12  2.028625  0.519656  2.140385  2.173042 -0.277947  1.726683  1.686367   \n",
      "13  0.076383  2.115442  0.152746 -0.025452  1.025738  1.027248  0.967382   \n",
      "14  1.603518  0.166390  1.609772  1.536372  0.986411  0.824680  1.606192   \n",
      "15  2.391454  0.237043  2.329048  2.677624 -0.316618  0.415721  1.015229   \n",
      "16 -0.765707 -0.844681 -0.777597 -0.709675 -0.121950 -0.704329 -0.699083   \n",
      "17 -0.370385 -1.397724 -0.430536 -0.402964 -1.881171 -1.308212 -0.859820   \n",
      "18  1.443765  1.496618  1.370013  1.380507  0.324410 -0.134462  0.988073   \n",
      "19 -0.086078 -1.385543 -0.099590 -0.196112  1.740176 -0.156630  0.153999   \n",
      "20  1.427518  0.010466  1.440761  1.449193  1.097837  1.117066  1.688953   \n",
      "21 -0.364969 -0.615667 -0.405381 -0.401643 -1.234900 -0.995377 -0.651237   \n",
      "22  0.506905 -0.238038  0.556405  0.366589  0.376845  0.958452  0.598838   \n",
      "23 -0.627615  0.463620 -0.588541 -0.600833 -0.799683 -0.088789 -0.301961   \n",
      "24 -1.532794 -0.062624 -1.523207 -1.181498  0.599697 -0.903457 -1.152069   \n",
      "25 -1.082506 -0.554759 -1.060984 -0.918376 -0.621401 -0.479401 -0.588520   \n",
      "26 -0.814445 -0.993296 -0.843629 -0.735565 -0.995661 -1.168898 -1.139478   \n",
      "27  0.390475  0.353986  0.328438  0.259332 -0.149479 -0.339324 -0.299504   \n",
      "28 -0.207924  0.838813 -0.233226 -0.283819 -0.703332 -0.549727 -0.537700   \n",
      "29 -0.373092 -0.155204 -0.329916 -0.463196  1.543542  1.153376 -0.528131   \n",
      "30 -0.562630  0.405149 -0.614089 -0.554074 -1.187707 -1.249161 -0.843915   \n",
      "31 -0.895676  0.607363 -0.823190 -0.785494  1.602532  0.306793  0.197966   \n",
      "32 -1.333508  0.751106 -1.329828 -1.068430 -0.383474 -0.949704 -0.635332   \n",
      "33  1.608933  1.942464  1.605841  1.494103  0.052399  1.168664  0.969969   \n",
      "34 -0.892968 -0.301382 -0.866818 -0.800024 -0.090489 -0.157203 -0.471491   \n",
      "35 -0.641153 -0.364727 -0.654180 -0.615099  1.589423 -0.642985 -0.385368   \n",
      "36 -1.020229 -0.060187 -1.021286 -0.882712  0.560371 -0.744651 -0.855036   \n",
      "37 -1.014814  0.324751 -0.951324 -0.882976  0.914312  0.469230 -0.314634   \n",
      "38 -0.059001 -0.700938 -0.120422 -0.151201 -0.896033 -0.894284 -0.695075   \n",
      "39  0.972627 -1.370925  0.855121  0.855055 -1.302411 -0.832749 -0.577270   \n",
      "40 -0.216047 -0.026079 -0.212395 -0.309973  0.606252  0.153911 -0.608822   \n",
      "41  0.501490 -0.196621  0.481726  0.381911 -0.068203  0.152000  0.069557   \n",
      "42 -1.238739  0.117664 -1.249646 -1.011103 -0.874404 -1.059588 -0.849863   \n",
      "\n",
      "          7         8         9   ...        20        21        22        23  \\\n",
      "0   0.371043  2.258125  0.527360  ... -0.449115 -2.078096 -0.465981 -0.483729   \n",
      "1  -1.086339  0.092213 -0.266698  ... -0.809820 -2.015501 -0.768356 -0.732107   \n",
      "2  -0.963298 -1.366928 -0.367974  ... -1.010902 -1.694291 -1.031331 -0.868471   \n",
      "3  -0.471956  0.510196  0.484795  ... -0.554839 -0.610414 -0.605975 -0.580230   \n",
      "4  -0.991081 -1.005942 -0.448701  ... -0.430458  1.134001 -0.424788 -0.452343   \n",
      "5  -0.831845 -0.675356 -0.185972  ... -0.082192 -0.808082 -0.160321 -0.180877   \n",
      "6   0.382877 -0.359969  1.336097  ... -0.098776 -0.490167 -0.157336 -0.191880   \n",
      "7   2.787624 -0.610758 -1.110661  ...  2.459322 -1.093053  2.433603  2.931696   \n",
      "8  -1.315469 -0.842549 -0.583735  ... -1.407677  0.867150 -1.427136 -1.091416   \n",
      "9   0.729646 -0.006583 -0.906642  ...  1.159543 -0.017412  1.012765  1.102681   \n",
      "10  0.774150  0.829383  0.699088  ...  0.336557  0.921509  0.260557  0.201701   \n",
      "11  2.085339  1.209368 -0.045066  ...  1.669504  0.061655  1.588861  1.687100   \n",
      "12  1.483381 -0.333370 -0.128729  ...  2.475906  0.506407  2.654490  2.724264   \n",
      "13  0.578899  1.878141  1.167304  ...  0.251563  1.980677  0.511293  0.125402   \n",
      "14  1.923273 -0.321970 -0.335683  ...  1.198930 -0.005881  1.164998  1.091858   \n",
      "15  1.264720 -1.154136 -1.170839  ...  3.014890 -0.203549  3.120143  3.442161   \n",
      "16 -0.575884 -0.283972 -0.611622  ... -0.813966 -0.508286 -0.807161 -0.742568   \n",
      "17 -1.014233 -1.792510 -1.028466  ... -0.544474 -1.575691 -0.584185 -0.535136   \n",
      "18  1.015962 -0.553761 -1.307341  ...  1.076622  0.964337  0.979931  0.987240   \n",
      "19  0.386221  0.635591  0.257292  ... -0.337173 -1.602047 -0.371656 -0.397509   \n",
      "20  1.416496 -0.063581  0.396730  ...  2.007405  0.186845  1.914221  2.127218   \n",
      "21 -0.703993  0.077014 -0.902239  ... -0.490576 -0.511581 -0.482398 -0.492567   \n",
      "22  0.678711  0.719188  0.342422  ...  0.821642  0.354862  0.767999  0.698638   \n",
      "23 -0.340245 -0.838749  0.710830  ... -0.623248  0.359804 -0.440907 -0.590872   \n",
      "24 -1.315469  1.346162  1.581213  ... -1.512157 -0.439103 -1.502058 -1.142823   \n",
      "25 -0.688301 -0.530962  0.634507  ... -1.122845 -0.504992 -1.071329 -0.931242   \n",
      "26 -1.239813 -0.135778 -0.294586  ... -0.722753 -0.501697 -0.742984 -0.687193   \n",
      "27  0.019386 -0.724754 -1.075435  ...  0.800912  1.270721  0.666511  0.713068   \n",
      "28 -0.444945  0.001017 -0.827383  ... -0.227303  1.105998 -0.237930 -0.315437   \n",
      "29  0.048713 -0.378968  1.425630  ... -0.248033 -0.121187 -0.228677 -0.358908   \n",
      "30 -0.860142 -0.264972 -0.887561  ... -0.602518  1.274016 -0.668361 -0.587806   \n",
      "31  0.142865  0.327804  0.863478  ... -0.706169  1.685823 -0.589558 -0.648773   \n",
      "32 -0.985679 -0.443565  0.431956  ... -1.327036 -0.009176 -1.316692 -1.042354   \n",
      "33  1.938708  0.251807 -0.384119  ...  1.646701  1.433797  1.582891  1.438181   \n",
      "34 -0.598264 -0.720954  0.185372  ... -0.959077  0.231320 -0.911634 -0.840874   \n",
      "35  0.589189  0.783785 -0.596945  ... -0.797382 -1.007397 -0.821190 -0.723809   \n",
      "36 -0.857569 -1.241533  0.467182  ... -1.075165 -0.093184 -1.080881 -0.898774   \n",
      "37 -0.526235  0.430399  2.202076  ... -1.054435 -0.388039 -1.025361 -0.900578   \n",
      "38 -0.558134 -0.359969 -0.563186  ... -0.121579 -0.580764 -0.172261 -0.223085   \n",
      "39 -0.240690 -2.237092 -1.524570  ...  0.740794 -1.101289  0.600842  0.606646   \n",
      "40 -0.504626 -0.325770  0.051806  ... -0.185843  0.008944 -0.151367 -0.277558   \n",
      "41  0.213351  0.198609 -0.049470  ...  0.761524  0.027063  0.657556  0.648133   \n",
      "42 -1.068126 -0.869148 -0.068551  ... -1.102115  0.079775 -1.130431 -0.918976   \n",
      "\n",
      "          24        25        26        27        28        29  \n",
      "0   0.575370 -0.724824 -0.958457 -0.675125 -0.305865 -0.659082  \n",
      "1  -1.092522 -0.561702 -0.969750 -1.109831  0.808512 -0.514128  \n",
      "2  -1.839590 -1.139828 -1.255499 -1.355443 -1.235058 -0.985974  \n",
      "3   1.174697 -0.479150 -0.414746 -0.512998  0.423396 -0.452090  \n",
      "4  -0.330588 -0.444148 -0.760901 -0.848232  0.333263 -0.055405  \n",
      "5  -0.293421 -0.602647 -0.773508 -0.465684 -0.148541 -0.306539  \n",
      "6  -1.527847 -0.002992 -0.240880 -0.222494 -1.023655  0.300120  \n",
      "7  -0.860225 -0.634347  0.274413  0.704405 -2.046259 -1.652882  \n",
      "8  -1.986402 -1.209964 -1.406988 -1.871212 -0.042020 -0.749753  \n",
      "9   0.496389  0.189189  0.616892  0.996685 -0.138709 -0.058387  \n",
      "10  0.189757  1.197640  0.477169  1.090344  1.313259  1.577267  \n",
      "11 -0.274837  0.434862  0.921551  1.597393  0.295571 -0.433598  \n",
      "12 -0.256253  1.484920  1.643277  1.101648 -0.104294  0.335315  \n",
      "13  1.629999  2.708006  2.283585  0.893337  2.165429  3.050669  \n",
      "14  1.569602  0.307403  1.381165  1.637764  0.221825 -0.131759  \n",
      "15 -0.595407  0.758465  0.585901  1.784711  0.018615 -0.607781  \n",
      "16 -0.637220 -0.547173 -0.672130 -0.502502 -0.361584 -0.913199  \n",
      "17 -1.698353 -0.878040 -0.784013 -1.004707 -0.963019 -0.823721  \n",
      "18  0.933108 -0.417071  0.682026  0.496095 -0.989240 -1.315253  \n",
      "19  0.310552 -0.734730 -0.588086 -0.232183 -1.207199 -0.622098  \n",
      "20  0.779792  1.082728  1.808213  1.326106  0.116943  0.974187  \n",
      "21 -1.069293 -0.640951 -0.480405 -0.541419  0.208715 -0.803439  \n",
      "22  0.552141  2.197507  1.363306  1.138788  2.250646  2.406428  \n",
      "23 -0.981020  0.222209 -0.164715 -0.006110 -0.707368  1.005206  \n",
      "24  0.092192 -1.122591 -1.406988 -1.871212  0.402092 -0.107302  \n",
      "25  0.621830 -0.152245 -0.470425 -0.525594 -0.337002  0.705753  \n",
      "26 -0.748723 -0.988725 -1.378003 -1.602024 -0.133792 -0.523672  \n",
      "27  0.403471  0.706953  0.117357  0.617205  0.885535  0.199905  \n",
      "28 -0.767307 -0.505567 -0.208313  0.198970 -0.042020 -0.822528  \n",
      "29  0.598600  0.319950 -0.752497 -0.101384 -0.515630  0.356193  \n",
      "30 -0.948498 -1.050276 -0.805549 -0.671249 -0.332086 -0.845792  \n",
      "31  2.703211  0.146262  0.706188  0.428273  0.110387  0.781511  \n",
      "32 -1.041417 -1.087523 -0.987136 -1.457176 -0.756532 -0.538585  \n",
      "33 -0.228377  0.620439  0.538101  1.603853  0.613496 -0.396017  \n",
      "34  0.468514  0.022104 -0.291832 -0.278851 -0.995795 -0.010666  \n",
      "35  0.147944 -1.082900 -1.031259 -0.426606 -1.108872 -1.349851  \n",
      "36  0.398825 -0.946063 -0.964550 -0.817066 -1.120343 -0.270748  \n",
      "37 -0.126167 -0.282347 -0.726758 -0.880367 -0.841749  0.435530  \n",
      "38 -0.613991 -0.808697 -0.820257 -0.667697 -0.281283 -0.633432  \n",
      "39 -1.584063 -0.976706 -0.856501 -0.524302 -1.825023 -1.446487  \n",
      "40  1.379118  0.427598 -0.012911  0.433117  1.018277  0.220187  \n",
      "41 -0.061124 -0.158849  0.064829  0.092393 -0.171484 -0.082248  \n",
      "42 -0.595407 -1.005104 -0.915857 -1.250158 -0.564794 -0.224220  \n",
      "\n",
      "[43 rows x 30 columns]\n",
      "\n",
      "🎯 y_test (primeros valores):\n",
      "[0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0]\n",
      "['radiusMEAN', 'textureMEAN', 'perimeterMEAN', 'areaMEAN', 'smoothnessMEAN', 'compactnessMEAN', 'concavityMEAN', 'concave pointsMEAN', 'symmetryMEAN', 'fractaldimensionMEAN', 'radiusSE', 'textureSE', 'perimeterSE', 'areaSE', 'smoothnessSE', 'compactnessSE', 'concavitySE', 'concave pointsSE', 'symmetrySE', 'fractalDimensionSE', 'radiusWORST', 'textureWORST', 'perimeterWORST', 'areaWORST', 'smoothnessWORST', 'compactnessWORST', 'concavityWORST', 'concavePointsWORST', 'symmetryWORST', 'fractalDimensionWORST']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=3, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\n📦 X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\n🎯 y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\n📦 X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\n🎯 y_test (primeros valores):\")\n",
    "print(y_test)\n",
    "\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 🌼 CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient, ClientUtilsMixin):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ✅ Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self._train_nn()\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                \n",
    "                # print(\"supertree_dict\")\n",
    "                # print(\"supertree_dict:\", supertree_dict)\n",
    "                # print(\"type:\", type(supertree_dict))\n",
    "                # print(\"dir(supertree_dict):\", dir(supertree_dict))\n",
    "                # print(\"\\n\")\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "                self.global_scaler = json.loads(config[\"global_scaler\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ❌ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(export_text(self.tree_model, feature_names=FEATURES))\n",
    "        # print(\"root_node:\", root_node)\n",
    "        # print(\"type:\", type(root_node))\n",
    "        # print(dir(root_node))\n",
    "        # print(\"\\n\")\n",
    "        # print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "        \n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None and config.get(\"server_round\", 0) == NUM_SERVER_ROUNDS:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"scaler_mean_{self.client_id}\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            f\"scaler_std_{self.client_id}\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"encoder_descriptor_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "\n",
    "        \n",
    "    \n",
    "        num_row = 0\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            self.scaler,\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "        # print(f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (decodificada):\")\n",
    "        # print(decoded)\n",
    "        # print(f\"[CLIENTE {self.client_id}] 🧪 Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "\n",
    "        # Asegúrate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        x_tensor = torch.tensor(self.X_test[num_row], dtype=torch.float32).unsqueeze(0)  # shape: [1, n_features]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.nn_model(x_tensor)   # shape: [1, n_classes]\n",
    "            probs = torch.softmax(logits, dim=1).numpy()\n",
    "            pred_class_idx = int(probs.argmax(axis=1)[0])\n",
    "\n",
    "        # Si tienes un label_encoder:\n",
    "        pred_class = self.label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        # Ahora crea el TabularDataset legible\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")    \n",
    "\n",
    "        # Explicabilidad local y la vecindad es generada del train (local_tabular_dataset)\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        \n",
    "        # Explicación LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        \n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        \n",
    "        # self.print_tree_readable(node=lore_tree.root,feature_names=self.feature_names,class_names=UNIQUE_LABELS,  numeric_features=self.numeric_features,scaler=self.scaler,encoder=self.encoder)\n",
    "        # print('\\n')\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        self.save_lore_tree_image(lore_tree.root, round_number, self.feature_names, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder, folder=\"LoreTree\")\n",
    "\n",
    "\n",
    "        merged_tree = SuperTree()\n",
    "        merged_tree.mergeDecisionTrees(\n",
    "            roots=[lore_tree.root, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()), \n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "        merged_tree.merge_equal_class_leaves()\n",
    "\n",
    "        self.save_supertree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=self.scaler,global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "        \n",
    "        tree_str = self.tree_to_str(merged_tree.root, self.feature_names, numeric_features=self.numeric_features, scaler=self.scaler, global_mapping=self.global_mapping, unique_labels=self.unique_labels)\n",
    "\n",
    "        rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"∧\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 ∧ ≤ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|≤|≥)\\s*([-\\d\\.]+)\\s*∧\\s*([><]=?|≤|≥)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # Evalúa las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('≤','<=').replace('≥','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('≤','<=').replace('≥','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu código tal cual ...\n",
    "                if \"≤\" in cond:\n",
    "                    var, val = cond.split(\"≤\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"≥\" in cond:\n",
    "                    var, val = cond.replace(\"≥\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"≠\" in cond:\n",
    "                    var, val = cond.split(\"≠\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "        # Buscar la regla factual (la que cubre la instancia)\n",
    "        regla_factual = None\n",
    "        for regla in rules:\n",
    "            if cumple_regla(decoded, regla):\n",
    "                regla_factual = regla\n",
    "                break\n",
    "\n",
    "        \n",
    "\n",
    "        # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "        cf_rules_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la más sencilla (menos condiciones)\n",
    "                    cf_rules_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================\n",
    "        # 📏 MÉTRICAS DE EXPLICACIÓN tipo LORE \n",
    "        # ========================================\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sintético generado alrededor del punto a explicar.\n",
    "        y_bb = explanation[\"neighborhood_Yb\"] # predicciones del modelo BBOX (red neuronal) sobre Z (el vecindario).\n",
    "\n",
    "        y_surrogate_preds = explanation[\"surrogate_preds\"]  # predicciones del modelo interpretable (arbol) sobre Z (el vecindario).\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        # ==============================================================================================\n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ==============================================================================================\n",
    "\n",
    "        mask_same_class = (y_bb == pred_class_idx)\n",
    "        mask_diff_class = (y_bb != pred_class_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "        # Fidelity: Porcentaje de veces que el modelo interpretable (LORE tree) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Un valor alto de fidelity significa que el árbol surrogate está imitando bien a la red neuronal para esa instancia.\n",
    "        # ===========================================================================================================================================================\n",
    "\n",
    "        fidelity = accuracy_score(y_bb, y_surrogate_preds)\n",
    "\n",
    "\n",
    "        # ====================================================================================================================================================================================================================================================\n",
    "        # Coverage: mide cuántas instancias del vecindario 𝑍 (generado alrededor de la instancia a explicar) cumplen la regla factual 𝑝. Es decir, calcula la proporción de instancias en las que la regla es aplicable.\n",
    "\n",
    "        # Precisión: proporción de las instancias del vecindario que cumplen la regla factual (es decir, de las instancias que cumplen el coverage) además tienen que cumplir que el modelo black-box (tu red neuronal) predice la clase de la regla factual.\n",
    "        # =====================================================================================================================================================================================================================================================\n",
    "\n",
    "        # Decodifica cada fila del vecindario a un formato legible\n",
    "        dfZ_decoded = dfZ.apply(lambda row: self.decode_onehot_instance(\n",
    "            row.values, self.numeric_features, self.encoder, self.scaler, self.feature_names\n",
    "        ), axis=1)\n",
    "\n",
    "        cf_rules_por_clase_simplify = self._simplify_rules_by_class(cf_rules_por_clase, mode='loose')\n",
    "\n",
    "        regla_factual_simplify = None\n",
    "        \n",
    "        if regla_factual:\n",
    "            regla_factual_simplify = self._simplify_rule(regla_factual, mode='loose')\n",
    "            cumplen_regla = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual), axis=1)\n",
    "            coverage = cumplen_regla.mean()\n",
    "\n",
    "            \n",
    "            covered_target_match = (y_bb[cumplen_regla.values] == pred_class_idx)\n",
    "\n",
    "            if cumplen_regla.sum() > 0:\n",
    "                precision = covered_target_match.sum() / cumplen_regla.sum()\n",
    "            else:\n",
    "                precision = 0.0\n",
    "        else:\n",
    "            coverage = \"Ninguna regla factual cubre la instancia. No hay una regla en el árbol que explique la predicción sobre esa muestra\"\n",
    "            precision = \"Si no hay regla factual, no se puede calcular la precisión (número de aciertos entre las instancias cubiertas)\"\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (decodificada):\\n{decoded}\\n\"\n",
    "            f\"🧪 Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\\n\"\n",
    "            f\"🧪 Clase predicha: {repr(pred_class)}\\n\"\n",
    "            # f\"{'Regla factual: ' + str(regla_factual) if regla_factual else 'Ninguna regla cubre la instancia. No hay explicación factual disponible para esta predicción.'}\\n\"\n",
    "            f\"{'Regla factual simplificada: ' + str(regla_factual_simplify) if regla_factual_simplify is not None else 'Ninguna regla cubre la instancia. No hay explicación factual disponible para esta predicción.'}\\n\"\n",
    "            # f\"Contrafactuales por clase: {cf_rules_por_clase}\\n\"\n",
    "            f\"Contrafactuales simplificado: {cf_rules_por_clase_simplify}\\n\"\n",
    "            f\"Métricas explicabilidad:\\n\"\n",
    "            f\"  - Silhouette: {silhouette:.3f}\\n\"\n",
    "            f\"  - Fidelity:   {fidelity:.3f}\\n\"\n",
    "            f\"  - Coverage:   {coverage}\\n\"\n",
    "            f\"  - Precision:  {precision}\\n\"\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=num_partitions)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📦 IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ⚙️ CONFIGURACIÓN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinámicamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 🧠 UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net está en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# 🚀 SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargó antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] 🌲 Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"distinct_values_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        client_encoders[client_id] = json.loads(value)\n",
    "                        for feat, d in client_encoders[client_id].items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            all_means = []\n",
    "            all_stds = []\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"tree_ensemble_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(value)\n",
    "                        local_encoder = client_encoders[client_id]\n",
    "                        feature_names = json.loads(metrics.get(f\"encoded_feature_names_{client_id}\"))\n",
    "                        numeric_features = json.loads(metrics.get(f\"numeric_features_{client_id}\"))\n",
    "                        unique_labels = json.loads(metrics.get(f\"unique_labels_{client_id}\"))\n",
    "                        scaler = {\n",
    "                            \"mean\": json.loads(metrics.get(f\"scaler_mean_{client_id}\")),\n",
    "                            \"std\": json.loads(metrics.get(f\"scaler_std_{client_id}\")),\n",
    "                        }\n",
    "\n",
    "                        # Guarda los scalers de cada cliente\n",
    "                        all_means.append(scaler[\"mean\"])\n",
    "                        all_stds.append(scaler[\"std\"])\n",
    "                        \n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "\n",
    "                            # print(\"Local tree del cliente\", client_id)\n",
    "                            # print(\"root:\", root)\n",
    "                            # print(\"type:\", type(root))\n",
    "                            # print(\"dir(root):\", dir(root))\n",
    "                            # print(\"\\n\")\n",
    "\n",
    "                            tree_dicts.append(root)\n",
    "\n",
    "                # Calcular el scaler promedio\n",
    "                global_mean = np.mean(np.stack(all_means), axis=0)\n",
    "                global_std = np.mean(np.stack(all_stds), axis=0)\n",
    "                global_scaler = {\"mean\": global_mean, \"std\": global_std}\n",
    "\n",
    "\n",
    "                            \n",
    "            # print(tree_dicts)\n",
    "            \n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] ⚠️ No se recibieron árboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "            \n",
    "            supertree = SuperTree()\n",
    "            roots = tree_dicts\n",
    "            \n",
    "            supertree.mergeDecisionTrees(roots, num_classes=len(UNIQUE_LABELS), feature_names=feature_names, categorical_features=list(global_mapping.keys()), global_mapping=global_mapping)\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            supertree.merge_equal_class_leaves()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"supertree.root.to_dict(): \", supertree.root.to_dict())\n",
    "            # print(\"type:\", type(supertree.root))\n",
    "            # print(\"dir(supertree.root): \", dir(supertree.root))\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] 🌳 SuperTree legible (nombre de variables):\")\n",
    "            # print_supertree_legible_fusionado(\n",
    "            #     supertree.root,\n",
    "            #     feature_names=feature_names,\n",
    "            #     class_names=UNIQUE_LABELS,\n",
    "            #     numeric_features=numeric_features,\n",
    "            #     scaler=global_scaler,  # <-- ahora el scaler promedio\n",
    "            #     global_mapping=global_mapping\n",
    "            # )\n",
    "            \n",
    "            \n",
    "\n",
    "            save_supertree_plot(\n",
    "                root_node=supertree.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=UNIQUE_LABELS,\n",
    "                numeric_features=numeric_features,\n",
    "                scaler=global_scaler,\n",
    "                global_mapping=global_mapping\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "      \n",
    "            global_scaler = {\n",
    "                \"mean\": global_mean.tolist(),\n",
    "                \"std\": global_std.tolist()\n",
    "            }\n",
    "\n",
    "            GLOBAL_SCALER_JSON = json.dumps(global_scaler)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ❌ Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# 🧩 FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            \n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                ins.config[\"global_scaler\"] = GLOBAL_SCALER_JSON\n",
    "                \n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUMÉRICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        idx = numeric_features.index(feat_name)\n",
    "        mean = scaler[\"mean\"][idx]\n",
    "        std = scaler[\"std\"][idx]\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "\n",
    "        # Robusto: asegura que bounds tiene len(children)+1\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        if len(bounds) != len(children) + 1:\n",
    "            print(f\"[DEPURACIÓN] NUMÉRICA '{feat_name}' mal construida\")\n",
    "            print(f\"    intervals: {intervals}\")\n",
    "            print(f\"    children: {len(children)}\")\n",
    "            print(f\"    bounds: {bounds}\")\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "            right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} ≤ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} ∈ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÓRICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÓRICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACIÓN] Error interpretando categórica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACIÓN] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACIÓN] Nombres numéricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACIÓN] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACIÓN] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,           # dict con mean y std\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\"\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        # Etiqueta del nodo\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            class_label = class_names[class_index]\n",
    "            label = f\"class: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            if \"_\" in fname:  # OneHotEncoder\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                label = var.strip()\n",
    "            else:\n",
    "                label = fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        # Nodos hijos (binario siempre)\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # --- Caso OneHotEncoder ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                # Solo dos hijos: [≠val, =val]\n",
    "                add_node(node.children[0], curr, f'≠ \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val}\"')\n",
    "            # --- Caso numérica ---\n",
    "            elif fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler[\"mean\"][idx]\n",
    "                std = scaler[\"std\"][idx]\n",
    "                # Solo dos hijos y un threshold\n",
    "                threshold = node.intervals[0]\n",
    "                thresh_real = threshold * std + mean if np.isfinite(threshold) else threshold\n",
    "                add_node(node.children[0], curr, f\"≤ {thresh_real:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thresh_real:.2f}\")\n",
    "            # --- Caso categórica ordinal ---\n",
    "            elif fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                # Binario: solo dos hijos, dividir por primer valor\n",
    "                val = vals_cat[node.intervals[0]] if node.intervals and len(node.intervals) > 0 else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'≠ \"{val}\"')\n",
    "            else:\n",
    "                # Caso raro/desconocido\n",
    "                for child in node.children:\n",
    "                    add_node(child, curr, \"?\")\n",
    "\n",
    "    # --- Guardado ---\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "    return f\"{filename}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 🔧 INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 12:23:16,571\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] ✅ Red neuronal entrenada\n",
      "[CLIENTE 3] ✅ Red neuronal entrenada\n",
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 4] ✅ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 3] ✅ Red neuronal entrenada\n",
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 2] ✅ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] ✅ Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "\n",
      "[CLIENTE 4] 🧪 Instancia a explicar (decodificada):\n",
      "radiusMEAN                13.530000\n",
      "textureMEAN               10.940000\n",
      "perimeterMEAN             87.910000\n",
      "areaMEAN                 559.200000\n",
      "smoothnessMEAN             0.129100\n",
      "compactnessMEAN            0.104700\n",
      "concavityMEAN              0.068770\n",
      "concave pointsMEAN         0.065560\n",
      "symmetryMEAN               0.240300\n",
      "fractaldimensionMEAN       0.066410\n",
      "radiusSE                   0.410100\n",
      "textureSE                  1.014000\n",
      "perimeterSE                2.652000\n",
      "areaSE                    32.650000\n",
      "smoothnessSE               0.013400\n",
      "compactnessSE              0.028390\n",
      "concavitySE                0.011620\n",
      "concave pointsSE           0.008239\n",
      "symmetrySE                 0.025720\n",
      "fractalDimensionSE         0.006164\n",
      "radiusWORST               14.080000\n",
      "textureWORST              12.490000\n",
      "perimeterWORST            91.360000\n",
      "areaWORST                605.500000\n",
      "smoothnessWORST            0.145100\n",
      "compactnessWORST           0.137900\n",
      "concavityWORST             0.085390\n",
      "concavePointsWORST         0.074070\n",
      "symmetryWORST              0.271000\n",
      "fractalDimensionWORST      0.071910\n",
      "dtype: float64\n",
      "🧪 Clase real: B\n",
      "🧪 Clase predicha: 'B'\n",
      "Regla factual simplificada: ['radiusSE ≤ 0.71']\n",
      "Contrafactuales simplificado: {'M': ['areaWORST > 953.95', 'concavePointsWORST > 0.14 ∧ ≤ 0.15', 'radiusSE > 0.71', 'radiusWORST > 16.65', 'smoothnessSE > 0.01', 'concave pointsMEAN > 0.06']}\n",
      "Métricas explicabilidad:\n",
      "  - Silhouette: 0.796\n",
      "  - Fidelity:   0.993\n",
      "  - Coverage:   0.46488294314381273\n",
      "  - Precision:  0.9928057553956835\n",
      "\n",
      "\n",
      "[CLIENTE 2] 🧪 Instancia a explicar (decodificada):\n",
      "radiusMEAN                 8.88800\n",
      "textureMEAN               14.64000\n",
      "perimeterMEAN             58.79000\n",
      "areaMEAN                 244.00000\n",
      "smoothnessMEAN             0.09783\n",
      "compactnessMEAN            0.15310\n",
      "concavityMEAN              0.08606\n",
      "concave pointsMEAN         0.02872\n",
      "symmetryMEAN               0.19020\n",
      "fractaldimensionMEAN       0.08980\n",
      "radiusSE                   0.52620\n",
      "textureSE                  0.85220\n",
      "perimeterSE                3.16800\n",
      "areaSE                    25.44000\n",
      "smoothnessSE               0.01721\n",
      "compactnessSE              0.09368\n",
      "concavitySE                0.05671\n",
      "concave pointsSE           0.01766\n",
      "symmetrySE                 0.02541\n",
      "fractalDimensionSE         0.02193\n",
      "radiusWORST                9.73300\n",
      "textureWORST              15.67000\n",
      "perimeterWORST            62.56000\n",
      "areaWORST                284.40000\n",
      "smoothnessWORST            0.12070\n",
      "compactnessWORST           0.24360\n",
      "concavityWORST             0.14340\n",
      "concavePointsWORST         0.04786\n",
      "symmetryWORST              0.22540\n",
      "fractalDimensionWORST      0.10840\n",
      "dtype: float64\n",
      "🧪 Clase real: B\n",
      "🧪 Clase predicha: 'B'\n",
      "Regla factual simplificada: ['smoothnessWORST ≤ 0.13']\n",
      "Contrafactuales simplificado: {'M': ['concavePointsWORST > 0.14', 'fractalDimensionSE ≤ 0.01', 'perimeterWORST > 101.31', 'radiusWORST ≤ 14.04', 'smoothnessWORST > 0.13', 'concave pointsMEAN > 0.05']}\n",
      "Métricas explicabilidad:\n",
      "  - Silhouette: 0.836\n",
      "  - Fidelity:   0.993\n",
      "  - Coverage:   0.45484949832775917\n",
      "  - Precision:  1.0\n",
      "\n",
      "\n",
      "[CLIENTE 3] 🧪 Instancia a explicar (decodificada):\n",
      "radiusMEAN                 17.010000\n",
      "textureMEAN                20.260000\n",
      "perimeterMEAN             109.700000\n",
      "areaMEAN                  904.300000\n",
      "smoothnessMEAN              0.087720\n",
      "compactnessMEAN             0.073040\n",
      "concavityMEAN               0.069500\n",
      "concave pointsMEAN          0.053900\n",
      "symmetryMEAN                0.202600\n",
      "fractaldimensionMEAN        0.052230\n",
      "radiusSE                    0.585800\n",
      "textureSE                   0.855400\n",
      "perimeterSE                 4.106000\n",
      "areaSE                     68.460000\n",
      "smoothnessSE                0.005038\n",
      "compactnessSE               0.015030\n",
      "concavitySE                 0.019460\n",
      "concave pointsSE            0.011230\n",
      "symmetrySE                  0.022940\n",
      "fractalDimensionSE          0.002581\n",
      "radiusWORST                19.800000\n",
      "textureWORST               25.050000\n",
      "perimeterWORST            130.000000\n",
      "areaWORST                1210.000000\n",
      "smoothnessWORST             0.111100\n",
      "compactnessWORST            0.148600\n",
      "concavityWORST              0.193200\n",
      "concavePointsWORST          0.109600\n",
      "symmetryWORST               0.327500\n",
      "fractalDimensionWORST       0.064690\n",
      "dtype: float64\n",
      "🧪 Clase real: M\n",
      "🧪 Clase predicha: 'M'\n",
      "Regla factual simplificada: ['areaSE > 41.72', 'areaWORST > 960.39', 'concavePointsWORST ≤ 0.13', 'fractalDimensionWORST > 0.06', 'radiusWORST > 16.81', 'textureWORST > 22.82']\n",
      "Contrafactuales simplificado: {'B': ['perimeterWORST ≤ 108.54', 'smoothnessSE ≤ 0.01', 'textureWORST ≤ 22.82', 'concave pointsMEAN ≤ 0.10']}\n",
      "Métricas explicabilidad:\n",
      "  - Silhouette: 0.736\n",
      "  - Fidelity:   0.990\n",
      "  - Coverage:   0.43812709030100333\n",
      "  - Precision:  1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLIENTE 1] 🧪 Instancia a explicar (decodificada):\n",
      "radiusMEAN                15.460000\n",
      "textureMEAN               23.950000\n",
      "perimeterMEAN            103.800000\n",
      "areaMEAN                 731.300000\n",
      "smoothnessMEAN             0.118300\n",
      "compactnessMEAN            0.187000\n",
      "concavityMEAN              0.203000\n",
      "concave pointsMEAN         0.085200\n",
      "symmetryMEAN               0.180700\n",
      "fractaldimensionMEAN       0.070830\n",
      "radiusSE                   0.333100\n",
      "textureSE                  1.961000\n",
      "perimeterSE                2.937000\n",
      "areaSE                    32.520000\n",
      "smoothnessSE               0.009538\n",
      "compactnessSE              0.049400\n",
      "concavitySE                0.060190\n",
      "concave pointsSE           0.020410\n",
      "symmetrySE                 0.021050\n",
      "fractalDimensionSE         0.006000\n",
      "radiusWORST               17.110000\n",
      "textureWORST              36.330000\n",
      "perimeterWORST           117.700000\n",
      "areaWORST                909.400000\n",
      "smoothnessWORST            0.173200\n",
      "compactnessWORST           0.496700\n",
      "concavityWORST             0.591100\n",
      "concavePointsWORST         0.216300\n",
      "symmetryWORST              0.301300\n",
      "fractalDimensionWORST      0.106700\n",
      "dtype: float64\n",
      "🧪 Clase real: M\n",
      "🧪 Clase predicha: 'M'\n",
      "Regla factual simplificada: ['areaMEAN > 490.49', 'concavePointsWORST > 0.13', 'perimeterWORST > 97.98', 'radiusWORST > 16.27', 'concave pointsMEAN > 0.04']\n",
      "Contrafactuales simplificado: {'B': ['areaMEAN ≤ 490.49', 'concavityWORST ≤ 0.55', 'textureMEAN ≤ 24.48']}\n",
      "Métricas explicabilidad:\n",
      "  - Silhouette: 0.506\n",
      "  - Fidelity:   0.990\n",
      "  - Coverage:   0.4425087108013937\n",
      "  - Precision:  1.0\n",
      "\n",
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 80.48s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar aún más\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesión previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
