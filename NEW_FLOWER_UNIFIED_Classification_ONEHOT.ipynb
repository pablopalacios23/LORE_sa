{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68391f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 12:52:40,991\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-08-28 12:52:44,496 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.piping.pipe(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-08-28 12:52:44,498 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.rendering.render(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-08-28 12:52:44,500 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.unflattening.unflatten(['stagger', 'fanout', 'chain', 'encoding'])\n",
      "2025-08-28 12:52:44,502 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.viewing.view(['quiet'])\n",
      "2025-08-28 12:52:44,508 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.quote(['is_html_string', 'is_valid_id', 'dot_keywords', 'endswith_odd_number_of_backslashes', 'escape_unescaped_quotes'])\n",
      "2025-08-28 12:52:44,509 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.a_list(['kwargs', 'attributes'])\n",
      "2025-08-28 12:52:44,509 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.attr_list(['kwargs', 'attributes'])\n",
      "2025-08-28 12:52:44,510 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.clear(['keep_attrs'])\n",
      "2025-08-28 12:52:44,510 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.__iter__(['subgraph'])\n",
      "2025-08-28 12:52:44,511 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.node(['_attributes'])\n",
      "2025-08-28 12:52:44,511 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.edge(['_attributes'])\n",
      "2025-08-28 12:52:44,511 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.attr(['_attributes'])\n",
      "2025-08-28 12:52:44,512 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.subgraph(['name', 'comment', 'graph_attr', 'node_attr', 'edge_attr', 'body'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.piping.Pipe._pipe_legacy(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.saving.Save.save(['directory'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.render(['directory', 'view', 'cleanup', 'format', 'renderer', 'formatter', 'neato_no_op', 'quiet', 'quiet_view'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.view(['directory', 'cleanup', 'quiet', 'quiet_view'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.unflattening.Unflatten.unflatten(['stagger', 'fanout', 'chain'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.graphs.BaseGraph.__init__(['comment', 'filename', 'directory', 'format', 'engine', 'encoding', 'graph_attr', 'node_attr', 'edge_attr', 'body', 'strict'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.from_file(['directory', 'format', 'engine', 'encoding', 'renderer', 'formatter'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.__init__(['filename', 'directory', 'format', 'engine', 'encoding'])\n",
      "2025-08-28 12:52:44,516 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.save(['directory'])\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# ðŸ“¦ IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "from lore_sa.client_utils import ClientUtilsMixin  \n",
    "\n",
    "# Otros\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# âš™ï¸ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "NUM_PARTITIONS_TOTAL = NUM_CLIENTS + 1   # +1 = holdout servidor\n",
    "SEED = 42\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ðŸ”§ UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ðŸ“¥ CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name, class_col):\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_tmp = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "    df = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado estÃ¡ndar\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df['SeniorCitizen'] = df['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "        df.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "    \n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado especÃ­fico para el dataset de cÃ¡ncer de mama\n",
    "        df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].nunique() < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [col for col in df.select_dtypes(include=\"category\").columns if col != class_col]\n",
    "    num_features = [col for col in df.columns if df[col].dtype.kind in \"fi\" and col != class_col]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    ohe.fit(df[cat_features])\n",
    "    categories_global = ohe.categories_\n",
    "    onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    return cat_features, num_features, categories_global, onehot_columns\n",
    "\n",
    "\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    # Saca info global siempre al principio\n",
    "    cat_features, num_features, categories_global, onehot_columns = get_global_onehot_info(flower_dataset_name, class_col)\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado especÃ­fico por dataset\n",
    "    if \"adult\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset['SeniorCitizen'] = dataset['SeniorCitizen'].map({0: 'No', 1: 'Yes'}).astype(str)\n",
    "\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    elif \"breastcancer\" in flower_dataset_name.lower():\n",
    "        # Preprocesado especÃ­fico para el dataset de cÃ¡ncer de mama\n",
    "        dataset.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info or not info[\"distinct_values\"]:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataset[class_col])\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "    dataset[class_col] = label_encoder.transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", categories=categories_global), categorical_indices)\n",
    "    ])\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    # ReconstrucciÃ³n del DataFrame\n",
    "    num_out = X_encoded[:, :len(numeric_features)]\n",
    "    cat_out = X_encoded[:, len(numeric_features):]\n",
    "    if categorical_features:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = numeric_features\n",
    "\n",
    "    X_df = pd.DataFrame(num_out, columns=num_names)\n",
    "    if len(cat_names) > 0:\n",
    "        X_cat_df = pd.DataFrame(cat_out, columns=cat_names)\n",
    "        X_full = pd.concat([X_df.reset_index(drop=True), X_cat_df.reset_index(drop=True)], axis=1)\n",
    "        for col in onehot_columns:\n",
    "            if col not in X_cat_df.columns:\n",
    "                X_full[col] = 0\n",
    "    else:\n",
    "        X_full = X_df\n",
    "\n",
    "    # Rellenar columnas onehot que falten y ordenar\n",
    "    final_columns = num_names + list(cat_names)\n",
    "    X_full = X_full[final_columns]\n",
    "    FEATURES[:] = final_columns\n",
    "\n",
    "    split_idx = int(0.7 * len(X_full))\n",
    "\n",
    "        # --- Â¡Construye el descriptor global! ---\n",
    "    descriptor_global = descriptor.copy()\n",
    "    for i, col in enumerate(cat_features):\n",
    "        if col in descriptor_global[\"categorical\"]:\n",
    "            descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(categories_global[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    return (\n",
    "        X_full.iloc[:split_idx].to_numpy(), y[:split_idx],\n",
    "        X_full.iloc[split_idx:].to_numpy(), y[split_idx:],\n",
    "        tabular_dataset, final_columns, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# Los resultados de las mÃ©tricas no son muy buenos aqui\n",
    "# DATASET_NAME = \"pablopalacios23/adult\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/HeartDisease\"\n",
    "# CLASS_COLUMN = \"HeartDisease\" \n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/breastcancer\"\n",
    "CLASS_COLUMN = \"diagnosis\" \n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Diabetes\"\n",
    "# CLASS_COLUMN = \"Outcome\" \n",
    "\n",
    "\n",
    " \n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 12:52:44,555 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-28 12:52:44,988 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:45,354 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 564\n",
      "2025-08-28 12:52:45,496 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:45,499 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-08-28 12:52:45,828 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:45,940 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:46,087 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 615\n",
      "2025-08-28 12:52:46,239 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-08-28 12:52:46,244 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-08-28 12:52:46,467 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2025-08-28 12:52:46,591 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 615\n",
      "2025-08-28 12:52:46,999 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e?recursive=False&expand=False HTTP/11\" 200 207\n",
      "2025-08-28 12:52:47,171 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-08-28 12:52:47,440 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-28 12:52:47,608 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 615\n",
      "2025-08-28 12:52:47,741 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-08-28 12:52:47,757 filelock     DEBUG    Attempting to acquire lock 1889278633088 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:47,757 filelock     DEBUG    Lock 1889278633088 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:47,757 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:47,757 filelock     DEBUG    Attempting to release lock 1889278633088 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:47,757 filelock     DEBUG    Lock 1889278633088 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:47,796 filelock     DEBUG    Attempting to acquire lock 1889286265920 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:47,797 filelock     DEBUG    Lock 1889286265920 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:47,798 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:47,798 filelock     DEBUG    Attempting to release lock 1889286265920 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:47,798 filelock     DEBUG    Lock 1889286265920 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:48,012 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:48,137 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 615\n",
      "2025-08-28 12:52:48,258 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:48,358 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:48,476 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:48,609 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-08-28 12:52:48,759 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2025-08-28 12:52:48,891 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-08-28 12:52:48,985 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-28 12:52:49,163 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 615\n",
      "2025-08-28 12:52:49,276 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-08-28 12:52:49,276 filelock     DEBUG    Attempting to acquire lock 1889103550208 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:49,293 filelock     DEBUG    Lock 1889103550208 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:49,295 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:49,296 filelock     DEBUG    Attempting to release lock 1889103550208 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:49,297 filelock     DEBUG    Lock 1889103550208 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:49,299 filelock     DEBUG    Attempting to acquire lock 1888648620240 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:49,300 filelock     DEBUG    Lock 1888648620240 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:49,301 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:49,302 filelock     DEBUG    Attempting to release lock 1888648620240 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:49,302 filelock     DEBUG    Lock 1888648620240 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ X_train (primeras filas):\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0  -0.630655 -0.741874 -0.617875 -0.637791  1.225325 -0.178274 -0.560067   \n",
      "1   1.043546  0.906161  0.918554  0.960623 -0.571024 -0.769649 -0.745296   \n",
      "2  -0.161170 -0.538561 -0.225872 -0.245204 -0.719919 -1.027444 -0.656819   \n",
      "3  -0.208414  0.781781 -0.119001 -0.298214  0.360772  0.929089  0.030582   \n",
      "4  -0.736953  1.867715 -0.779263 -0.715123 -1.791691 -1.262363 -0.913176   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "74 -0.793055 -0.634237 -0.835078 -0.760650 -0.921648 -1.258185 -0.948406   \n",
      "75 -0.338334 -0.471587 -0.368654 -0.421385  0.539172 -0.496339 -0.704994   \n",
      "76  1.560274  1.678753  1.701696  1.580842  1.822279  1.541748  1.453824   \n",
      "77 -0.819630 -0.584007 -0.815608 -0.785284  0.875387 -0.468491 -0.724611   \n",
      "78  0.131151 -0.263489  0.062723  0.025148 -2.002340 -0.982489 -0.800811   \n",
      "\n",
      "          7         8         9   ...        20        21        22        23  \\\n",
      "0  -0.357751  0.230350  0.140329  ... -0.539668 -0.683399 -0.548138 -0.539009   \n",
      "1  -0.440143 -0.871438 -1.333085  ...  0.883483  1.008529  0.754198  0.740408   \n",
      "2  -0.546554 -1.130682 -0.838805  ... -0.143017 -0.339191 -0.164288 -0.234568   \n",
      "3   0.192458  0.218913  1.364582  ... -0.271602  0.612947 -0.144267 -0.367642   \n",
      "4  -1.145222  0.291349 -0.900758  ... -0.809915  1.398974 -0.759302 -0.725695   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "74 -0.849868 -1.073496 -0.200415  ... -0.897091 -0.906021 -0.930424 -0.785434   \n",
      "75 -0.644028 -0.326262 -0.421293  ... -0.454672 -0.767310 -0.481504 -0.491907   \n",
      "76  2.870332 -0.604569  1.248756  ...  0.988094  1.222589  1.164013  0.893585   \n",
      "77 -0.539851 -0.299576  0.058173  ... -0.737994  0.147153 -0.755861 -0.689123   \n",
      "78 -0.848751 -0.600756 -0.957324  ...  0.092359 -0.054919  0.053446 -0.022226   \n",
      "\n",
      "          24        25        26        27        28        29  \n",
      "0   0.843649 -0.676976 -0.563210 -0.135780 -0.173470 -0.239822  \n",
      "1  -0.348748 -0.671278 -0.689572 -0.388235 -0.789106 -1.057541  \n",
      "2  -0.198656 -0.782080 -0.429149 -0.157670 -0.694662 -0.457122  \n",
      "3   0.105697  1.175004  0.194056  0.095103  2.142162  1.242803  \n",
      "4  -1.681647 -0.705468 -0.744827 -1.371209 -0.094766 -0.747577  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "74 -1.224283 -1.218387 -1.030614 -1.057878 -0.909785 -0.749745  \n",
      "75  0.251620 -0.844762 -0.776078 -0.808939 -0.376350 -0.855414  \n",
      "76  0.284973  0.171452 -0.136569  1.237540 -1.035710  0.399072  \n",
      "77  0.189081 -0.673810 -0.754791 -0.516699  0.165830 -0.317313  \n",
      "78 -1.580752 -0.240099 -0.529242 -0.500881 -0.467296 -0.428401  \n",
      "\n",
      "[79 rows x 30 columns]\n",
      "\n",
      "ðŸŽ¯ y_train (primeros valores):\n",
      "[0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
      " 0 0 1 0 0]\n",
      "\n",
      "ðŸ“¦ X_test (primeras filas):\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   2.014995 -0.459627  1.948320  2.213846 -0.033080  0.119503  0.906680   \n",
      "1   0.024852 -0.634237 -0.034629 -0.079937 -1.829429 -0.962796 -0.834307   \n",
      "2   0.863429  0.554549  0.879613  0.762302  0.079449  0.636683  0.667805   \n",
      "3  -0.246799  0.018758 -0.227170 -0.346235  0.058179  0.216971 -0.182671   \n",
      "4   1.917555  1.862931  1.866112  2.067288  0.347049  0.143373  0.896004   \n",
      "5  -0.060777  1.365411 -0.074435 -0.132011 -0.004261 -0.327858  0.019906   \n",
      "6  -0.790103 -1.244178 -0.801762 -0.749736 -1.523405 -0.814604 -0.529908   \n",
      "7  -0.698568  0.406249 -0.690565 -0.697038  1.046926 -0.158581 -0.518431   \n",
      "8   1.232521 -2.126798  1.429110  1.172352  1.609571  3.501065  2.873731   \n",
      "9   1.421496  1.396506  1.515645  1.443639  0.786187  1.732707  1.963603   \n",
      "10  1.161655  2.094948  1.191139  1.111858  0.344991  0.592922  1.134879   \n",
      "11 -0.527309 -0.320895 -0.591915 -0.558276 -1.242768 -1.246648 -1.110468   \n",
      "12  1.140986  0.351235  1.195466  1.056041  1.136126  1.291115  1.344395   \n",
      "13  1.577991  1.752902  1.576220  1.599551  0.484280  0.869414  1.432472   \n",
      "14 -0.435774  0.705240 -0.429229 -0.500588  0.429388 -0.001836 -0.414474   \n",
      "15  0.063238  0.473223  0.000850 -0.067464 -0.291073 -0.639360 -0.935996   \n",
      "16  2.791565  0.662185  2.696848  3.308351 -0.724036  0.256754  0.635777   \n",
      "17 -0.654277  1.248207 -0.672392 -0.648081 -1.386174 -0.889396 -0.867669   \n",
      "18  1.864406  2.147570  1.792557  1.983096  0.196095  0.035958  0.790579   \n",
      "19 -0.633608  0.179017 -0.629990 -0.651199  0.456834 -0.140480 -0.570610   \n",
      "20 -0.651324 -0.777753 -0.619173 -0.676145  0.950864  0.302505 -0.184006   \n",
      "21  0.485479  1.119043  0.607028  0.331360  1.602710  1.698891  1.577933   \n",
      "22 -0.663135 -0.055392 -0.673690 -0.671468 -0.650618 -0.484802 -0.399127   \n",
      "23  0.999255  1.255382  1.057010  0.949085  0.834218  1.619325  1.126872   \n",
      "24 -0.217272 -0.851903 -0.179143 -0.327525  0.861664  0.505399 -0.521500   \n",
      "25  0.018947 -0.744266 -0.060590 -0.088668 -1.693571 -1.059867 -0.858194   \n",
      "26 -1.436752 -0.839943 -1.342606 -1.184420 -0.022101  0.451692  0.105047   \n",
      "27 -1.413425 -0.464411 -1.340442 -1.167893  0.799910  0.789848  3.045881   \n",
      "28 -0.772386  2.415465 -0.826424 -0.745370 -1.403328 -1.313086 -1.131099   \n",
      "29 -1.304764  0.576076 -1.298906 -1.110829 -1.046528 -0.816792 -0.632664   \n",
      "30 -0.170028  0.205328 -0.125491 -0.256117 -0.830390  0.411908  0.216744   \n",
      "31 -0.308807  0.765038 -0.348319 -0.370245 -0.301366 -0.874875 -0.502417   \n",
      "32 -0.409200 -0.543344 -0.483313 -0.460675 -1.329224 -1.333773 -0.952010   \n",
      "33  0.228591  0.815268  0.286416  0.099986 -0.700707  0.624748  0.242100   \n",
      "34  0.485479  0.049853  0.516166  0.386241  0.978310  0.411908  0.825276   \n",
      "\n",
      "          7         8         9   ...        20        21        22        23  \\\n",
      "0   1.264669 -0.844751 -1.102780  ...  2.086514 -0.412827  1.961745  2.134325   \n",
      "1  -0.858526  0.077854 -1.024665  ... -0.143017 -0.647437 -0.090459 -0.215230   \n",
      "2   0.448292  0.394284 -0.863047  ...  0.918353  0.588972  0.920001  0.761470   \n",
      "3  -0.408583 -0.120392  0.327536  ... -0.299934 -0.621750 -0.152713 -0.377215   \n",
      "4   1.374152  0.081666 -0.976180  ...  1.859856  2.325424  1.820969  1.939023   \n",
      "5   0.021251 -0.699879 -0.573483  ...  0.260173  1.659271  0.141040  0.143972   \n",
      "6  -0.821659 -0.391074 -0.287958  ... -0.801197 -1.671498 -0.765559 -0.723397   \n",
      "7  -0.608837  0.188414  0.953804  ... -0.585436  1.126690 -0.611957 -0.586303   \n",
      "8   2.875918  2.388177  2.120144  ...  2.088694 -1.412914  2.518593  2.274100   \n",
      "9   2.241920  1.488447 -0.134421  ...  1.602686  1.443498  1.764658  1.605862   \n",
      "10  1.083686 -0.303388 -0.512876  ...  1.249623  2.219251  1.292276  1.159732   \n",
      "11 -1.076320 -1.565297 -0.305467  ... -0.594154 -0.570375 -0.667954 -0.589558   \n",
      "12  1.711261  0.680215 -0.176173  ...  1.018606 -0.080607  0.901231  0.901244   \n",
      "13  1.466599  0.417158 -0.103445  ...  1.726913  1.767157  1.742760  1.709257   \n",
      "14 -0.444333 -0.920999  0.625181  ... -0.483004  0.530748 -0.523424 -0.532882   \n",
      "15 -0.702959 -1.050621 -0.621968  ... -0.101608  0.804744 -0.182120 -0.221740   \n",
      "16  1.477212  0.032105 -1.001769  ...  2.661877  0.451974  2.506079  3.009353   \n",
      "17 -0.865788  0.542968 -0.564055  ... -0.731456  1.034216 -0.710187 -0.679358   \n",
      "18  1.502069 -0.154704 -1.028706  ...  1.720374  2.169589  1.592598  1.722660   \n",
      "19 -0.630063  0.253225  0.221137  ... -0.533130  0.553010 -0.534686 -0.537477   \n",
      "20  0.023485  0.356160  0.031237  ... -0.687868 -1.012194 -0.693294 -0.680890   \n",
      "21  1.147086  0.054979  1.058855  ...  0.286326  1.840793  0.425720  0.149525   \n",
      "22 -0.833669  0.908960 -0.040144  ... -0.594154  0.239627 -0.552518 -0.595111   \n",
      "23  0.986213  0.512469  0.256154  ...  1.639736  1.410961  1.486234  1.626924   \n",
      "24 -0.363896  0.664965  0.692522  ... -0.282499 -0.871771 -0.250318 -0.384491   \n",
      "25 -0.994906 -0.707504 -1.108168  ... -0.062379 -0.960820 -0.138636 -0.164681   \n",
      "26 -0.587890 -1.858853  1.166600  ... -1.390944 -1.457439 -1.274856 -1.074748   \n",
      "27 -0.010589  1.213953  2.355837  ... -1.195669 -0.501876 -1.207283 -0.970013   \n",
      "28 -1.232501 -2.792895 -1.070457  ... -0.844785  2.178151 -0.904145 -0.750012   \n",
      "29 -1.089279 -1.977038  0.575349  ... -1.270859  0.412587 -1.172559 -1.015392   \n",
      "30 -0.441260 -0.730378  0.182080  ... -0.077635 -0.013820  0.341255 -0.187275   \n",
      "31 -0.477289 -0.791377 -0.312201  ... -0.284678  1.333900 -0.377017 -0.340261   \n",
      "32 -0.757980 -1.222180 -0.990995  ... -0.631203 -0.919721 -0.698612 -0.603918   \n",
      "33 -0.189058 -1.290804 -0.201762  ... -0.068917  0.289289  0.056574 -0.187275   \n",
      "34  1.026152  0.527719 -0.674494  ...  0.754898  0.071804  0.650962  0.621695   \n",
      "\n",
      "          24        25        26        27        28        29  \n",
      "0   0.993740  0.322143  0.645607  1.670547 -0.374601 -0.764918  \n",
      "1  -1.606184 -0.342037 -0.611219 -0.709875 -0.392091 -0.416479  \n",
      "2   0.997910  0.816005  1.211745  1.192801  3.427652  0.090192  \n",
      "3  -0.148625  0.447508  0.319512 -0.128910  1.211712  0.379022  \n",
      "4   0.826972  0.252496  0.477578  1.007455  0.433422 -0.253369  \n",
      "5  -0.244517 -0.381926  0.197226  0.187777 -0.441062 -0.579590  \n",
      "6  -1.822983 -0.816903 -0.694101 -1.077372 -0.572234 -0.784968  \n",
      "7   0.972894 -0.550978 -0.443189 -0.725054 -0.246926  0.208325  \n",
      "8   1.243893  2.602137  1.986222  2.536561  2.969423  1.855144  \n",
      "9   0.697726  1.083196  1.539652  1.248724  0.945869  0.712829  \n",
      "10  0.322496  0.769784  1.660126  1.443658  0.120357  0.329710  \n",
      "11 -1.295160 -1.144245 -1.203028 -1.257285 -1.280566 -0.774672  \n",
      "12  0.393373  0.602631  0.384731  0.716653 -0.769867 -0.394804  \n",
      "13  1.910968  0.819804  1.367999  1.287072  0.620561  0.679232  \n",
      "14  0.535126 -0.131829 -0.473534 -0.394786 -1.110916  0.333503  \n",
      "15 -0.152795 -0.654182 -0.955838 -0.431695 -1.184372 -0.460373  \n",
      "16 -0.398778  0.656450  0.550043  2.044435  1.199469  0.390402  \n",
      "17 -1.539059 -0.750422 -0.910185 -0.934687  0.596075 -0.932905  \n",
      "18 -0.657270 -0.395222  0.218060  0.897206 -0.579230 -0.991430  \n",
      "19  0.943710  0.264526  0.011080 -0.406610  0.529615  0.261973  \n",
      "20  0.380865 -0.238199 -0.195447  0.061549 -0.203202 -0.563333  \n",
      "21  1.702507  1.532737  1.439106  1.752035  0.192064  1.194032  \n",
      "22 -0.311225 -0.385724 -0.078596 -0.639251  0.230541 -0.098929  \n",
      "23  1.089632  3.069405  1.735762  1.330213  0.716754  2.667986  \n",
      "24 -0.048564  0.145493 -0.382046 -0.540347  0.491137 -0.153660  \n",
      "25 -1.981413 -0.831466 -0.743922 -0.979106 -0.631699 -0.999016  \n",
      "26 -0.603070 -0.422448 -0.538753 -1.089515 -2.188279 -0.403474  \n",
      "27  0.660203  1.151577  4.432390  1.092139  2.317058  1.779278  \n",
      "28 -1.654964 -1.264291 -1.238047 -1.704034 -2.338690 -1.388097  \n",
      "29 -0.990807 -0.417382 -0.392010 -1.294355 -0.922028  0.400698  \n",
      "30 -0.511347  1.962650  1.731233  0.464198 -0.101762  2.180281  \n",
      "31  0.397542 -0.647851 -0.252061 -0.213115 -0.129746 -0.216520  \n",
      "32 -2.012683 -1.313804 -1.136731 -1.251533 -1.752787 -1.373466  \n",
      "33 -1.240960  0.395589  0.420511  0.061549 -1.128406 -0.250660  \n",
      "34  0.927033 -0.096372  0.478936  0.715055 -0.115754 -0.242531  \n",
      "\n",
      "[35 rows x 30 columns]\n",
      "\n",
      "ðŸŽ¯ y_test (primeros valores):\n",
      "[1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1]\n",
      "['radiusMEAN', 'textureMEAN', 'perimeterMEAN', 'areaMEAN', 'smoothnessMEAN', 'compactnessMEAN', 'concavityMEAN', 'concave pointsMEAN', 'symmetryMEAN', 'fractaldimensionMEAN', 'radiusSE', 'textureSE', 'perimeterSE', 'areaSE', 'smoothnessSE', 'compactnessSE', 'concavitySE', 'concave pointsSE', 'symmetrySE', 'fractalDimensionSE', 'radiusWORST', 'textureWORST', 'perimeterWORST', 'areaWORST', 'smoothnessWORST', 'compactnessWORST', 'concavityWORST', 'concavePointsWORST', 'symmetryWORST', 'fractalDimensionWORST']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_PARTITIONS_TOTAL\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\nðŸ“¦ X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\nðŸŽ¯ y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nðŸ“¦ X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\nðŸŽ¯ y_test (primeros valores):\")\n",
    "print(y_test)\n",
    "\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d65b7",
   "metadata": {},
   "source": [
    "La Ãºltima particiÃ³n la usamos como holdout del servidor para evaluar posteriormente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d92dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 12:52:49,492 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:49,633 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer HTTP/11\" 200 615\n",
      "2025-08-28 12:52:49,950 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:50,045 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/breastcancer/pablopalacios23/breastcancer.py HTTP/11\" 404 0\n",
      "2025-08-28 12:52:50,176 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/README.md HTTP/11\" 404 0\n",
      "2025-08-28 12:52:50,293 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-08-28 12:52:50,445 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/breastcancer HTTP/11\" 200 None\n",
      "2025-08-28 12:52:50,561 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/tree/d21fb27c44731c56662f52e0f762dcc070083b0e/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-08-28 12:52:50,710 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-08-28 12:52:50,861 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/breastcancer/revision/d21fb27c44731c56662f52e0f762dcc070083b0e HTTP/11\" 200 615\n",
      "2025-08-28 12:52:50,995 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/breastcancer/resolve/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Attempting to acquire lock 1889286165024 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Lock 1889286165024 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:50,995 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Attempting to release lock 1889286165024 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Lock 1889286165024 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___breastcancer_default_0.0.0_d21fb27c44731c56662f52e0f762dcc070083b0e.lock\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Attempting to acquire lock 1889288150128 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Lock 1889288150128 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:50,995 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___breastcancer/default/0.0.0/d21fb27c44731c56662f52e0f762dcc070083b0e/dataset_info.json\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Attempting to release lock 1889288150128 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n",
      "2025-08-28 12:52:50,995 filelock     DEBUG    Lock 1889288150128 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___breastcancer\\default\\0.0.0\\d21fb27c44731c56662f52e0f762dcc070083b0e_builder.lock\n"
     ]
    }
   ],
   "source": [
    "HOLDOUT_PID = NUM_PARTITIONS_TOTAL - 1\n",
    "\n",
    "(Xh_tr, yh_tr, Xh_te, yh_te, ds_h, feat_h, le_h, scaler_h, num_h, enc_h, prep_h) = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=HOLDOUT_PID, num_partitions=NUM_PARTITIONS_TOTAL\n",
    ")\n",
    "\n",
    "X_holdout = np.vstack([Xh_tr, Xh_te])\n",
    "y_holdout = np.concatenate([yh_tr, yh_te])\n",
    "\n",
    "# GuÃ¡rdalo en globals para reusar en cada ronda\n",
    "SERVER_HOLDOUT = {\n",
    "    \"X\": X_holdout,\n",
    "    \"y\": y_holdout,\n",
    "    \"feature_names\": feat_h,\n",
    "    \"numeric_features\": num_h,\n",
    "    \"scaler\": scaler_h,\n",
    "    \"label_encoder\": le_h,\n",
    "    \"encoder\": enc_h,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸŒ¼ CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient, ClientUtilsMixin):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] âœ… Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            self._train_nn()\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [\n",
    "            self.tree_model.get_params()[\"max_depth\"],\n",
    "            self.tree_model.get_params()[\"min_samples_split\"],\n",
    "            self.tree_model.get_params()[\"min_samples_leaf\"],\n",
    "        ], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                \n",
    "                # print(\"supertree_dict\")\n",
    "                # print(\"supertree_dict:\", supertree_dict)\n",
    "                # print(\"type:\", type(supertree_dict))\n",
    "                # print(\"dir(supertree_dict):\", dir(supertree_dict))\n",
    "                # print(\"\\n\")\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "                self.global_scaler = json.loads(config[\"global_scaler\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] âŒ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(export_text(self.tree_model, feature_names=FEATURES))\n",
    "        # print(\"root_node:\", root_node)\n",
    "        # print(\"type:\", type(root_node))\n",
    "        # print(dir(root_node))\n",
    "        # print(\"\\n\")\n",
    "        # print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "        \n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None and config.get(\"server_round\", 0) == NUM_SERVER_ROUNDS:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"scaler_mean_{self.client_id}\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            f\"scaler_std_{self.client_id}\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"encoder_descriptor_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "\n",
    "        \n",
    "    \n",
    "        num_row = 0\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            self.scaler,\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "        # print(f\"\\n[CLIENTE {self.client_id}] ðŸ§ª Instancia a explicar (decodificada):\")\n",
    "        # print(decoded)\n",
    "        # print(f\"[CLIENTE {self.client_id}] ðŸ§ª Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "\n",
    "        # AsegÃºrate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        x_tensor = torch.tensor(self.X_test[num_row], dtype=torch.float32).unsqueeze(0)  # shape: [1, n_features]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.nn_model(x_tensor)   # shape: [1, n_classes]\n",
    "            probs = torch.softmax(logits, dim=1).numpy()\n",
    "            pred_class_idx = int(probs.argmax(axis=1)[0])\n",
    "\n",
    "        # Si tienes un label_encoder:\n",
    "        pred_class = self.label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        # Ahora crea el TabularDataset legible\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")    \n",
    "\n",
    "        # Explicabilidad local y la vecindad es generada del train (local_tabular_dataset)\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        \n",
    "        # ExplicaciÃ³n LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        \n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        \n",
    "        # self.print_tree_readable(node=lore_tree.root,feature_names=self.feature_names,class_names=UNIQUE_LABELS,  numeric_features=self.numeric_features,scaler=self.scaler,encoder=self.encoder)\n",
    "        # print('\\n')\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        self.save_lore_tree_image(lore_tree.root, round_number, self.feature_names, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder, folder=\"LoreTree\")\n",
    "\n",
    "\n",
    "        merged_tree = SuperTree()\n",
    "        merged_tree.mergeDecisionTrees(\n",
    "            roots=[lore_tree.root, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()), \n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "        merged_tree.merge_equal_class_leaves()\n",
    "\n",
    "        self.save_supertree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=self.scaler,global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "        \n",
    "        tree_str = self.tree_to_str(merged_tree.root, self.feature_names, numeric_features=self.numeric_features, scaler=self.scaler, global_mapping=self.global_mapping, unique_labels=self.unique_labels)\n",
    "\n",
    "        rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"âˆ§\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 âˆ§ â‰¤ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)\\s*âˆ§\\s*([><]=?|â‰¤|â‰¥)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # EvalÃºa las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('â‰¤','<=').replace('â‰¥','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('â‰¤','<=').replace('â‰¥','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu cÃ³digo tal cual ...\n",
    "                if \"â‰¤\" in cond:\n",
    "                    var, val = cond.split(\"â‰¤\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"â‰¥\" in cond:\n",
    "                    var, val = cond.replace(\"â‰¥\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"â‰ \" in cond:\n",
    "                    var, val = cond.split(\"â‰ \")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "        # Buscar la regla factual (la que cubre la instancia)\n",
    "        regla_factual = None\n",
    "        for regla in rules:\n",
    "            if cumple_regla(decoded, regla):\n",
    "                regla_factual = regla\n",
    "                break\n",
    "\n",
    "        \n",
    "\n",
    "        # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "        cf_rules_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la mÃ¡s sencilla (menos condiciones)\n",
    "                    cf_rules_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "        \n",
    "\n",
    "        # ========================================\n",
    "        # ðŸ“ MÃ‰TRICAS DE EXPLICACIÃ“N tipo LORE \n",
    "        # ========================================\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sintÃ©tico generado alrededor del punto a explicar.\n",
    "        y_bb = explanation[\"neighborhood_Yb\"] # predicciones del modelo BBOX (red neuronal) sobre Z (el vecindario).\n",
    "\n",
    "        y_surrogate_preds = explanation[\"surrogate_preds\"]  # predicciones del modelo interpretable (arbol) sobre Z (el vecindario).\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        # ==============================================================================================\n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ==============================================================================================\n",
    "\n",
    "        mask_same_class = (y_bb == pred_class_idx)\n",
    "        mask_diff_class = (y_bb != pred_class_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette = (b - a) / max(a, b)\n",
    "\n",
    "\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "        # Fidelity: Porcentaje de veces que el modelo interpretable (LORE tree) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Un valor alto de fidelity significa que el Ã¡rbol surrogate estÃ¡ imitando bien a la red neuronal para esa instancia.\n",
    "        # ===========================================================================================================================================================\n",
    "\n",
    "        fidelity = accuracy_score(y_bb, y_surrogate_preds)\n",
    "\n",
    "\n",
    "        # ====================================================================================================================================================================================================================================================\n",
    "        # Coverage: mide cuÃ¡ntas instancias del vecindario ð‘ (generado alrededor de la instancia a explicar) cumplen la regla factual ð‘. Es decir, calcula la proporciÃ³n de instancias en las que la regla es aplicable.\n",
    "\n",
    "        # PrecisiÃ³n: proporciÃ³n de las instancias del vecindario que cumplen la regla factual (es decir, de las instancias que cumplen el coverage) ademÃ¡s tienen que cumplir que el modelo black-box (tu red neuronal) predice la clase de la regla factual.\n",
    "        # =====================================================================================================================================================================================================================================================\n",
    "\n",
    "        # Decodifica cada fila del vecindario a un formato legible\n",
    "        dfZ_decoded = dfZ.apply(lambda row: self.decode_onehot_instance(\n",
    "            row.values, self.numeric_features, self.encoder, self.scaler, self.feature_names\n",
    "        ), axis=1)\n",
    "\n",
    "        cf_rules_por_clase_simplify = self._simplify_rules_by_class(cf_rules_por_clase, mode='loose')\n",
    "\n",
    "        regla_factual_simplify = None\n",
    "        \n",
    "        if regla_factual:\n",
    "            regla_factual_simplify = self._simplify_rule(regla_factual, mode='loose')\n",
    "            cumplen_regla = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual), axis=1)\n",
    "            coverage = cumplen_regla.mean()\n",
    "\n",
    "            \n",
    "            covered_target_match = (y_bb[cumplen_regla.values] == pred_class_idx)\n",
    "\n",
    "            if cumplen_regla.sum() > 0:\n",
    "                precision = covered_target_match.sum() / cumplen_regla.sum()\n",
    "            else:\n",
    "                precision = 0.0\n",
    "        else:\n",
    "            coverage = \"Ninguna regla factual cubre la instancia. No hay una regla en el Ã¡rbol que explique la predicciÃ³n sobre esa muestra\"\n",
    "            precision = \"Si no hay regla factual, no se puede calcular la precisiÃ³n (nÃºmero de aciertos entre las instancias cubiertas)\"\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"\\n[CLIENTE {self.client_id}] ðŸ§ª Instancia a explicar (decodificada):\\n{decoded}\\n\"\n",
    "            f\"ðŸ§ª Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\\n\"\n",
    "            f\"ðŸ§ª Clase predicha: {repr(pred_class)}\\n\"\n",
    "            # f\"{'Regla factual: ' + str(regla_factual) if regla_factual else 'Ninguna regla cubre la instancia. No hay explicaciÃ³n factual disponible para esta predicciÃ³n.'}\\n\"\n",
    "            f\"{'Regla factual simplificada: ' + str(regla_factual_simplify) if regla_factual_simplify is not None else 'Ninguna regla cubre la instancia. No hay explicaciÃ³n factual disponible para esta predicciÃ³n.'}\\n\"\n",
    "            # f\"Contrafactuales por clase: {cf_rules_por_clase}\\n\"\n",
    "            f\"Contrafactuales simplificado: {cf_rules_por_clase_simplify}\\n\"\n",
    "            f\"MÃ©tricas explicabilidad:\\n\"\n",
    "            f\"  - Silhouette: {silhouette:.3f}\\n\"\n",
    "            f\"  - Fidelity:   {fidelity:.3f}\\n\"\n",
    "            f\"  - Coverage factual:   {coverage}\\n\"\n",
    "            f\"  - Precision factual:  {precision}\\n\"\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    HOLDOUT_PID = NUM_PARTITIONS_TOTAL - 1\n",
    "    if partition_id == HOLDOUT_PID:\n",
    "        print(f\"El cliente con partition_id={HOLDOUT_PID} es el holdout del servidor y no debe iniciar un cliente Flower.\")\n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=NUM_PARTITIONS_TOTAL)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ“¦ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# âš™ï¸ CONFIGURACIÃ“N GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinÃ¡micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ§  UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net estÃ¡ en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# ðŸš€ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_PARTITIONS_TOTAL)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargÃ³ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"distinct_values_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        client_encoders[client_id] = json.loads(value)\n",
    "                        for feat, d in client_encoders[client_id].items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            all_means = []\n",
    "            all_stds = []\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"tree_ensemble_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(value)\n",
    "                        local_encoder = client_encoders[client_id]\n",
    "                        feature_names = json.loads(metrics.get(f\"encoded_feature_names_{client_id}\"))\n",
    "                        numeric_features = json.loads(metrics.get(f\"numeric_features_{client_id}\"))\n",
    "                        unique_labels = json.loads(metrics.get(f\"unique_labels_{client_id}\"))\n",
    "                        scaler = {\n",
    "                            \"mean\": json.loads(metrics.get(f\"scaler_mean_{client_id}\")),\n",
    "                            \"std\": json.loads(metrics.get(f\"scaler_std_{client_id}\")),\n",
    "                        }\n",
    "\n",
    "                        # Guarda los scalers de cada cliente\n",
    "                        all_means.append(scaler[\"mean\"])\n",
    "                        all_stds.append(scaler[\"std\"])\n",
    "                        \n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "\n",
    "                            # print(\"Local tree del cliente\", client_id)\n",
    "                            # print(\"root:\", root)\n",
    "                            # print(\"type:\", type(root))\n",
    "                            # print(\"dir(root):\", dir(root))\n",
    "                            # print(\"\\n\")\n",
    "\n",
    "                            tree_dicts.append(root)\n",
    "\n",
    "                # Calcular el scaler promedio\n",
    "                global_mean = np.mean(np.stack(all_means), axis=0)\n",
    "                global_std = np.mean(np.stack(all_stds), axis=0)\n",
    "                global_scaler = {\"mean\": global_mean, \"std\": global_std}\n",
    "\n",
    "\n",
    "                            \n",
    "            # print(tree_dicts)\n",
    "            \n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] âš ï¸ No se recibieron Ã¡rboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "            \n",
    "            supertree = SuperTree()\n",
    "            roots = tree_dicts\n",
    "            \n",
    "            supertree.mergeDecisionTrees(roots, num_classes=len(UNIQUE_LABELS), feature_names=feature_names, categorical_features=list(global_mapping.keys()), global_mapping=global_mapping)\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            supertree.merge_equal_class_leaves()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"supertree.root.to_dict(): \", supertree.root.to_dict())\n",
    "            # print(\"type:\", type(supertree.root))\n",
    "            # print(\"dir(supertree.root): \", dir(supertree.root))\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] ðŸŒ³ SuperTree legible (nombre de variables):\")\n",
    "            # print_supertree_legible_fusionado(\n",
    "            #     supertree.root,\n",
    "            #     feature_names=feature_names,\n",
    "            #     class_names=UNIQUE_LABELS,\n",
    "            #     numeric_features=numeric_features,\n",
    "            #     scaler=global_scaler,  # <-- ahora el scaler promedio\n",
    "            #     global_mapping=global_mapping\n",
    "            # )\n",
    "            \n",
    "            \n",
    "\n",
    "            save_supertree_plot(\n",
    "                root_node=supertree.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=UNIQUE_LABELS,\n",
    "                numeric_features=numeric_features,\n",
    "                scaler=global_scaler,\n",
    "                global_mapping=global_mapping\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "      \n",
    "            global_scaler = {\n",
    "                \"mean\": global_mean.tolist(),\n",
    "                \"std\": global_std.tolist()\n",
    "            }\n",
    "\n",
    "            GLOBAL_SCALER_JSON = json.dumps(global_scaler)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] âŒ Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# ðŸ§© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            \n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                ins.config[\"global_scaler\"] = GLOBAL_SCALER_JSON\n",
    "                \n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    import numpy as np\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    # ====== NUMÃ‰RICA ======\n",
    "    if feat_name in numeric_features:\n",
    "        idx = numeric_features.index(feat_name)\n",
    "        mean = scaler[\"mean\"][idx]\n",
    "        std = scaler[\"std\"][idx]\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "\n",
    "        # Robusto: asegura que bounds tiene len(children)+1\n",
    "        while len(bounds) < len(children) + 1:\n",
    "            bounds.append(np.inf)\n",
    "\n",
    "        if len(bounds) != len(children) + 1:\n",
    "            print(f\"[DEPURACIÃ“N] NUMÃ‰RICA '{feat_name}' mal construida\")\n",
    "            print(f\"    intervals: {intervals}\")\n",
    "            print(f\"    children: {len(children)}\")\n",
    "            print(f\"    bounds: {bounds}\")\n",
    "\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i + 1]\n",
    "            left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "            right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} â‰¤ {right_real:.2f}\"\n",
    "            elif i == len(children) - 1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} âˆˆ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ONEHOT ======\n",
    "    elif \"=\" in feat_name or \"_\" in feat_name:\n",
    "        # Soporta 'var=valor' o 'var_valor'\n",
    "        if \"=\" in feat_name:\n",
    "            var, val = feat_name.split(\"=\", 1)\n",
    "        else:\n",
    "            var, val = feat_name.split(\"_\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if len(children) != 2:\n",
    "            print(f\"[ERROR] Nodo OneHot {feat_name} tiene {len(children)} hijos, esperado 2.\")\n",
    "\n",
    "        # Primero !=, luego ==\n",
    "        conds = [\n",
    "            f'{var} != \"{val}\"',\n",
    "            f'{var} == \"{val}\"'\n",
    "        ]\n",
    "        for i, child in enumerate(children):\n",
    "            print(f\"{indent}{conds[i]}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== CATEGÃ“RICA ORDINAL ======\n",
    "    elif global_mapping and feat_name in global_mapping:\n",
    "        vals_cat = global_mapping[feat_name]\n",
    "        # Primero !=, luego ==\n",
    "        for i, child in enumerate(children):\n",
    "            try:\n",
    "                val_idx = node.intervals[i] if hasattr(node, \"intervals\") and i < len(node.intervals) else int(getattr(node, \"thresh\", 0))\n",
    "                val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "            except Exception as e:\n",
    "                print(f\"[DEPURACIÃ“N] Error interpretando categÃ³rica: {e}\")\n",
    "                val = \"?\"\n",
    "            cond = f'{feat_name} != \"{val}\"' if i == 0 else f'{feat_name} == \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    # ====== TIPO DESCONOCIDO ======\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres de features: {feature_names}\")\n",
    "        print(f\"    [DEPURACIÃ“N] Nombres numÃ©ricas: {numeric_features}\")\n",
    "        print(f\"    [DEPURACIÃ“N] global_mapping: {list(global_mapping.keys()) if global_mapping else None}\")\n",
    "        print(f\"    [DEPURACIÃ“N] children: {len(children)}\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,           # dict con mean y std\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\"\n",
    "):\n",
    "    from graphviz import Digraph\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        # Etiqueta del nodo\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            class_label = class_names[class_index]\n",
    "            label = f\"class: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            if \"_\" in fname:  # OneHotEncoder\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                label = var.strip()\n",
    "            else:\n",
    "                label = fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        # Nodos hijos (binario siempre)\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # --- Caso OneHotEncoder ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                # Solo dos hijos: [â‰ val, =val]\n",
    "                add_node(node.children[0], curr, f'â‰  \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'= \"{val}\"')\n",
    "            # --- Caso numÃ©rica ---\n",
    "            elif fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler[\"mean\"][idx]\n",
    "                std = scaler[\"std\"][idx]\n",
    "                # Solo dos hijos y un threshold\n",
    "                threshold = node.intervals[0]\n",
    "                thresh_real = threshold * std + mean if np.isfinite(threshold) else threshold\n",
    "                add_node(node.children[0], curr, f\"â‰¤ {thresh_real:.2f}\")\n",
    "                add_node(node.children[1], curr, f\"> {thresh_real:.2f}\")\n",
    "            # --- Caso categÃ³rica ordinal ---\n",
    "            elif fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                # Binario: solo dos hijos, dividir por primer valor\n",
    "                val = vals_cat[node.intervals[0]] if node.intervals and len(node.intervals) > 0 else \"?\"\n",
    "                add_node(node.children[0], curr, f'= \"{val}\"')\n",
    "                add_node(node.children[1], curr, f'â‰  \"{val}\"')\n",
    "            else:\n",
    "                # Caso raro/desconocido\n",
    "                for child in node.children:\n",
    "                    add_node(child, curr, \"?\")\n",
    "\n",
    "    # --- Guardado ---\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "    return f\"{filename}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ðŸ”§ INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 12:52:55,857\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-08-28 12:52:59,792 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] âœ… Red neuronal entrenada\n",
      "[CLIENTE 4] âœ… Red neuronal entrenada\n",
      "[CLIENTE 1] âœ… Red neuronal entrenada\n",
      "[CLIENTE 3] âœ… Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] âœ… Red neuronal entrenada\n",
      "[CLIENTE 1] âœ… Red neuronal entrenada\n",
      "[CLIENTE 4] âœ… Red neuronal entrenada\n",
      "[CLIENTE 3] âœ… Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "\n",
      "[CLIENTE 2] ðŸ§ª Instancia a explicar (decodificada):\n",
      "radiusMEAN                11.700000\n",
      "textureMEAN               19.110000\n",
      "perimeterMEAN             74.330000\n",
      "areaMEAN                 418.700000\n",
      "smoothnessMEAN             0.088140\n",
      "compactnessMEAN            0.052530\n",
      "concavityMEAN              0.015830\n",
      "concave pointsMEAN         0.011480\n",
      "symmetryMEAN               0.193600\n",
      "fractaldimensionMEAN       0.061280\n",
      "radiusSE                   0.160100\n",
      "textureSE                  1.430000\n",
      "perimeterSE                1.109000\n",
      "areaSE                    11.280000\n",
      "smoothnessSE               0.006064\n",
      "compactnessSE              0.009110\n",
      "concavitySE                0.010420\n",
      "concave pointsSE           0.007638\n",
      "symmetrySE                 0.023490\n",
      "fractalDimensionSE         0.001661\n",
      "radiusWORST               12.610000\n",
      "textureWORST              26.550000\n",
      "perimeterWORST            80.920000\n",
      "areaWORST                483.100000\n",
      "smoothnessWORST            0.122300\n",
      "compactnessWORST           0.108700\n",
      "concavityWORST             0.079150\n",
      "concavePointsWORST         0.057410\n",
      "symmetryWORST              0.348700\n",
      "fractalDimensionWORST      0.069580\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: B\n",
      "ðŸ§ª Clase predicha: 'B'\n",
      "Regla factual simplificada: ['perimeterWORST â‰¤ 84.06']\n",
      "Contrafactuales simplificado: {'M': ['areaSE â‰¤ 75.12', 'areaWORST â‰¤ 1023.38', 'concavePointsWORST â‰¤ 0.14', 'perimeterWORST > 84.06', 'radiusSE > 0.65', 'symmetryMEAN > 0.15', 'textureSE â‰¤ 2.53']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.756\n",
      "  - Fidelity:   0.973\n",
      "  - Coverage factual:   0.44816053511705684\n",
      "  - Precision factual:  1.0\n",
      "\n",
      "\n",
      "[CLIENTE 1] ðŸ§ª Instancia a explicar (decodificada):\n",
      "radiusMEAN                 20.640000\n",
      "textureMEAN                17.350000\n",
      "perimeterMEAN             134.800000\n",
      "areaMEAN                 1335.000000\n",
      "smoothnessMEAN              0.094460\n",
      "compactnessMEAN             0.107600\n",
      "concavityMEAN               0.152700\n",
      "concave pointsMEAN          0.089410\n",
      "symmetryMEAN                0.157100\n",
      "fractaldimensionMEAN        0.054780\n",
      "radiusSE                    0.613700\n",
      "textureSE                   0.657500\n",
      "perimeterSE                 4.119000\n",
      "areaSE                     77.020000\n",
      "smoothnessSE                0.006211\n",
      "compactnessSE               0.018950\n",
      "concavitySE                 0.026810\n",
      "concave pointsSE            0.012320\n",
      "symmetrySE                  0.012760\n",
      "fractalDimensionSE          0.001711\n",
      "radiusWORST                25.370000\n",
      "textureWORST               23.170000\n",
      "perimeterWORST            166.800000\n",
      "areaWORST                1946.000000\n",
      "smoothnessWORST             0.156200\n",
      "compactnessWORST            0.305500\n",
      "concavityWORST              0.415900\n",
      "concavePointsWORST          0.211200\n",
      "symmetryWORST               0.268900\n",
      "fractalDimensionWORST       0.070550\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: M\n",
      "ðŸ§ª Clase predicha: 'M'\n",
      "Regla factual simplificada: ['concavePointsWORST > 0.13', 'perimeterWORST > 100.40', 'radiusMEAN > 9.51', 'smoothnessMEAN > 0.08', 'symmetryMEAN > 0.15']\n",
      "Contrafactuales simplificado: {'B': ['radiusMEAN â‰¤ 9.51']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.908\n",
      "  - Fidelity:   0.997\n",
      "  - Coverage factual:   0.8795986622073578\n",
      "  - Precision factual:  1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLIENTE 3] ðŸ§ª Instancia a explicar (decodificada):\n",
      "radiusMEAN               8.734000e+00\n",
      "textureMEAN              1.684000e+01\n",
      "perimeterMEAN            5.527000e+01\n",
      "areaMEAN                 2.343000e+02\n",
      "smoothnessMEAN           1.039000e-01\n",
      "compactnessMEAN          7.428000e-02\n",
      "concavityMEAN            1.387779e-17\n",
      "concave pointsMEAN       0.000000e+00\n",
      "symmetryMEAN             1.985000e-01\n",
      "fractaldimensionMEAN     7.098000e-02\n",
      "radiusSE                 5.169000e-01\n",
      "textureSE                2.079000e+00\n",
      "perimeterSE              3.167000e+00\n",
      "areaSE                   2.885000e+01\n",
      "smoothnessSE             1.582000e-02\n",
      "compactnessSE            1.966000e-02\n",
      "concavitySE              0.000000e+00\n",
      "concave pointsSE         0.000000e+00\n",
      "symmetrySE               1.865000e-02\n",
      "fractalDimensionSE       6.736000e-03\n",
      "radiusWORST              1.017000e+01\n",
      "textureWORST             2.280000e+01\n",
      "perimeterWORST           6.401000e+01\n",
      "areaWORST                3.170000e+02\n",
      "smoothnessWORST          1.460000e-01\n",
      "compactnessWORST         1.310000e-01\n",
      "concavityWORST           0.000000e+00\n",
      "concavePointsWORST       0.000000e+00\n",
      "symmetryWORST            2.445000e-01\n",
      "fractalDimensionWORST    8.865000e-02\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: B\n",
      "ðŸ§ª Clase predicha: 'B'\n",
      "Regla factual simplificada: ['concavePointsWORST â‰¤ 0.05', 'fractalDimensionSE â‰¤ 0.02']\n",
      "Contrafactuales simplificado: {'M': ['areaWORST â‰¤ 1156.26', 'concavePointsWORST â‰¤ 0.05', 'fractalDimensionSE > 0.02', 'perimeterWORST > 109.31', 'radiusSE > 0.77', 'symmetryMEAN > 0.15']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.847\n",
      "  - Fidelity:   0.993\n",
      "  - Coverage factual:   0.451505016722408\n",
      "  - Precision factual:  0.9925925925925926\n",
      "\n",
      "\n",
      "[CLIENTE 4] ðŸ§ª Instancia a explicar (decodificada):\n",
      "radiusMEAN                12.720000\n",
      "textureMEAN               17.670000\n",
      "perimeterMEAN             80.980000\n",
      "areaMEAN                 501.300000\n",
      "smoothnessMEAN             0.078960\n",
      "compactnessMEAN            0.045220\n",
      "concavityMEAN              0.014020\n",
      "concave pointsMEAN         0.018350\n",
      "symmetryMEAN               0.145900\n",
      "fractaldimensionMEAN       0.055440\n",
      "radiusSE                   0.295400\n",
      "textureSE                  0.883600\n",
      "perimeterSE                2.109000\n",
      "areaSE                    23.240000\n",
      "smoothnessSE               0.007337\n",
      "compactnessSE              0.011740\n",
      "concavitySE                0.005383\n",
      "concave pointsSE           0.005623\n",
      "symmetrySE                 0.019400\n",
      "fractalDimensionSE         0.001180\n",
      "radiusWORST               13.820000\n",
      "textureWORST              20.960000\n",
      "perimeterWORST            88.870000\n",
      "areaWORST                586.800000\n",
      "smoothnessWORST            0.106800\n",
      "compactnessWORST           0.096050\n",
      "concavityWORST             0.034690\n",
      "concavePointsWORST         0.036120\n",
      "symmetryWORST              0.216500\n",
      "fractalDimensionWORST      0.060250\n",
      "dtype: float64\n",
      "ðŸ§ª Clase real: B\n",
      "ðŸ§ª Clase predicha: 'B'\n",
      "Regla factual simplificada: ['areaMEAN â‰¤ 617.34']\n",
      "Contrafactuales simplificado: {'M': ['areaMEAN > 617.34', 'compactnessSE â‰¤ 0.07', 'concavePointsWORST > 0.13', 'perimeterWORST > 102.37', 'smoothnessMEAN > 0.08', 'symmetryMEAN > 0.16', 'symmetrySE â‰¤ 0.03']}\n",
      "MÃ©tricas explicabilidad:\n",
      "  - Silhouette: 0.838\n",
      "  - Fidelity:   0.983\n",
      "  - Coverage factual:   0.45484949832775917\n",
      "  - Precision factual:  1.0\n",
      "\n",
      "\n",
      "[SERVIDOR] ðŸŒ² Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 77.64s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar aÃºn mÃ¡s\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesiÃ³n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
