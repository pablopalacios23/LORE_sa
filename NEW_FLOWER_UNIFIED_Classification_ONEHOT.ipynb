{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68391f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 12:24:20,621\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-07-16 12:24:24,188 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.piping.pipe(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-07-16 12:24:24,196 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.rendering.render(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-07-16 12:24:24,200 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.unflattening.unflatten(['stagger', 'fanout', 'chain', 'encoding'])\n",
      "2025-07-16 12:24:24,203 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.viewing.view(['quiet'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.quote(['is_html_string', 'is_valid_id', 'dot_keywords', 'endswith_odd_number_of_backslashes', 'escape_unescaped_quotes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.a_list(['kwargs', 'attributes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.attr_list(['kwargs', 'attributes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.clear(['keep_attrs'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.__iter__(['subgraph'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.node(['_attributes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.edge(['_attributes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.attr(['_attributes'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.subgraph(['name', 'comment', 'graph_attr', 'node_attr', 'edge_attr', 'body'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.piping.Pipe._pipe_legacy(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.saving.Save.save(['directory'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.render(['directory', 'view', 'cleanup', 'format', 'renderer', 'formatter', 'neato_no_op', 'quiet', 'quiet_view'])\n",
      "2025-07-16 12:24:24,209 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.view(['directory', 'cleanup', 'quiet', 'quiet_view'])\n",
      "2025-07-16 12:24:24,225 graphviz._tools DEBUG    deprecate positional args: graphviz.unflattening.Unflatten.unflatten(['stagger', 'fanout', 'chain'])\n",
      "2025-07-16 12:24:24,225 graphviz._tools DEBUG    deprecate positional args: graphviz.graphs.BaseGraph.__init__(['comment', 'filename', 'directory', 'format', 'engine', 'encoding', 'graph_attr', 'node_attr', 'edge_attr', 'body', 'strict'])\n",
      "2025-07-16 12:24:24,225 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.from_file(['directory', 'format', 'engine', 'encoding', 'renderer', 'formatter'])\n",
      "2025-07-16 12:24:24,225 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.__init__(['filename', 'directory', 'format', 'engine', 'encoding'])\n",
      "2025-07-16 12:24:24,225 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.save(['directory'])\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 📦 IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "# Otros\n",
    "from graphviz import Digraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ⚙️ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "\n",
    "# =======================\n",
    "# 🔧 UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 📥 CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def get_global_onehot_info(flower_dataset_name, class_col):\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_tmp = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "    df = fds_tmp.load_partition(0, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado estándar\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df = df[~df[\"workclass\"].isin([\" ?\"])]\n",
    "        df = df[~df[\"occupation\"].isin([\" ?\"])]\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "        df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].nunique() < 50:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    cat_features = [col for col in df.select_dtypes(include=\"category\").columns if col != class_col]\n",
    "    num_features = [col for col in df.columns if df[col].dtype.kind in \"fi\" and col != class_col]\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    ohe.fit(df[cat_features])\n",
    "    categories_global = ohe.categories_\n",
    "    onehot_columns = ohe.get_feature_names_out(cat_features).tolist()\n",
    "    return cat_features, num_features, categories_global, onehot_columns\n",
    "\n",
    "\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    # Saca info global siempre al principio\n",
    "    cat_features, num_features, categories_global, onehot_columns = get_global_onehot_info(flower_dataset_name, class_col)\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Preprocesado específico por dataset\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset = dataset[~dataset[\"workclass\"].isin([\" ?\"])]\n",
    "        dataset = dataset[~dataset[\"occupation\"].isin([\" ?\"])]\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataset[class_col])\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "    label_encoder.classes_ = np.array(UNIQUE_LABELS)\n",
    "    dataset[class_col] = label_encoder.transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", categories=categories_global), categorical_indices)\n",
    "    ])\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    # Reconstrucción del DataFrame\n",
    "    num_out = X_encoded[:, :len(numeric_features)]\n",
    "    cat_out = X_encoded[:, len(numeric_features):]\n",
    "    if categorical_features:\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        cat_names = []\n",
    "\n",
    "    num_names = numeric_features\n",
    "\n",
    "    X_df = pd.DataFrame(num_out, columns=num_names)\n",
    "    if len(cat_names) > 0:\n",
    "        X_cat_df = pd.DataFrame(cat_out, columns=cat_names)\n",
    "        X_full = pd.concat([X_df.reset_index(drop=True), X_cat_df.reset_index(drop=True)], axis=1)\n",
    "        for col in onehot_columns:\n",
    "            if col not in X_cat_df.columns:\n",
    "                X_full[col] = 0\n",
    "    else:\n",
    "        X_full = X_df\n",
    "\n",
    "    # Rellenar columnas onehot que falten y ordenar\n",
    "    final_columns = num_names + list(cat_names)\n",
    "    X_full = X_full[final_columns]\n",
    "    FEATURES[:] = final_columns\n",
    "\n",
    "    split_idx = int(0.8 * len(X_full))\n",
    "\n",
    "        # --- ¡Construye el descriptor global! ---\n",
    "    descriptor_global = descriptor.copy()\n",
    "    for i, col in enumerate(cat_features):\n",
    "        if col in descriptor_global[\"categorical\"]:\n",
    "            descriptor_global[\"categorical\"][col][\"distinct_values\"] = list(categories_global[i])\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor_global)\n",
    "\n",
    "    return (\n",
    "        X_full.iloc[:split_idx].to_numpy(), y[:split_idx],\n",
    "        X_full.iloc[split_idx:].to_numpy(), y[split_idx:],\n",
    "        tabular_dataset, final_columns, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/adult_small\"\n",
    "CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DEMASIADO GRANDE EL DATASET :/\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    " \n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 12:24:24,288 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-07-16 12:24:24,566 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-07-16 12:24:24,747 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small HTTP/11\" 200 612\n",
      "2025-07-16 12:24:24,873 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/adult_small.py HTTP/11\" 404 0\n",
      "2025-07-16 12:24:24,873 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-07-16 12:24:25,257 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/adult_small/pablopalacios23/adult_small.py HTTP/11\" 404 0\n",
      "2025-07-16 12:24:25,399 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/README.md HTTP/11\" 404 0\n",
      "2025-07-16 12:24:25,666 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-07-16 12:24:25,866 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-07-16 12:24:25,866 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-07-16 12:24:27,002 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/adult_small HTTP/11\" 200 None\n",
      "2025-07-16 12:24:27,134 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-07-16 12:24:27,301 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/tree/475f19aed5f80dea1d48deab705f11928fe27493?recursive=False&expand=False HTTP/11\" 200 204\n",
      "2025-07-16 12:24:27,450 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/tree/475f19aed5f80dea1d48deab705f11928fe27493/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-07-16 12:24:27,565 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-07-16 12:24:27,762 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-07-16 12:24:27,893 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-07-16 12:24:27,893 filelock     DEBUG    Attempting to acquire lock 2340250719520 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:27,893 filelock     DEBUG    Lock 2340250719520 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:27,893 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-07-16 12:24:27,909 filelock     DEBUG    Attempting to release lock 2340250719520 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:27,909 filelock     DEBUG    Lock 2340250719520 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:27,941 filelock     DEBUG    Attempting to acquire lock 2340254583120 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:27,941 filelock     DEBUG    Lock 2340254583120 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:27,941 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-07-16 12:24:27,941 filelock     DEBUG    Attempting to release lock 2340254583120 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:27,941 filelock     DEBUG    Lock 2340254583120 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:28,131 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-07-16 12:24:28,275 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small HTTP/11\" 200 612\n",
      "2025-07-16 12:24:28,418 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/adult_small.py HTTP/11\" 404 0\n",
      "2025-07-16 12:24:28,517 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/adult_small/pablopalacios23/adult_small.py HTTP/11\" 404 0\n",
      "2025-07-16 12:24:28,665 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/README.md HTTP/11\" 404 0\n",
      "2025-07-16 12:24:28,826 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-07-16 12:24:28,984 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/adult_small HTTP/11\" 200 None\n",
      "2025-07-16 12:24:29,133 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/tree/475f19aed5f80dea1d48deab705f11928fe27493/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-07-16 12:24:29,237 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-07-16 12:24:29,415 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-07-16 12:24:29,548 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-07-16 12:24:29,548 filelock     DEBUG    Attempting to acquire lock 2340258084432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:29,548 filelock     DEBUG    Lock 2340258084432 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:29,548 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-07-16 12:24:29,563 filelock     DEBUG    Attempting to release lock 2340258084432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:29,563 filelock     DEBUG    Lock 2340258084432 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-07-16 12:24:29,563 filelock     DEBUG    Attempting to acquire lock 2340133656848 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:29,563 filelock     DEBUG    Lock 2340133656848 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:29,563 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-07-16 12:24:29,563 filelock     DEBUG    Attempting to release lock 2340133656848 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-07-16 12:24:29,572 filelock     DEBUG    Lock 2340133656848 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 X_train (primeras filas):\n",
      "         0         1    2    3    4    5    6    7    8    9   ...   34   35  \\\n",
      "0  0.800327  1.345708  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1 -0.457330  0.681161  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  ...  1.0  0.0   \n",
      "2  0.342997 -2.375756  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0   \n",
      "3 -0.114332  0.016614  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4 -1.028992  0.016614  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  ...  1.0  0.0   \n",
      "5  1.143324  0.016614  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "    36   37   38   39   40   41   42   43  \n",
      "0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  1.0  \n",
      "1  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "2  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "3  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  \n",
      "5  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
      "\n",
      "[6 rows x 44 columns]\n",
      "\n",
      "🎯 y_train (primeros valores):\n",
      "[1 0 1 1 0 0]\n",
      "\n",
      "📦 X_test (primeras filas):\n",
      "         0         1    2    3    4    5    6    7    8    9   ...   34   35  \\\n",
      "0  1.143324  0.016614  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1 -1.829318  0.282433  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "    36   37   38   39   40   41   42   43  \n",
      "0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  1.0  \n",
      "1  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[2 rows x 44 columns]\n",
      "\n",
      "🎯 y_test (primeros valores):\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=1, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\n📦 X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\n🎯 y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\n📦 X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\n🎯 y_test (primeros valores):\")\n",
    "print(y_test)\n",
    "\n",
    "# print(encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 🌼 CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ✅ Red neuronal entrenada\")\n",
    "\n",
    "    def decode_onehot_instance(self, X_row, numeric_features, encoder, scaler, feature_names):\n",
    "        \"\"\"\n",
    "        X_row: array 1D (ya escalada y codificada con OneHotEncoder)\n",
    "        numeric_features: lista de nombres de variables numéricas originales\n",
    "        encoder: tu ColumnTransformerEnc, con .dataset_descriptor\n",
    "        scaler: StandardScaler ajustado a las numéricas\n",
    "        feature_names: nombres de TODAS las columnas tras preprocesar (el orden de X_row)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        # 1. Variables numéricas: desescalar y añadir\n",
    "        for i, col in enumerate(numeric_features):\n",
    "            mean = scaler.mean_[i]\n",
    "            std = scaler.scale_[i]\n",
    "            val = X_row[i] * std + mean\n",
    "            data[col] = val\n",
    "\n",
    "        # 2. Variables categóricas: busca los 1s en columnas OneHot\n",
    "        cat_map = encoder.dataset_descriptor[\"categorical\"]\n",
    "        cat_cols = list(cat_map.keys())\n",
    "        start = len(numeric_features)\n",
    "\n",
    "        for col in cat_cols:\n",
    "            # Todas las columnas onehot de esa variable\n",
    "            prefix = col + \"_\"\n",
    "            candidates = [fname for fname in feature_names if fname.startswith(prefix)]\n",
    "            found = False\n",
    "            for cname in candidates:\n",
    "                idx = feature_names.index(cname)\n",
    "                if X_row[idx] == 1:\n",
    "                    valor = cname[len(prefix):].strip()\n",
    "                    data[col] = valor\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                data[col] = None\n",
    "\n",
    "        return pd.Series(data)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "            self._train_nn()\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                \n",
    "                # print(\"supertree_dict\")\n",
    "                # print(\"supertree_dict:\", supertree_dict)\n",
    "                # print(\"type:\", type(supertree_dict))\n",
    "                # print(\"dir(supertree_dict):\", dir(supertree_dict))\n",
    "                # print(\"\\n\")\n",
    "\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "                self.global_scaler = json.loads(config[\"global_scaler\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ❌ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "\n",
    "        # print(f\"[CLIENTE {self.client_id}]\")\n",
    "        # print(export_text(self.tree_model, feature_names=FEATURES))\n",
    "        # print(\"root_node:\", root_node)\n",
    "        # print(\"type:\", type(root_node))\n",
    "        # print(dir(root_node))\n",
    "        # print(\"\\n\")\n",
    "        # print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "        \n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"scaler_mean_{self.client_id}\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            f\"scaler_std_{self.client_id}\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"encoder_descriptor_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "    \n",
    "        num_row = 0\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "    \n",
    "        \n",
    "        decoded = self.decode_onehot_instance(\n",
    "            self.X_test[num_row],\n",
    "            self.numeric_features,\n",
    "            self.encoder,\n",
    "            self.scaler,\n",
    "            self.feature_names\n",
    "        )\n",
    "\n",
    "        # print(f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (decodificada):\")\n",
    "        # print(decoded)\n",
    "        # print(f\"[CLIENTE {self.client_id}] 🧪 Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "\n",
    "        # Asegúrate de que X_test[num_row] es un numpy array del shape correcto (1, n_features)\n",
    "        x_tensor = torch.tensor(self.X_test[num_row], dtype=torch.float32).unsqueeze(0)  # shape: [1, n_features]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.nn_model(x_tensor)   # shape: [1, n_classes]\n",
    "            probs = torch.softmax(logits, dim=1).numpy()\n",
    "            pred_class_idx = int(probs.argmax(axis=1)[0])\n",
    "\n",
    "        # Si tienes un label_encoder:\n",
    "        pred_class = self.label_encoder.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "\n",
    "        # Ahora crea el TabularDataset legible\n",
    "        local_df = pd.DataFrame(self.X_train, columns=self.feature_names).astype(np.float32)\n",
    "        local_df[\"class\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"class\")    \n",
    "\n",
    "        # Explicabilidad local y la vecindad es generada del train (local_tabular_dataset)\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore_vecindad = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        \n",
    "        # Explicación LORE\n",
    "        x_instance = pd.Series(self.X_test[num_row], index=self.feature_names)\n",
    "        \n",
    "        explanation = lore_vecindad.explain_instance(x_instance, merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping, UNIQUE_LABELS=UNIQUE_LABELS)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        \n",
    "        # self.print_tree_readable(node=lore_tree.root,feature_names=self.feature_names,class_names=UNIQUE_LABELS,  numeric_features=self.numeric_features,scaler=self.scaler,encoder=self.encoder)\n",
    "        # print('\\n')\n",
    "\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        \n",
    "        self.save_lore_tree_image(lore_tree.root, round_number, self.feature_names, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder, folder=\"LoreTree\")\n",
    "\n",
    "\n",
    "        merged_tree = SuperTree()\n",
    "        merged_tree.mergeDecisionTrees(\n",
    "            roots=[lore_tree.root, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()), \n",
    "            global_mapping=self.global_mapping\n",
    "        )\n",
    "\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "\n",
    "        merged_tree.merge_equal_class_leaves()\n",
    "\n",
    "        self.save_supertree_plot(root_node=merged_tree.root,round_number=round_number,feature_names=self.feature_names,class_names=self.unique_labels,numeric_features=self.numeric_features,scaler=self.scaler,global_mapping=self.global_mapping,folder=\"MergedTree\")\n",
    "        \n",
    "        tree_str = self.tree_to_str(merged_tree.root, self.feature_names, numeric_features=self.numeric_features, scaler=self.scaler, global_mapping=self.global_mapping, unique_labels=self.unique_labels)\n",
    "\n",
    "        rules = self.extract_rules_from_str(tree_str, target_class_label=pred_class)\n",
    "\n",
    "\n",
    "        print(f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (decodificada):\")\n",
    "        print(decoded)\n",
    "\n",
    "        \n",
    "        print(f\"🧪 Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(f\"pred_class: {repr(pred_class)}\")\n",
    "    \n",
    "        def cumple_regla(instancia, regla):\n",
    "            for cond in regla:\n",
    "                if \"∧\" in cond:\n",
    "                    # Maneja condiciones tipo intervalo: 'age > 44.33 ∧ ≤ 48.50'\n",
    "                    import re\n",
    "                    # Busca: variable, operador1, valor1, operador2, valor2\n",
    "                    m = re.match(r'(.+?)([><]=?|≤|≥)\\s*([-\\d\\.]+)\\s*∧\\s*([><]=?|≤|≥)\\s*([-\\d\\.]+)', cond)\n",
    "                    if m:\n",
    "                        var = m.group(1).strip()\n",
    "                        op1, val1 = m.group(2), float(m.group(3))\n",
    "                        op2, val2 = m.group(4), float(m.group(5))\n",
    "                        v = instancia[var]\n",
    "                        # Evalúa las dos condiciones del intervalo\n",
    "                        if not (\n",
    "                            eval(f\"v {op1.replace('≤','<=').replace('≥','>=')} {val1}\") and\n",
    "                            eval(f\"v {op2.replace('≤','<=').replace('≥','>=')} {val2}\")\n",
    "                        ):\n",
    "                            return False\n",
    "                        continue  # sigue al siguiente cond\n",
    "                # ... resto de tu código tal cual ...\n",
    "                if \"≤\" in cond:\n",
    "                    var, val = cond.split(\"≤\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] > val:\n",
    "                        return False\n",
    "                elif \">=\" in cond or \"≥\" in cond:\n",
    "                    var, val = cond.replace(\"≥\", \">=\").split(\">=\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] < val:\n",
    "                        return False\n",
    "                elif \">\" in cond:\n",
    "                    var, val = cond.split(\">\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] <= val:\n",
    "                        return False\n",
    "                elif \"<\" in cond:\n",
    "                    var, val = cond.split(\"<\")\n",
    "                    var = var.strip()\n",
    "                    val = float(val.strip())\n",
    "                    if instancia[var] >= val:\n",
    "                        return False\n",
    "                elif \"≠\" in cond:\n",
    "                    var, val = cond.split(\"≠\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] == val:\n",
    "                        return False\n",
    "                elif \"=\" in cond:\n",
    "                    var, val = cond.split(\"=\")\n",
    "                    var = var.strip()\n",
    "                    val = val.strip().replace('\"', \"\")\n",
    "                    if instancia[var] != val:\n",
    "                        return False\n",
    "            return True\n",
    "\n",
    "        # Buscar la regla factual (la que cubre la instancia)\n",
    "        regla_factual = None\n",
    "        for regla in rules:\n",
    "            if cumple_regla(decoded, regla):\n",
    "                regla_factual = regla\n",
    "                break\n",
    "\n",
    "        if regla_factual:\n",
    "            print(\"Regla factual encontrada:\", regla_factual)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"Ninguna regla cubre la instancia. No hay explicación factual disponible para esta predicción.\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Extraer 1 contrafactual por cada clase distinta a la predicha\n",
    "        cf_rules_por_clase = {}\n",
    "        for clase in self.unique_labels:\n",
    "            if clase != pred_class:\n",
    "                rules_clase = self.extract_rules_from_str(tree_str, target_class_label=clase)\n",
    "                if rules_clase:\n",
    "                    # Elige la más sencilla (menos condiciones)\n",
    "                    cf_rules_por_clase[clase] = min(rules_clase, key=len)\n",
    "\n",
    "        print(\"cf_rules_por_clase:\", cf_rules_por_clase)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # ========================================\n",
    "        # 📏 MÉTRICAS DE EXPLICACIÓN tipo LORE \n",
    "        # ========================================\n",
    "\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sintético generado alrededor del punto a explicar.\n",
    "        y_surrogate = explanation[\"neighborhood_Yb\"] # predicciones del modelo interpretable (arbol) sobre Z.\n",
    "\n",
    "        # print(\"y_surrogate\")\n",
    "        # print(y_surrogate)\n",
    "\n",
    "        y_nn = nn_wrapper.predict(Z)\n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.feature_names)\n",
    "\n",
    "\n",
    "        # ==============================================================================================\n",
    "        # Silhouette:  Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        # ==============================================================================================\n",
    "\n",
    "        mask_same_class = (y_nn == pred_class_idx)\n",
    "        mask_diff_class = (y_nn != pred_class_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        x = self.X_test[num_row]\n",
    "\n",
    "        a = pairwise_distances([x], Z_plus).mean() if len(Z_plus) > 0 else 0.0\n",
    "\n",
    "        b = pairwise_distances([x], Z_minus).mean() if len(Z_minus) > 0 else 0.0\n",
    "\n",
    "        silhouette = 0.0\n",
    "        if (a + b) > 0:\n",
    "            silhouette = (b - a) / max(a, b)\n",
    "\n",
    "        print(f\"Silhouette: {silhouette:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ===========================================================================================================================================================\n",
    "        # Fidelity: Porcentaje de veces que el modelo interpretable (LORE tree) predice lo mismo que el modelo original (Red neuronal) en el vecindario generado.\n",
    "\n",
    "        # Un valor alto de fidelity significa que el árbol está imitando bien a la red neuronal para esa instancia.\n",
    "        # ===========================================================================================================================================================\n",
    "\n",
    "        fidelity = accuracy_score(y_nn, y_surrogate)\n",
    "\n",
    "        print(f\"Fidelity: {fidelity:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ==============================================================================================================================================================================================================\n",
    "        # Coverage: mide cuántas instancias del vecindario 𝑍 (generado alrededor de la instancia a explicar) cumplen la regla factual 𝑝. Es decir, calcula la proporción de instancias en las que la regla es aplicable.\n",
    "\n",
    "        # Precisión: proporción de las instancias del vecindario que cumplen la regla factual y que, además, el modelo black-box (tu red neuronal) predice la clase de la regla factual.\n",
    "        # ==============================================================================================================================================================================================================\n",
    "\n",
    "        # Decodifica cada fila del vecindario a un formato legible\n",
    "        dfZ_decoded = dfZ.apply(lambda row: self.decode_onehot_instance(\n",
    "            row.values, self.numeric_features, self.encoder, self.scaler, self.feature_names\n",
    "        ), axis=1)\n",
    "        \n",
    "\n",
    "        if regla_factual:\n",
    "            cumplen_regla = dfZ_decoded.apply(lambda row: cumple_regla(row, regla_factual), axis=1)\n",
    "            coverage = cumplen_regla.mean()\n",
    "            covered_target_match = (y_nn[cumplen_regla.values] == pred_class_idx)\n",
    "            if cumplen_regla.sum() > 0:\n",
    "                precision = covered_target_match.sum() / cumplen_regla.sum()\n",
    "            else:\n",
    "                precision = 0.0\n",
    "        else:\n",
    "            coverage = 0.0\n",
    "            precision = 0.0\n",
    "\n",
    "        print(f\"Coverage: {coverage:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def extract_rules_from_str(self, tree_str, target_class_label, exclude=False):\n",
    "        target_class_label = target_class_label.strip()\n",
    "        lines = tree_str.strip().split(\"\\n\")\n",
    "        path = []\n",
    "        rules = []\n",
    "\n",
    "        def recurse(idx, indent_level):\n",
    "            seen = set()\n",
    "            while idx < len(lines):\n",
    "                line = lines[idx]\n",
    "                current_indent = len(line) - len(line.lstrip())\n",
    "                if current_indent < indent_level:\n",
    "                    return idx\n",
    "                if \"⮕\" in line:\n",
    "                    import re\n",
    "                    m = re.search(r'class = \"([^\"]+)\"', line)\n",
    "                    leaf_class = m.group(1).strip() if m else None\n",
    "                    condition = (leaf_class == target_class_label)\n",
    "                    if exclude:\n",
    "                        condition = not condition  # cambia la lógica\n",
    "                    if condition:\n",
    "                        cleaned = []\n",
    "                        for cond in path:\n",
    "                            if cond not in seen:\n",
    "                                cleaned.append(cond)\n",
    "                                seen.add(cond)\n",
    "                        rules.append(cleaned)\n",
    "                    return idx + 1\n",
    "                elif \"if\" in line:\n",
    "                    condition = line.strip()[3:]\n",
    "                    path.append(condition)\n",
    "                    idx = recurse(idx + 1, current_indent + 2)\n",
    "                    path.pop()\n",
    "                else:\n",
    "                    idx += 1\n",
    "            return idx\n",
    "\n",
    "        recurse(0, 0)\n",
    "        return rules\n",
    "    \n",
    "\n",
    "\n",
    "    def decode_onehot_instance(self, X_row, numeric_features, encoder, scaler, feature_names):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        x_named = pd.Series(X_row, index=feature_names)\n",
    "        data = {}\n",
    "\n",
    "        # Numéricas\n",
    "        for i, col in enumerate(numeric_features):\n",
    "            if col in x_named:\n",
    "                val = x_named[col]\n",
    "                idx = numeric_features.index(col)\n",
    "                mean = scaler.mean_[idx]\n",
    "                std = scaler.scale_[idx]\n",
    "                data[col] = val * std + mean\n",
    "            else:\n",
    "                data[col] = None\n",
    "\n",
    "        # Categóricas\n",
    "        cat_map = encoder.dataset_descriptor[\"categorical\"]\n",
    "        for cat in cat_map:\n",
    "            onehot_names = [c for c in feature_names if c.startswith(cat + \"_\")]\n",
    "            val_found = None\n",
    "            for c in onehot_names:\n",
    "                if c in x_named and x_named[c] == 1:\n",
    "                    val_found = c[len(cat) + 1 :]\n",
    "                    break\n",
    "            if val_found is not None:\n",
    "                data[cat] = val_found.strip()\n",
    "            else:\n",
    "                data[cat] = None  # O \"?\"\n",
    "\n",
    "        return pd.Series(data)\n",
    "    \n",
    "    def decode_Xtrain_to_df(self, X_test, numeric_features, encoder, scaler, feature_names):\n",
    "        # Lista de diccionarios para cada fila\n",
    "        decoded_rows = []\n",
    "        for x in X_test:\n",
    "            decoded = self.decode_onehot_instance(x, numeric_features, encoder, scaler, feature_names)\n",
    "            decoded_rows.append(decoded)\n",
    "        df = pd.DataFrame(decoded_rows)\n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def print_tree_readable(self, node, feature_names, class_names, numeric_features, scaler, encoder, depth=0):\n",
    "        indent = \"|   \" * depth\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_idx = int(np.argmax(node.labels))\n",
    "            print(f\"{indent}|--- class: {class_names[class_idx]}\")\n",
    "            return\n",
    "\n",
    "        feat_name = feature_names[node.feat]\n",
    "\n",
    "        # --- CASO NUMÉRICA ---\n",
    "        if feat_name in numeric_features:\n",
    "            idx = numeric_features.index(feat_name)\n",
    "            threshold = node.thresh * scaler.scale_[idx] + scaler.mean_[idx]\n",
    "            print(f\"{indent}|--- {feat_name} <= {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            print(f\"{indent}|--- {feat_name} > {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            return\n",
    "\n",
    "        # --- CASO CATEGÓRICA ONE-HOT ---\n",
    "        if \"=\" in feat_name:\n",
    "            # Ejemplo: occupation= Adm-clerical\n",
    "            var, valor = feat_name.split(\"=\")\n",
    "            var = var.strip()\n",
    "            valor = valor.strip()\n",
    "            # Si threshold == 0.5, OneHot típico: <= 0.5 (no es ese valor), > 0.5 (es ese valor)\n",
    "            if node.thresh == 0.5:\n",
    "                print(f\"{indent}|--- {var} == \\\"{valor}\\\"\")\n",
    "                self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "                print(f\"{indent}|--- {var} != \\\"{valor}\\\"\")\n",
    "                self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            else:\n",
    "                # Por si hay rarezas (poco frecuente)\n",
    "                print(f\"{indent}|--- {feat_name} <= {node.thresh:.2f}\")\n",
    "                self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "                print(f\"{indent}|--- {feat_name} > {node.thresh:.2f}\")\n",
    "                self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            return\n",
    "\n",
    "        # --- SI NO ENCAJA ---\n",
    "        print(f\"{indent}|--- {feat_name} <= {node.thresh:.2f}\")\n",
    "        self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "        print(f\"{indent}|--- {feat_name} > {node.thresh:.2f}\")\n",
    "        self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tree_to_str(self, node, feature_names, numeric_features=None, scaler=None, global_mapping=None, unique_labels=None, depth=0):\n",
    "        indent = \"  \" * depth\n",
    "        result = \"\"\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_idx = int(np.argmax(node.labels))\n",
    "            class_label = unique_labels[class_idx] if unique_labels is not None else str(class_idx)\n",
    "            result += f'{indent}⮕ Leaf: class = \"{class_label.strip()}\" | {node.labels}\\n'\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "\n",
    "            # --- Split OneHot ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                for i, child in enumerate(node.children):\n",
    "                    cond = f'{var} {\"≠\" if i == 0 else \"=\"} \"{val}\"'\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "\n",
    "            # --- Split categórico ordinal ---\n",
    "            elif global_mapping and fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                for i, child in enumerate(node.children):\n",
    "                    val_idx = node.intervals[i] if hasattr(node, \"intervals\") else int(node.thresh)\n",
    "                    val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                    cond = f'{fname} {\"≠\" if i == 0 else \"=\"} \"{val}\"'\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "\n",
    "            # --- Split numérico ---\n",
    "            elif numeric_features and fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler.mean_[idx]\n",
    "                std = scaler.scale_[idx]\n",
    "                bounds = [-np.inf] + list(node.intervals)\n",
    "                for i, child in enumerate(node.children):\n",
    "                    left = bounds[i]\n",
    "                    right = bounds[i+1]\n",
    "                    left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "                    right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "                    if i == 0:\n",
    "                        cond = f\"{fname} ≤ {right_real:.2f}\"\n",
    "                    elif i == len(node.children)-1:\n",
    "                        cond = f\"{fname} > {left_real:.2f}\"\n",
    "                    else:\n",
    "                        cond = f\"{fname} ∈ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "                    result += f\"{indent}if {cond}\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "            else:\n",
    "                # Por si acaso, caso no detectado\n",
    "                for child in node.children:\n",
    "                    result += f\"{indent}if {fname} ?\\n\"\n",
    "                    result += self.tree_to_str(child, feature_names, numeric_features, scaler, global_mapping, unique_labels, depth + 1)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def save_supertree_plot(self, root_node, round_number, feature_names, class_names, numeric_features, scaler, global_mapping, folder=\"Supertree\"):\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1\n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = int(np.argmax(node.labels))\n",
    "                class_label = class_names[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                fname = feature_names[node.feat]\n",
    "                # Si es OneHot: \"sex_ Male\"\n",
    "                if \"_\" in fname:\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    label = var.strip()\n",
    "                else:\n",
    "                    label = fname\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Nodos hijos\n",
    "            if not node.is_leaf:\n",
    "                fname = feature_names[node.feat]\n",
    "                # --- Caso OneHotEncoder ---\n",
    "                if \"_\" in fname:\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    left_label = f'≠ \"{val}\"'    # <= 0.5\n",
    "                    right_label = f'= \"{val}\"'   # > 0.5\n",
    "                    add_node(node.children[0], curr, left_label)\n",
    "                    add_node(node.children[1], curr, right_label)\n",
    "\n",
    "                # --- Caso numérica ---\n",
    "                elif fname in numeric_features:\n",
    "                    idx = numeric_features.index(fname)\n",
    "                    mean = scaler.mean_[idx]\n",
    "                    std = scaler.scale_[idx]\n",
    "                    bounds = [-np.inf] + list(node.intervals)\n",
    "                    for i, child in enumerate(node.children):\n",
    "                        left = bounds[i]\n",
    "                        right = bounds[i+1]\n",
    "                        left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "                        right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "                        if i == 0:\n",
    "                            cond = f\"≤ {right_real:.2f}\"\n",
    "                        elif i == len(node.children)-1:\n",
    "                            cond = f\"> {left_real:.2f}\"\n",
    "                        else:\n",
    "                            cond = f\"∈ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "                        add_node(child, curr, cond)\n",
    "\n",
    "                # --- Caso categórica ordinal ---\n",
    "                elif fname in global_mapping:\n",
    "                    vals_cat = global_mapping[fname]\n",
    "                    for i, child in enumerate(node.children):\n",
    "                        val = vals_cat[node.intervals[i] if i < len(node.intervals) else -1]\n",
    "                        edge = f'= \"{val}\"' if i == 0 else f'≠ \"{val}\"'\n",
    "                        add_node(child, curr, edge)\n",
    "                else:\n",
    "                    # Desconocido\n",
    "                    for child in node.children:\n",
    "                        add_node(child, curr, \"?\")\n",
    "\n",
    "        # --- Guardado ---\n",
    "        folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        filename = f\"{folder_path}/LoreTree_cliente{self.client_id}_Supertree_ronda_{round_number}\"\n",
    "        add_node(root_node)\n",
    "        dot.render(filename, format=\"png\", cleanup=True)\n",
    "        return f\"{filename}.png\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_lore_tree_image(self, root_node, round_number, feature_names, numeric_features, scaler, unique_labels, encoder, tree_type=\"LoreTree\", folder = \"LoreTree\"):\n",
    "\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            match = re.match(r\"([a-zA-Z0-9\\- ]+)\", feat)\n",
    "            return match.group(1).strip() if match else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1 \n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = int(np.argmax(node.labels))\n",
    "                class_label = unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Recorrido binario (OneHot y splits normales)\n",
    "            if hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                except:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                # Si es OneHot\n",
    "                if \"=\" in fname or \"_\" in fname:\n",
    "                    if \"=\" in fname:\n",
    "                        var, val = fname.split(\"=\")\n",
    "                    else:\n",
    "                        var, val = fname.split(\"_\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    left_label = f'≠ \"{val}\"'   # <= 0.5 → no es ese valor\n",
    "                    right_label = f'= \"{val}\"'  # > 0.5  → sí es ese valor\n",
    "                else:\n",
    "                    original_feat = base_name(fname)\n",
    "\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = int(node.thresh)\n",
    "                        vals_cat = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]\n",
    "                        val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                        left_label = f'= \"{val}\"'\n",
    "                        right_label = f'≠ \"{val}\"'\n",
    "\n",
    "                    elif fname in numeric_features:\n",
    "                        idx = numeric_features.index(fname)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        thresh = node.thresh * std + mean\n",
    "                        left_label = f\"<= {thresh:.2f}\"\n",
    "                        right_label = f\"> {thresh:.2f}\"\n",
    "\n",
    "                    else:\n",
    "                        left_label = \"≤ ?\"\n",
    "                        right_label = \"> ?\"\n",
    "\n",
    "                if hasattr(node, \"_left_child\") and node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if hasattr(node, \"_right_child\") and node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        filename = f\"{folder_path}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        add_node(root_node)\n",
    "        dot.render(filename, format=\"png\", cleanup=True)\n",
    "        return f\"{filename}.png\"\n",
    "            \n",
    "\n",
    "    \n",
    "    def _save_local_tree(self, root_node, round_number, feature_names, numeric_features, scaler, unique_labels, encoder, tree_type= \"LocalTree\"):\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            # Extrae solo el nombre de la variable, antes de '_' o '=' o espacios\n",
    "            match = re.match(r\"([a-zA-Z0-9\\- ]+)\", feat)\n",
    "            return match.group(1).strip() if match else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1 \n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = np.argmax(node.labels)\n",
    "                class_label = unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Árbol tipo SuperTree\n",
    "            if hasattr(node, \"children\") and node.children is not None and hasattr(node, \"intervals\"):\n",
    "                for i, child in enumerate(node.children):\n",
    "                    try:\n",
    "                        fname = feature_names[node.feat]\n",
    "                    except:\n",
    "                        fname = f\"X_{node.feat}\"\n",
    "\n",
    "                    original_feat = base_name(fname)\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val_idx = int(val_idx)\n",
    "                        vals_cat = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]\n",
    "                        val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                        edge = f'= \"{val}\"' if i == 0 else f'≠ \"{val}\"'\n",
    "                    elif original_feat in numeric_features:\n",
    "                        idx = numeric_features.index(original_feat)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val = val * std + mean\n",
    "                        edge = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "                    else:\n",
    "                        edge = \"?\"\n",
    "\n",
    "                    add_node(child, curr, edge)\n",
    "\n",
    "            elif hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                except:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                # Si es OneHot\n",
    "                if \"_\" in fname:\n",
    "                    var, val = fname.split(\"_\", 1)\n",
    "                    var = var.strip()\n",
    "                    val = val.strip()\n",
    "                    # La split es: Si sex_ Male <= 0.5  (NO es Male)\n",
    "                    #              Si sex_ Male > 0.5   (SÍ es Male)\n",
    "                    left_label = f'≠ \"{val}\"'   # <= 0.5 → no es ese valor\n",
    "                    right_label = f'= \"{val}\"'  # > 0.5  → sí es ese valor\n",
    "                else:\n",
    "                    original_feat = base_name(fname)\n",
    "\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = int(node.thresh)\n",
    "                        vals_cat = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]\n",
    "                        val = vals_cat[val_idx] if val_idx < len(vals_cat) else f\"desconocido({val_idx})\"\n",
    "                        left_label = f'= \"{val}\"'\n",
    "                        right_label = f'≠ \"{val}\"'\n",
    "\n",
    "                    elif fname in numeric_features:\n",
    "                        idx = numeric_features.index(fname)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        thresh = node.thresh * std + mean\n",
    "                        left_label = f\"<= {thresh:.2f}\"\n",
    "                        right_label = f\"> {thresh:.2f}\"\n",
    "                        \n",
    "                    else:\n",
    "                        left_label = \"≤ ?\"\n",
    "                        right_label = \"> ?\"\n",
    "\n",
    "                if node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        add_node(root_node)\n",
    "        folder = f\"Ronda_{round_number}/{tree_type}_Cliente_{self.client_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        filepath = f\"{folder}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        dot.render(filepath, format=\"png\", cleanup=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    \n",
    "    # dataset_name = context.node_config.get(\"dataset_name\", \"pablopalacios23/Iris\")\n",
    "    # class_col = context.node_config.get(\"class_col\", \"target\")\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=num_partitions)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📦 IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ⚙️ CONFIGURACIÓN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinámicamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "GLOBAL_SCALER_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 🧠 UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net está en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# 🚀 SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargó antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] 🌲 Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"distinct_values_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        client_encoders[client_id] = json.loads(value)\n",
    "                        for feat, d in client_encoders[client_id].items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            all_means = []\n",
    "            all_stds = []\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"tree_ensemble_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(value)\n",
    "                        local_encoder = client_encoders[client_id]\n",
    "                        feature_names = json.loads(metrics.get(f\"encoded_feature_names_{client_id}\"))\n",
    "                        numeric_features = json.loads(metrics.get(f\"numeric_features_{client_id}\"))\n",
    "                        unique_labels = json.loads(metrics.get(f\"unique_labels_{client_id}\"))\n",
    "                        scaler = {\n",
    "                            \"mean\": json.loads(metrics.get(f\"scaler_mean_{client_id}\")),\n",
    "                            \"std\": json.loads(metrics.get(f\"scaler_std_{client_id}\")),\n",
    "                        }\n",
    "\n",
    "                        # Guarda los scalers de cada cliente\n",
    "                        all_means.append(scaler[\"mean\"])\n",
    "                        all_stds.append(scaler[\"std\"])\n",
    "                        \n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "\n",
    "                            # print(\"Local tree del cliente\", client_id)\n",
    "                            # print(\"root:\", root)\n",
    "                            # print(\"type:\", type(root))\n",
    "                            # print(\"dir(root):\", dir(root))\n",
    "                            # print(\"\\n\")\n",
    "\n",
    "                            tree_dicts.append(root)\n",
    "\n",
    "                # Calcular el scaler promedio\n",
    "                global_mean = np.mean(np.stack(all_means), axis=0)\n",
    "                global_std = np.mean(np.stack(all_stds), axis=0)\n",
    "                global_scaler = {\"mean\": global_mean, \"std\": global_std}\n",
    "\n",
    "\n",
    "                            \n",
    "            # print(tree_dicts)\n",
    "            \n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] ⚠️ No se recibieron árboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "            \n",
    "            supertree = SuperTree()\n",
    "            roots = tree_dicts\n",
    "            \n",
    "            supertree.mergeDecisionTrees(roots, num_classes=len(UNIQUE_LABELS), feature_names=feature_names, categorical_features=list(global_mapping.keys()), global_mapping=global_mapping)\n",
    "            # print(\"\\n[SERVIDOR] SuperTree unpruned:\")\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree prune_redundant_leaves_full:\")\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] SuperTree merge_equal_class_leaves:\")\n",
    "            supertree.merge_equal_class_leaves()\n",
    "            # print(supertree)\n",
    "            # print(\"\\n\")\n",
    "            \n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "            # print(\"supertree.root.to_dict(): \", supertree.root.to_dict())\n",
    "            # print(\"type:\", type(supertree.root))\n",
    "            # print(\"dir(supertree.root): \", dir(supertree.root))\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] 🌳 SuperTree legible (nombre de variables):\")\n",
    "            # print_supertree_legible_fusionado(\n",
    "            #     supertree.root,\n",
    "            #     feature_names=feature_names,\n",
    "            #     class_names=UNIQUE_LABELS,\n",
    "            #     numeric_features=numeric_features,\n",
    "            #     scaler=global_scaler,  # <-- ahora el scaler promedio\n",
    "            #     global_mapping=global_mapping\n",
    "            # )\n",
    "            \n",
    "\n",
    "            save_supertree_plot(\n",
    "                root_node=supertree.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=UNIQUE_LABELS,\n",
    "                numeric_features=numeric_features,\n",
    "                scaler=global_scaler,\n",
    "                global_mapping=global_mapping\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "      \n",
    "            global_scaler = {\n",
    "                \"mean\": global_mean.tolist(),\n",
    "                \"std\": global_std.tolist()\n",
    "            }\n",
    "\n",
    "            GLOBAL_SCALER_JSON = json.dumps(global_scaler)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ❌ Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# 🧩 FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON, GLOBAL_SCALER_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            \n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                ins.config[\"global_scaler\"] = GLOBAL_SCALER_JSON\n",
    "                \n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "def print_supertree_legible_fusionado(\n",
    "    node,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,  # dict con mean y std\n",
    "    global_mapping,\n",
    "    depth=0\n",
    "):\n",
    "    indent = \"|   \" * depth\n",
    "    if node is None:\n",
    "        print(f\"{indent}[Nodo None]\")\n",
    "        return\n",
    "\n",
    "    if getattr(node, \"is_leaf\", False):\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}class: {class_names[class_idx]} (pred: {node.labels})\")\n",
    "        return\n",
    "\n",
    "    feat_idx = node.feat\n",
    "    feat_name = feature_names[feat_idx]\n",
    "    intervals = node.intervals\n",
    "    children = node.children\n",
    "\n",
    "    if feat_name in numeric_features:\n",
    "        idx = numeric_features.index(feat_name)\n",
    "        mean = scaler[\"mean\"][idx]\n",
    "        std = scaler[\"std\"][idx]\n",
    "        # Construir los límites reales\n",
    "        bounds = [-np.inf] + list(intervals)\n",
    "        for i, child in enumerate(children):\n",
    "            left = bounds[i]\n",
    "            right = bounds[i+1]\n",
    "            left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "            right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "\n",
    "            if i == 0:\n",
    "                cond = f\"{feat_name} ≤ {right_real:.2f}\"\n",
    "            elif i == len(children)-1:\n",
    "                cond = f\"{feat_name} > {left_real:.2f}\"\n",
    "            else:\n",
    "                cond = f\"{feat_name} ∈ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "    elif \"=\" in feat_name:\n",
    "        var, val = feat_name.split(\"=\", 1)\n",
    "        var = var.strip()\n",
    "        val = val.strip()\n",
    "        for i, child in enumerate(children):\n",
    "            cond = f'{var} == \"{val}\"' if i == 0 else f'{var} != \"{val}\"'\n",
    "            print(f\"{indent}{cond}\")\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "    else:\n",
    "        print(f\"{indent}{feat_name} [tipo desconocido]\")\n",
    "        for child in children:\n",
    "            print_supertree_legible_fusionado(\n",
    "                child, feature_names, class_names, numeric_features, scaler, global_mapping, depth + 1\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def save_supertree_plot(\n",
    "    root_node,\n",
    "    round_number,\n",
    "    feature_names,\n",
    "    class_names,\n",
    "    numeric_features,\n",
    "    scaler,           # dict con mean y std\n",
    "    global_mapping,\n",
    "    folder=\"Supertree\"\n",
    "):\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def add_node(node, parent=None, edge_label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        # Etiqueta del nodo\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            class_label = class_names[class_index]\n",
    "            label = f\"class: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat]\n",
    "            # Si es OneHot: \"sex_ Male\"\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                label = var.strip()\n",
    "            else:\n",
    "                label = fname\n",
    "\n",
    "        dot.node(curr, label)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "        # Nodos hijos\n",
    "        if not node.is_leaf:\n",
    "            fname = feature_names[node.feat]\n",
    "            # --- Caso OneHotEncoder ---\n",
    "            if \"_\" in fname:\n",
    "                var, val = fname.split(\"_\", 1)\n",
    "                var = var.strip()\n",
    "                val = val.strip()\n",
    "                left_label = f'≠ \"{val}\"'    # <= 0.5\n",
    "                right_label = f'= \"{val}\"'   # > 0.5\n",
    "                # Solo 2 hijos: left y right\n",
    "                add_node(node.children[0], curr, left_label)\n",
    "                add_node(node.children[1], curr, right_label)\n",
    "\n",
    "            # --- Caso numérica ---\n",
    "            elif fname in numeric_features:\n",
    "                idx = numeric_features.index(fname)\n",
    "                mean = scaler[\"mean\"][idx]\n",
    "                std = scaler[\"std\"][idx]\n",
    "                bounds = [-np.inf] + list(node.intervals)\n",
    "                for i, child in enumerate(node.children):\n",
    "                    left = bounds[i]\n",
    "                    right = bounds[i+1]\n",
    "                    left_real = left * std + mean if np.isfinite(left) else -np.inf\n",
    "                    right_real = right * std + mean if np.isfinite(right) else np.inf\n",
    "                    if i == 0:\n",
    "                        cond = f\"≤ {right_real:.2f}\"\n",
    "                    elif i == len(node.children)-1:\n",
    "                        cond = f\"> {left_real:.2f}\"\n",
    "                    else:\n",
    "                        cond = f\"∈ ({left_real:.2f}, {right_real:.2f}]\"\n",
    "                    add_node(child, curr, cond)\n",
    "\n",
    "            # --- Caso categórica ordinal ---\n",
    "            elif fname in global_mapping:\n",
    "                vals_cat = global_mapping[fname]\n",
    "                for i, child in enumerate(node.children):\n",
    "                    val = vals_cat[node.intervals[i] if i < len(node.intervals) else -1]\n",
    "                    edge = f'= \"{val}\"' if i == 0 else f'≠ \"{val}\"'\n",
    "                    add_node(child, curr, edge)\n",
    "            else:\n",
    "                # Desconocido\n",
    "                for child in node.children:\n",
    "                    add_node(child, curr, \"?\")\n",
    "\n",
    "    # --- Guardado ---\n",
    "    folder_path = f\"Ronda_{round_number}/{folder}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    filename = f\"{folder_path}/supertree_ronda_{round_number}\"\n",
    "    add_node(root_node)\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "    # print(f\"Árbol SuperTree guardado en: {filename}.png\")\n",
    "    return f\"{filename}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 🔧 INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 12:24:34,365\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-07-16 12:24:38,568 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 3] ✅ Red neuronal entrenada\n",
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 2] ✅ Red neuronal entrenada\n",
      "[CLIENTE 4] ✅ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] ✅ Red neuronal entrenada\n",
      "[CLIENTE 3] ✅ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 2] ✅ Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "\n",
      "[CLIENTE 4] 🧪 Instancia a explicar (decodificada):\n",
      "age                        41.0\n",
      "hours-per-week             40.0\n",
      "workclass               Private\n",
      "education                   9th\n",
      "marital-status        Separated\n",
      "occupation        Other-service\n",
      "relationship      Not-in-family\n",
      "race                      White\n",
      "sex                        Male\n",
      "native-country    United-States\n",
      "dtype: object\n",
      "🧪 Clase real:  <=50K\n",
      "\n",
      "\n",
      "pred_class: ' >50K'\n",
      "Ninguna regla cubre la instancia. No hay explicación factual disponible para esta predicción.\n",
      "\n",
      "\n",
      "cf_rules_por_clase: {' <=50K': ['age > 48.50', 'hours-per-week ≤ 24.50']}\n",
      "\n",
      "\n",
      "Silhouette: 0.922\n",
      "Fidelity: 0.569\n",
      "\n",
      "[CLIENTE 2] 🧪 Instancia a explicar (decodificada):\n",
      "age                             53.0\n",
      "hours-per-week                  40.0\n",
      "workclass                  Local-gov\n",
      "education               Some-college\n",
      "marital-status    Married-civ-spouse\n",
      "occupation           Protective-serv\n",
      "relationship                 Husband\n",
      "race                           White\n",
      "sex                             Male\n",
      "native-country         United-States\n",
      "dtype: object\n",
      "🧪 Clase real:  >50K\n",
      "\n",
      "\n",
      "pred_class: ' >50K'\n",
      "Regla factual encontrada: ['age > 44.67', 'hours-per-week > 32.84', 'education ≠ \"10th\"', 'marital-status = \"Married-civ-spouse\"', 'relationship ≠ \"Unmarried\"', 'occupation ≠ \"Craft-repair\"']\n",
      "\n",
      "\n",
      "cf_rules_por_clase: {' <=50K': ['age > 44.67', 'hours-per-week ≤ 32.84']}\n",
      "\n",
      "\n",
      "Coverage: 0.000\n",
      "Precision: 0.000\n",
      "Silhouette: 0.693\n",
      "Fidelity: 0.656\n",
      "Coverage: 0.441\n",
      "Precision: 1.000\n",
      "\n",
      "[CLIENTE 1] 🧪 Instancia a explicar (decodificada):\n",
      "age                             38.0\n",
      "hours-per-week                  55.0\n",
      "workclass                    Private\n",
      "education                  Bachelors\n",
      "marital-status    Married-civ-spouse\n",
      "occupation           Exec-managerial\n",
      "relationship                 Husband\n",
      "race                           White\n",
      "sex                             Male\n",
      "native-country         United-States\n",
      "dtype: object\n",
      "🧪 Clase real:  >50K\n",
      "\n",
      "\n",
      "pred_class: ' >50K'\n",
      "Regla factual encontrada: ['age ≤ 48.35', 'hours-per-week ∈ (42.50, 56.84]', 'education ≠ \"10th\"', 'age ≤ 42.83', 'marital-status = \"Married-civ-spouse\"']\n",
      "\n",
      "\n",
      "cf_rules_por_clase: {' <=50K': ['age ≤ 48.35', 'hours-per-week > 56.84']}\n",
      "\n",
      "\n",
      "Silhouette: 0.936\n",
      "Fidelity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.880\n",
      "Precision: 1.000\n",
      "\n",
      "[CLIENTE 3] 🧪 Instancia a explicar (decodificada):\n",
      "age                             65.0\n",
      "hours-per-week                  15.0\n",
      "workclass                    Private\n",
      "education                  Bachelors\n",
      "marital-status    Married-civ-spouse\n",
      "occupation                     Sales\n",
      "relationship                 Husband\n",
      "race                           White\n",
      "sex                             Male\n",
      "native-country         United-States\n",
      "dtype: object\n",
      "🧪 Clase real:  <=50K\n",
      "\n",
      "\n",
      "pred_class: ' >50K'\n",
      "Ninguna regla cubre la instancia. No hay explicación factual disponible para esta predicción.\n",
      "\n",
      "\n",
      "cf_rules_por_clase: {' <=50K': ['age > 49.94', 'hours-per-week ≤ 30.15']}\n",
      "\n",
      "\n",
      "Silhouette: 0.564\n",
      "Fidelity: 0.971\n",
      "Coverage: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 93.80s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)  # O ERROR para ocultar aún más\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesión previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
