{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:53:24,119\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-06-10 13:53:27,431 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-10 13:53:27,598 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/Iris/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-06-10 13:53:27,722 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris HTTP/11\" 200 610\n",
      "2025-06-10 13:53:27,848 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/Iris/resolve/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/Iris.py HTTP/11\" 404 0\n",
      "2025-06-10 13:53:27,851 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-06-10 13:53:28,187 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/Iris/pablopalacios23/Iris.py HTTP/11\" 404 0\n",
      "2025-06-10 13:53:28,307 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/Iris/resolve/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/README.md HTTP/11\" 404 0\n",
      "2025-06-10 13:53:28,431 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris/revision/6bbdbfec420ddde25fd56eb2d01f4bb904d94740 HTTP/11\" 200 610\n",
      "2025-06-10 13:53:28,554 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/Iris/resolve/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-06-10 13:53:28,558 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-06-10 13:53:28,755 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/Iris HTTP/11\" 200 None\n",
      "2025-06-10 13:53:28,879 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris/revision/6bbdbfec420ddde25fd56eb2d01f4bb904d94740 HTTP/11\" 200 610\n",
      "2025-06-10 13:53:29,022 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris/tree/6bbdbfec420ddde25fd56eb2d01f4bb904d94740?recursive=False&expand=False HTTP/11\" 200 209\n",
      "2025-06-10 13:53:29,145 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris/tree/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-06-10 13:53:29,255 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-10 13:53:29,426 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/Iris/revision/6bbdbfec420ddde25fd56eb2d01f4bb904d94740 HTTP/11\" 200 610\n",
      "2025-06-10 13:53:29,557 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/Iris/resolve/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-06-10 13:53:29,561 filelock     DEBUG    Attempting to acquire lock 2324408877616 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___iris_default_0.0.0_6bbdbfec420ddde25fd56eb2d01f4bb904d94740.lock\n",
      "2025-06-10 13:53:29,562 filelock     DEBUG    Lock 2324408877616 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___iris_default_0.0.0_6bbdbfec420ddde25fd56eb2d01f4bb904d94740.lock\n",
      "2025-06-10 13:53:29,563 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___iris/default/0.0.0/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/dataset_info.json\n",
      "2025-06-10 13:53:29,565 filelock     DEBUG    Attempting to release lock 2324408877616 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___iris_default_0.0.0_6bbdbfec420ddde25fd56eb2d01f4bb904d94740.lock\n",
      "2025-06-10 13:53:29,567 filelock     DEBUG    Lock 2324408877616 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___iris_default_0.0.0_6bbdbfec420ddde25fd56eb2d01f4bb904d94740.lock\n",
      "2025-06-10 13:53:29,596 filelock     DEBUG    Attempting to acquire lock 2324556430608 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___iris\\default\\0.0.0\\6bbdbfec420ddde25fd56eb2d01f4bb904d94740_builder.lock\n",
      "2025-06-10 13:53:29,597 filelock     DEBUG    Lock 2324556430608 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___iris\\default\\0.0.0\\6bbdbfec420ddde25fd56eb2d01f4bb904d94740_builder.lock\n",
      "2025-06-10 13:53:29,598 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___iris/default/0.0.0/6bbdbfec420ddde25fd56eb2d01f4bb904d94740/dataset_info.json\n",
      "2025-06-10 13:53:29,600 filelock     DEBUG    Attempting to release lock 2324556430608 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___iris\\default\\0.0.0\\6bbdbfec420ddde25fd56eb2d01f4bb904d94740_builder.lock\n",
      "2025-06-10 13:53:29,600 filelock     DEBUG    Lock 2324556430608 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___iris\\default\\0.0.0\\6bbdbfec420ddde25fd56eb2d01f4bb904d94740_builder.lock\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0097124 ,  0.37652839,  0.52950125,  0.81472018],\n",
       "        [-0.23633508, -1.21325815, -0.20940727, -0.24826237],\n",
       "        [ 0.25575988, -1.21325815,  0.98421419,  0.2832289 ],\n",
       "        [-1.58959622,  0.11156397, -1.34618962, -1.31124492],\n",
       "        [ 0.50180736, -0.94829372,  0.58634037,  0.81472018],\n",
       "        [ 1.2399498 ,  0.37652839,  1.0410533 ,  1.47908427],\n",
       "        [-0.48238256, -1.74318699, -0.09572904, -0.24826237],\n",
       "        [ 2.10111598, -0.15340046,  1.55260536,  1.21333863],\n",
       "        [ 1.8550685 , -0.6833293 ,  1.26840977,  0.947593  ],\n",
       "        [-0.48238256,  3.02617262, -1.40302874, -1.31124492],\n",
       "        [ 0.99390232,  0.64149281,  1.0410533 ,  1.21333863],\n",
       "        [-1.09750126,  1.43638608, -1.40302874, -1.31124492],\n",
       "        [-1.34354874,  0.90645724, -1.11883315, -1.31124492],\n",
       "        [ 0.6248311 , -0.94829372,  0.81369684,  0.947593  ],\n",
       "        [ 1.60902102, -0.15340046,  1.09789242,  0.54897454],\n",
       "        [-1.09750126, -0.15340046, -1.2893505 , -1.31124492],\n",
       "        [-1.46657248,  0.37652839, -1.45986785, -1.31124492],\n",
       "        [-0.6054063 ,  0.90645724, -1.23251138, -1.31124492],\n",
       "        [ 1.60902102,  0.37652839,  1.21157066,  0.81472018],\n",
       "        [ 0.13273614,  0.90645724,  0.3589839 ,  0.54897454],\n",
       "        [-1.220525  ,  1.43638608, -1.40302874, -1.44411774],\n",
       "        [ 1.11692606,  0.37652839,  1.15473154,  1.47908427],\n",
       "        [ 0.37878362, -0.6833293 ,  0.52950125,  0.81472018],\n",
       "        [ 0.25575988, -0.15340046,  0.41582302,  0.2832289 ],\n",
       "        [-0.97447752,  1.17142166, -1.40302874, -1.31124492],\n",
       "        [ 0.50180736, -2.00815142,  0.30214478,  0.15035608],\n",
       "        [ 0.13273614, -0.41836488,  0.3589839 ,  0.41610172],\n",
       "        [-0.85145378,  1.17142166, -1.34618962, -1.31124492],\n",
       "        [-0.11331134, -0.94829372,  0.01794919,  0.01748327],\n",
       "        [-0.48238256, -1.21325815,  0.30214478,  0.01748327]]),\n",
       " 0     1\n",
       " 1     1\n",
       " 2     2\n",
       " 3     0\n",
       " 4     2\n",
       " 5     2\n",
       " 6     1\n",
       " 7     2\n",
       " 8     2\n",
       " 9     0\n",
       " 10    2\n",
       " 11    0\n",
       " 12    0\n",
       " 13    2\n",
       " 14    2\n",
       " 15    0\n",
       " 16    0\n",
       " 17    0\n",
       " 18    2\n",
       " 19    1\n",
       " 20    0\n",
       " 21    2\n",
       " 22    2\n",
       " 23    1\n",
       " 24    0\n",
       " 25    1\n",
       " 26    1\n",
       " 27    0\n",
       " 28    1\n",
       " 29    1\n",
       " Name: class, dtype: int32,\n",
       " array([[-0.35935882, -0.94829372,  0.18846655,  0.15035608],\n",
       "        [ 0.50180736,  0.64149281,  0.47266213,  0.54897454],\n",
       "        [-0.23633508, -0.41836488,  0.18846655,  0.15035608],\n",
       "        [ 0.50180736, -0.41836488,  0.98421419,  0.81472018],\n",
       "        [ 1.2399498 ,  0.11156397,  0.58634037,  0.41610172],\n",
       "        [-1.95866743, -0.15340046, -1.57354609, -1.44411774],\n",
       "        [-0.11331134, -0.6833293 ,  0.7000186 ,  1.61195709],\n",
       "        [-0.85145378,  0.90645724, -1.40302874, -1.31124492]]),\n",
       " 30    1\n",
       " 31    1\n",
       " 32    1\n",
       " 33    2\n",
       " 34    1\n",
       " 35    0\n",
       " 36    2\n",
       " 37    0\n",
       " Name: class, dtype: int32,\n",
       " <lore_sa.dataset.tabular_dataset.TabularDataset at 0x21d3a134e30>,\n",
       " ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " LabelEncoder(),\n",
       " StandardScaler(),\n",
       " ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " <lore_sa.encoder_decoder.tabular_enc.ColumnTransformerEnc at 0x21d39cfc230>,\n",
       " ColumnTransformer(transformers=[('num', StandardScaler(), [0, 1, 2, 3]),\n",
       "                                 ('cat',\n",
       "                                  OrdinalEncoder(handle_unknown='use_encoded_value',\n",
       "                                                 unknown_value=-1),\n",
       "                                  [])]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# üì¶ IMPORTACIONES\n",
    "# =======================\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import Context, NDArrays, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder.tabular_enc import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =======================\n",
    "# ‚öôÔ∏è VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 4\n",
    "MIN_AVAILABLE_CLIENTS = 4\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# =======================\n",
    "# üîß UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# üì• CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset = dataset[~dataset[\"workclass\"].isin([\" ?\"])]\n",
    "        dataset = dataset[~dataset[\"occupation\"].isin([\" ?\"])]\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    # A√ëADIR DISTINCT_VALUES si falta en categ√≥ricas\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    dataset[class_col] = label_encoder.fit_transform(dataset[class_col])\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"]\n",
    "\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_indices)\n",
    "    ])\n",
    "\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor)\n",
    "    feature_names = list(encoder.encoded_features.values())\n",
    "\n",
    "    split_idx = int(0.8 * len(X_encoded))\n",
    "    return (\n",
    "        X_encoded[:split_idx], y[:split_idx],\n",
    "        X_encoded[split_idx:], y[split_idx:],\n",
    "        tabular_dataset, feature_names, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/adult_small\"\n",
    "# CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DEMASIADO GRANDE EL DATASET :/\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    " \n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# üåº CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ‚úÖ Red neuronal entrenada\")\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "            self._train_nn()\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                # self.received_supertree = SuperTree.SuperNode.from_dict(supertree_dict)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ‚ùå Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        y_pred = self.tree_model.predict(self.X_test)\n",
    "        y_proba = self.tree_model.predict_proba(self.X_test)\n",
    "\n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        self._save_local_tree(root_node, round_number)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        if self.received_supertree is not None:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        if len(np.unique(self.y_test)) == 2:\n",
    "            # Clasificaci√≥n binaria: usar la probabilidad de clase positiva\n",
    "            auc = roc_auc_score(self.y_test, y_proba[:, 1])\n",
    "        else:\n",
    "            # Clasificaci√≥n multiclase\n",
    "            auc = roc_auc_score(self.y_test, y_proba, multi_class=\"ovr\")\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            # \"Accuracy\": accuracy_score(self.y_test, y_pred),\n",
    "            # \"Precision\": precision_score(self.y_test, y_pred, average=\"weighted\", zero_division=1),\n",
    "            # \"Recall\": recall_score(self.y_test, y_pred, average=\"weighted\"),\n",
    "            # \"F1_Score\": f1_score(self.y_test, y_pred, average=\"weighted\"),\n",
    "            # \"AUC\": auc,\n",
    "            \"tree_ensemble\": tree_json,\n",
    "            \"scaler_mean\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            \"scaler_std\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            \"encoded_feature_names\": json.dumps(self.feature_names)\n",
    "        }\n",
    "\n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "    \n",
    "        num_row = 5\n",
    "\n",
    "        # Reconstruir DataFrame original codificado\n",
    "        # feature_cols = self.feature_names\n",
    "        \n",
    "        local_df = pd.DataFrame(self.X_train, columns=FEATURES).astype(np.float32)\n",
    "        local_df[\"target\"] = self.label_encoder.inverse_transform(self.y_train_nn)\n",
    "\n",
    "        # print(local_df.head())\n",
    "\n",
    "        # print(local_df)\n",
    "\n",
    "        # TabularDataset + descriptor\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"target\")\n",
    "        descriptor = local_tabular_dataset.get_descriptor()\n",
    "\n",
    "        encoder = ColumnTransformerEnc(descriptor)\n",
    "        encoder.set_classes(self.unique_labels)\n",
    "        self.encoder = encoder  # opcional\n",
    "\n",
    "        # Explicabilidad local\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        instance_scaled = local_tabular_dataset.df.iloc[num_row][:-1]\n",
    "        \n",
    "        target = local_tabular_dataset.df.iloc[num_row][-1]\n",
    "        instance_array = instance_scaled.values.reshape(1, -1).astype(np.float32)\n",
    "        pred_idx = self.nn_model(torch.tensor(instance_array)).argmax(dim=1).item()\n",
    "        pred_label = self.label_encoder.inverse_transform([pred_idx])[0]\n",
    "        print(f\"[CLIENTE {self.client_id}] ü§ñ Predicci√≥n de la red neuronal: {pred_label}\")\n",
    "\n",
    "        # Explicaci√≥n LORE\n",
    "        explanation = lore.explain_instance(instance_scaled.astype(np.float32), merge=True)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        self._save_lore_tree(lore_tree.root, round_number)\n",
    "\n",
    "        # √Årbol fusionado\n",
    "        merged_tree = SuperTree()\n",
    "        node_LORE_Tree = SuperTree.convert_SuperNode_to_Node(lore_tree.root)\n",
    "        merged_root = merged_tree.mergeDecisionTrees(\n",
    "            roots=[node_LORE_Tree, self.received_supertree],\n",
    "            num_classes=len(self.unique_labels),\n",
    "            feature_names=self.dataset.df.columns[:-1].tolist()\n",
    "        )\n",
    "        merged_tree.root = merged_root\n",
    "        merged_tree.prune_redundant_leaves_full()\n",
    "        merged_tree.merge_equal_class_leaves()\n",
    "        self._save_merged_tree(merged_tree.root, round_number)\n",
    "\n",
    "        # Decodificaci√≥n para mostrar instancia legible\n",
    "        raw_instance = self.dataset.df.iloc[num_row][:-1].values.reshape(1, -1)\n",
    "        z_encoded = encoder.encode(raw_instance)[0]\n",
    "        decoded_instance = encoder.decode(np.array([z_encoded]))[0]\n",
    "        decoded_instance = pd.Series(decoded_instance, index=self.dataset.df.columns[:-1])\n",
    "\n",
    "        # Desescalar variables num√©ricas\n",
    "        if hasattr(self, 'numeric_features') and hasattr(self, 'scaler'):\n",
    "            for i, col in enumerate(self.numeric_features):\n",
    "                if col in decoded_instance.index:\n",
    "                    decoded_instance[col] = instance_scaled[col] * self.scaler.scale_[i] + self.scaler.mean_[i]\n",
    "\n",
    "        print(f\"\\n[CLIENTE {self.client_id}] üß™ Instancia a explicar:\")\n",
    "        print(decoded_instance)\n",
    "        print(f\"[CLIENTE {self.client_id}] üß™ Clase real: {target}\")\n",
    "\n",
    "        # Preprocesamiento para obtener z_encoded\n",
    "        # ‚ö†Ô∏è Utiliza instancia ya codificada num√©ricamente para evitar strings\n",
    "        raw_instance_df = pd.DataFrame(instance_scaled.values.reshape(1, -1), columns=self.dataset.df.columns[:-1])\n",
    "        raw_instance_preprocessed = self.preprocessor.transform(raw_instance_df)\n",
    "        z_encoded = encoder.encode(raw_instance_preprocessed).astype(np.float32)[0]\n",
    "\n",
    "        tree_str = self.tree_to_str(merged_tree.root, self.feature_names)\n",
    "\n",
    "\n",
    "        # Guardar reglas legibles\n",
    "        # Regla del √°rbol fusionado\n",
    "        # Extraer reglas\n",
    "        \n",
    "        final_rule = []\n",
    "        rules = self.extract_rules_from_str(tree_str, target_class=pred_idx)\n",
    "        if rules:\n",
    "            for cond in rules[0]:\n",
    "                descaled = False\n",
    "                for var in self.numeric_features:\n",
    "                    if cond.startswith(var):\n",
    "                        import re\n",
    "                        match1 = re.match(rf\"{re.escape(var)} (‚â§|<|>|‚â•) ([\\d\\.\\-e]+)\", cond)\n",
    "                        match2 = re.match(rf\"{re.escape(var)} > ([\\d\\.\\-e]+) ‚àß ‚â§ ([\\d\\.\\-e]+)\", cond)\n",
    "                        if match2:\n",
    "                            low, high = map(float, match2.groups())\n",
    "                            idx = self.numeric_features.index(var)\n",
    "                            low_real = low * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                            high_real = high * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                            final_rule.append(f\"{var} > {low_real:.2f} ‚àß ‚â§ {high_real:.2f}\")\n",
    "                            descaled = True\n",
    "                            break\n",
    "                        elif match1:\n",
    "                            op, val = match1.groups()\n",
    "                            idx = self.numeric_features.index(var)\n",
    "                            val_real = float(val) * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                            final_rule.append(f\"{var} {op} {val_real:.2f}\")\n",
    "                            descaled = True\n",
    "                            break\n",
    "                if not descaled:\n",
    "                    final_rule.append(cond)\n",
    "\n",
    "            print(f\"\\n[CLIENTE {self.client_id}] üìú Regla desde √°rbol:\")\n",
    "            \n",
    "            # print(final_rule)\n",
    "            final_rule = self.filtrar_condiciones_redundantes(final_rule)\n",
    "            # print(\"filtradas condiciones redundantes:\",final_rule)\n",
    "            \n",
    "            for line in final_rule:\n",
    "                print(f\"   - {line}\")\n",
    "            print(f\" ‚áí target = {pred_label}\")\n",
    "        else:\n",
    "            print(f\"[CLIENTE {self.client_id}] ‚ö†Ô∏è No se encontr√≥ regla para clase {pred_label}\")\n",
    "            \n",
    "\n",
    "        cf_rules = self.extract_counterfactual_rules(tree_str, pred_idx) \n",
    "        \n",
    "\n",
    "        if cf_rules:\n",
    "            print(f\"\\nüß¨ [CLIENTE {self.client_id}] Contrafactuales sugeridos:\")\n",
    "            for class_idx, rule in cf_rules.items():\n",
    "                final_cf = []\n",
    "                label = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "                print(f\" ‚áí Posible clase: {label}\")\n",
    "                seen = set()\n",
    "                for cond in rule:\n",
    "                    if cond in seen:\n",
    "                        continue\n",
    "                    seen.add(cond)\n",
    "\n",
    "                    descaled = False\n",
    "                    for var in self.numeric_features:\n",
    "                        if cond.startswith(var):\n",
    "                            import re\n",
    "                            match1 = re.match(rf\"{re.escape(var)} (‚â§|<|>|‚â•) ([\\d\\.\\-e]+)\", cond)\n",
    "                            match2 = re.match(rf\"{re.escape(var)} > ([\\d\\.\\-e]+) ‚àß ‚â§ ([\\d\\.\\-e]+)\", cond)\n",
    "                            if match2:\n",
    "                                low, high = map(float, match2.groups())\n",
    "                                idx = self.numeric_features.index(var)\n",
    "                                low_real = low * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                                high_real = high * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                                final_cf.append(f\"{var} > {low_real:.2f} ‚àß ‚â§ {high_real:.2f}\")\n",
    "                                descaled = True\n",
    "                                break\n",
    "                            elif match1:\n",
    "                                op, val = match1.groups()\n",
    "                                idx = self.numeric_features.index(var)\n",
    "                                val_real = float(val) * self.scaler.scale_[idx] + self.scaler.mean_[idx]\n",
    "                                final_cf.append(f\"{var} {op} {val_real:.2f}\")\n",
    "                                descaled = True\n",
    "                                break\n",
    "                    if not descaled:\n",
    "                        final_cf.append(cond)\n",
    "\n",
    "                # print(final_cf)\n",
    "                final_cf = self.filtrar_condiciones_redundantes(final_cf)\n",
    "                # print(\"filtradas condiciones redundantes:\", final_cf)\n",
    "                for line in final_cf:\n",
    "                    print(f\"   - {line}\")\n",
    "\n",
    "        \n",
    "\n",
    "        # ==========================\n",
    "        # üìè M√âTRICAS DE EXPLICACI√ìN\n",
    "        # ==========================\n",
    "        Z = explanation[\"neighborhood_Z\"] # instancias del vecindario sint√©tico generado alrededor del punto a explicar.\n",
    "        y_surrogate = explanation[\"neighborhood_Yb\"] # predicciones del modelo interpretable (arbol) sobre Z.\n",
    "        y_nn = nn_wrapper.predict(Z) \n",
    "\n",
    "        # Convertir Z en DataFrame legible\n",
    "        dfZ = pd.DataFrame(Z, columns=self.dataset.df.columns[:-1])\n",
    "\n",
    "\n",
    "        # Silhouette\n",
    "        # 1Ô∏è‚É£ Distancia media entre x y las instancias de su misma clase en el vecindario (Z+)\n",
    "        mask_same_class = (y_nn == pred_idx)\n",
    "        mask_diff_class = (y_nn != pred_idx)\n",
    "\n",
    "        Z_plus = dfZ[mask_same_class]\n",
    "        Z_minus = dfZ[mask_diff_class]\n",
    "\n",
    "        # Evitar divisi√≥n por cero\n",
    "        if not Z_plus.empty and not Z_minus.empty:\n",
    "            dist_same = pairwise_distances(\n",
    "                instance_scaled.values.reshape(1, -1).astype(float),\n",
    "                Z_plus.values.astype(float)\n",
    "            ).mean()\n",
    "            dist_diff = pairwise_distances(\n",
    "                instance_scaled.values.reshape(1, -1).astype(float),\n",
    "                Z_minus.values.astype(float)\n",
    "            ).mean()\n",
    "            \n",
    "            silhouette = (dist_diff - dist_same) / max(dist_diff, dist_same)\n",
    "        else:\n",
    "            silhouette = np.nan\n",
    "\n",
    "        # Fidelity\n",
    "        fidelity = accuracy_score(y_nn, y_surrogate)\n",
    "        \n",
    "\n",
    "        # Funci√≥n para evaluar si una fila cumple la regla\n",
    "        def instancia_cumple_regla(fila, condiciones):\n",
    "            for cond in condiciones:\n",
    "                cond = cond.replace(\"‚â§\", \"<=\").replace(\"‚â•\", \">=\").strip()\n",
    "                if \"‚àß\" in cond:\n",
    "                    variable = None\n",
    "                    partes = []\n",
    "                    for parte in cond.split(\"‚àß\"):\n",
    "                        parte = parte.strip()\n",
    "                        for col in fila.index:\n",
    "                            if col in parte:\n",
    "                                variable = col\n",
    "                                break\n",
    "                        if variable is None:\n",
    "                            raise ValueError(f\"No se encontr√≥ la variable en {parte}\")\n",
    "                        if variable not in parte:\n",
    "                            parte = f\"{variable} {parte}\"\n",
    "                        partes.append(parte)\n",
    "                    if not all(eval(ajustar_expresion(p, fila)) for p in partes):\n",
    "                        return False\n",
    "                elif any(op in cond for op in [\">\", \"<\", \"<=\", \">=\"]):\n",
    "                    if not eval(ajustar_expresion(cond, fila)):\n",
    "                        return False\n",
    "                else:\n",
    "                    if \" ‚â† \" in cond:\n",
    "                        var, val = cond.split(\" ‚â† \")\n",
    "                        if str(fila.get(var.strip(), \"\")) == val.strip():\n",
    "                            return False\n",
    "                    elif \"=\" in cond:\n",
    "                        var, val = cond.split(\"=\")\n",
    "                        if str(fila.get(var.strip(), \"\")) != val.strip():\n",
    "                            return False\n",
    "            return True\n",
    "\n",
    "        def ajustar_expresion(cond, fila):\n",
    "            for col in fila.index:\n",
    "                if col in cond:\n",
    "                    cond = cond.replace(col, f\"fila[{repr(col)}]\")\n",
    "            return cond\n",
    "    \n",
    "\n",
    "\n",
    "        dfZ_coverage = pd.DataFrame(Z, columns=self.dataset.df.columns[:-1])\n",
    "\n",
    "        if hasattr(self, 'scaler'):\n",
    "            for i, col in enumerate(self.numeric_features):\n",
    "                if col in dfZ_coverage.columns:\n",
    "                    dfZ_coverage[col] = dfZ_coverage[col] * self.scaler.scale_[i] + self.scaler.mean_[i]\n",
    "\n",
    "        # Agregamos la clase predicha del modelo interpretable\n",
    "        dfZ_coverage['class'] = y_surrogate\n",
    "\n",
    "\n",
    "    \n",
    "        # Coverage: mide cu√°ntas instancias del vecindario ùëç (generado alrededor de la instancia a explicar) cumplen la regla factual ùëù. Es decir, calcula la proporci√≥n de instancias en las que la regla es aplicable.\n",
    "        mask_class_predicha = (dfZ_coverage['class'] == pred_idx)\n",
    "        cumplen_regla = dfZ_coverage[mask_class_predicha].apply(lambda fila: instancia_cumple_regla(fila, final_rule), axis=1)\n",
    "\n",
    "        dfZ_coverage[mask_class_predicha].to_csv(f\"dfZ_coverage_cliente_{self.client_id}.csv\", index=False)\n",
    "\n",
    "        num_cumplen = cumplen_regla.sum()\n",
    "        \n",
    "        print(\"num_cumplen: \", num_cumplen)\n",
    "        coverage = num_cumplen / len(dfZ_coverage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ahora filtra y_nn\n",
    "        covered_target_match = (y_nn[mask_class_predicha][cumplen_regla.values] == pred_idx)\n",
    "\n",
    "        # Calcular precisi√≥n\n",
    "        if cumplen_regla.sum() > 0:\n",
    "            precision = covered_target_match.sum() / cumplen_regla.sum()\n",
    "        else:\n",
    "            precision = 0\n",
    "\n",
    "        # Complexity --> M√°s condiciones = menos interpretable (m√°s complejo) /////  Menos condiciones = m√°s interpretable (menos complejo)\n",
    "        complexity = len(final_rule)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Dissimilarity\n",
    "\n",
    "        instance_cf = instance_scaled.copy() # Esta es la instancia original (escalada)\n",
    "\n",
    "        # print(\"cf_rules:\")\n",
    "        # print(cf_rules) # Contrafactual rule --> Esto NO es una instancia concreta todav√≠a, sino instrucciones para modificar la instancia factual.\n",
    "\n",
    "        def aplicar_condicion(cond, x_cf):\n",
    "            import re\n",
    "            match = re.match(r\"(.+?)\\s*(<=|<|>|>=)\\s*([\\d\\.\\-e]+)\", cond)\n",
    "            if match:\n",
    "                var, op, val = match.groups()\n",
    "                val = float(val)\n",
    "                if op in [\">\", \">=\"]:\n",
    "                    x_cf[var] = val + 1e-3  # Ajuste m√≠nimo para cumplir\n",
    "                else:\n",
    "                    x_cf[var] = val  # Ajuste m√°ximo para cumplir\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #  Instancia contrafactual generada\n",
    "\n",
    "        # Iterar sobre las reglas contrafactuales\n",
    "        # cf_rules es un diccionario donde las claves son las clases y los valores son listas de condiciones\n",
    "        # que definen c√≥mo modificar la instancia original para que pertenezca a esa clase.\n",
    "\n",
    "        # Por ejemplo, si cf_rules = {0: [\"X1 > 5\", \"X2 ‚â§ 3\"], 1: [\"X1 ‚â§ 4\", \"X2 > 2\"]}, significa que\n",
    "        # para que la instancia original pertenezca a la clase 0, debe cumplir \"X1 > 5\" y \"X2 ‚â§ 3\",\n",
    "        # y para la clase 1, debe cumplir \"X1 ‚â§ 4\" y \"X2 > 2\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Ejemplo real\n",
    "        \n",
    "        # instancia factual = {sepal length (cm): -1.37\n",
    "        #                     sepal width (cm): 0.23\n",
    "        #                     petal length (cm): -1.12\n",
    "        #                     petal width (cm): -1.23}\n",
    "\n",
    "\n",
    "\n",
    "        # Aplicamos las condiciones de la regla contrafactual:\n",
    "        # 'petal length (cm) > -0.56 ‚àß ‚â§ 0.99': ajustamos petal length (cm) para que sea justo 0.99 (o -0.56 + 1e-3 si queremos ir seguros)\n",
    "        # 'sepal length (cm) ‚â§ 1.81': ajustamos sepal length (cm) a 1.81\n",
    "\n",
    "\n",
    "        # Esto da como resultado una nueva instancia (la contrafactual):\n",
    "\n",
    "        # instancia contrafactual =   {sepal length (cm): 1.81\n",
    "        #                             sepal width (cm): 0.23\n",
    "        #                             petal length (cm): 0.99\n",
    "        #                             petal width (cm): -1.23}\n",
    "\n",
    "        \n",
    "        dissimilarities = {}\n",
    "\n",
    "        for class_idx, cf_rule in cf_rules.items(): \n",
    "            instance_cf = instance_scaled.copy()\n",
    "            for cond in cf_rule:\n",
    "                cond = cond.replace(\"‚â§\", \"<=\").replace(\"‚â•\", \">=\").strip()\n",
    "                if \"‚àß\" in cond:\n",
    "                    partes = cond.split(\"‚àß\")\n",
    "                    for parte in partes:\n",
    "                        aplicar_condicion(parte.strip(), instance_cf)\n",
    "                else:\n",
    "                    aplicar_condicion(cond, instance_cf)\n",
    "\n",
    "            # Calcular dissimilarity\n",
    "            dissimilarity = pairwise_distances(\n",
    "                instance_scaled.values.reshape(1, -1).astype(float),\n",
    "                instance_cf.values.reshape(1, -1).astype(float),\n",
    "                metric='euclidean'\n",
    "            )[0][0]\n",
    "\n",
    "            # Guardar en el diccionario\n",
    "            label = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            dissimilarities[label] = dissimilarity\n",
    "            \n",
    "\n",
    "\n",
    "        # Mostrar m√©tricas al final\n",
    "        print(f\"\\nüìä [CLIENTE {self.client_id}] M√©tricas de explicaci√≥n:\")\n",
    "        print(f\" - Silhouette: {silhouette:.3f}\")\n",
    "        print(f\" - Fidelity: {fidelity:.2f}\")\n",
    "        print(f\" - Coverage: {coverage:.3f}\")\n",
    "        print(f\" - Precision: {precision:.3f}\")\n",
    "        print(f\" - Complexity: {complexity:.3f}\")\n",
    "        for label, dissim in dissimilarities.items():\n",
    "            print(f\" - Dissimilarity con clase {label}: {dissim:.3f}\")\n",
    "\n",
    "        \n",
    "    def filtrar_condiciones_redundantes(self, condiciones):\n",
    "        import re\n",
    "        from collections import defaultdict\n",
    "\n",
    "        condiciones_filtradas = []\n",
    "        agrupadas = defaultdict(list)\n",
    "\n",
    "        for cond in condiciones:\n",
    "            match_intervalo = re.match(r\"(.+?) > ([\\d\\.\\-e]+) ‚àß ‚â§ ([\\d\\.\\-e]+)\", cond)\n",
    "            match_simple = re.match(r\"(.+?) (‚â§|<|>|‚â•) ([\\d\\.\\-e]+)\", cond)\n",
    "\n",
    "            if match_intervalo:\n",
    "                var, low, high = match_intervalo.groups()\n",
    "                agrupadas[var].append((\"intervalo\", float(low), float(high), cond))\n",
    "            elif match_simple:\n",
    "                var, op, val = match_simple.groups()\n",
    "                agrupadas[var].append((op, float(val), cond))\n",
    "            else:\n",
    "                condiciones_filtradas.append(cond)  # categ√≥ricas o fuera de formato\n",
    "\n",
    "        for var, items in agrupadas.items():\n",
    "            low_vals = []\n",
    "            high_vals = []\n",
    "\n",
    "            for item in items:\n",
    "                if item[0] == \"intervalo\":\n",
    "                    low_vals.append(item[1])\n",
    "                    high_vals.append(item[2])\n",
    "                elif item[0] in {\">\", \"‚â•\"}:\n",
    "                    low_vals.append(item[1])\n",
    "                elif item[0] in {\"<\", \"‚â§\"}:\n",
    "                    high_vals.append(item[1])\n",
    "\n",
    "            if low_vals or high_vals:\n",
    "                low_final = min(low_vals) if low_vals else None\n",
    "                high_final = max(high_vals) if high_vals else None\n",
    "\n",
    "                if low_final is not None and high_final is not None and low_final < high_final:\n",
    "                    condiciones_filtradas.append(f\"{var} > {low_final:.2f} ‚àß ‚â§ {high_final:.2f}\")\n",
    "                elif low_final is not None:\n",
    "                    condiciones_filtradas.append(f\"{var} > {low_final:.2f}\")\n",
    "                elif high_final is not None:\n",
    "                    condiciones_filtradas.append(f\"{var} ‚â§ {high_final:.2f}\")\n",
    "            # No se mete ninguna condici√≥n adicional (las intermedias quedan absorbidas)\n",
    "\n",
    "        return condiciones_filtradas\n",
    "\n",
    "\n",
    "    def extract_rules_from_str(self, tree_str, target_class):\n",
    "        lines = tree_str.strip().split(\"\\n\")\n",
    "        path = []\n",
    "        rules = []\n",
    "\n",
    "        def recurse(idx, indent_level):\n",
    "            seen = set()  # ‚Üê Aqu√≠ guardamos las premisas √∫nicas\n",
    "            while idx < len(lines):\n",
    "                line = lines[idx]\n",
    "                current_indent = len(line) - len(line.lstrip())\n",
    "\n",
    "                if current_indent < indent_level:\n",
    "                    return idx\n",
    "                if \"‚Æï\" in line:\n",
    "                    if f\"class = {target_class}\" in line:\n",
    "                        cleaned = []\n",
    "                        for cond in path:\n",
    "                            if cond not in seen:\n",
    "                                cleaned.append(cond)\n",
    "                                seen.add(cond)\n",
    "                        rules.append(cleaned)\n",
    "                    return idx + 1\n",
    "                elif \"if\" in line:\n",
    "                    condition = line.strip()[3:]  # remove 'if '\n",
    "                    path.append(condition)\n",
    "                    idx = recurse(idx + 1, current_indent + 2)\n",
    "                    path.pop()\n",
    "                else:\n",
    "                    idx += 1\n",
    "            return idx\n",
    "        \n",
    "        \n",
    "        recurse(0, 0)\n",
    "        return rules\n",
    "    \n",
    "\n",
    "    \n",
    "    def extract_counterfactual_rules(self, tree_str, predicted_class):\n",
    "        lines = tree_str.strip().split(\"\\n\")\n",
    "        path = []\n",
    "        counterfactuals = {}\n",
    "        \n",
    "        def recurse(idx, indent_level):\n",
    "            while idx < len(lines):\n",
    "                line = lines[idx]\n",
    "                current_indent = len(line) - len(line.lstrip())\n",
    "\n",
    "                if current_indent < indent_level:\n",
    "                    return idx\n",
    "                if \"‚Æï\" in line:\n",
    "                    import re\n",
    "                    match = re.search(r\"class = (\\d+)\", line)\n",
    "                    if match:\n",
    "                        class_idx = int(match.group(1))\n",
    "                        if class_idx != predicted_class and class_idx not in counterfactuals:\n",
    "                            counterfactuals[class_idx] = list(path)\n",
    "                    return idx + 1\n",
    "                elif \"if\" in line:\n",
    "                    condition = line.strip()[3:]\n",
    "                    path.append(condition)\n",
    "                    idx = recurse(idx + 1, current_indent + 2)\n",
    "                    path.pop()\n",
    "                else:\n",
    "                    idx += 1\n",
    "            return idx\n",
    "\n",
    "        recurse(0, 0)\n",
    "        return counterfactuals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tree_to_str(self, node, feature_names, depth=0):\n",
    "        indent = \"  \" * depth\n",
    "        result = \"\"\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_idx = np.argmax(node.labels)\n",
    "            result += f\"{indent}‚Æï Leaf: class = {class_idx} | {node.labels}\\n\"\n",
    "        else:\n",
    "            fname = feature_names[node.feat] if node.feat < len(feature_names) else f\"X_{node.feat}\"\n",
    "\n",
    "            is_cat = \"=\" in fname\n",
    "            base_feat = fname.split(\"=\")[0] if is_cat else fname\n",
    "            cat_val = fname.split(\"=\")[1] if is_cat else None\n",
    "\n",
    "            for i, child in enumerate(node.children):\n",
    "                if is_cat:\n",
    "                    cond = f\"{base_feat} {'‚â†' if i == 0 else '='} {cat_val}\"\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        cond = f\"{base_feat} ‚â§ {node.intervals[i]:.3f}\"\n",
    "                    elif i < len(node.intervals):\n",
    "                        cond = f\"{base_feat} > {node.intervals[i-1]:.3f} ‚àß ‚â§ {node.intervals[i]:.3f}\"\n",
    "                    else:\n",
    "                        cond = f\"{base_feat} > {node.intervals[i-1]:.3f}\"\n",
    "\n",
    "                result += f\"{indent}if {cond}\\n\"\n",
    "                result += self.tree_to_str(child, feature_names, depth + 1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _save_local_tree(self, root_node, round_number):\n",
    "        self._save_generic_tree(\n",
    "            root_node,\n",
    "            round_number,\n",
    "            tree_type=\"Arbol_Local\"\n",
    "        )\n",
    "\n",
    "    def _save_lore_tree(self, root_node, round_number):\n",
    "        self._save_generic_tree(\n",
    "            root_node, \n",
    "            round_number, \n",
    "            tree_type=\"LoreTree\"\n",
    "        )\n",
    "\n",
    "    def _save_merged_tree(self, root_node, round_number):\n",
    "        self._save_generic_tree(\n",
    "            root_node, \n",
    "            round_number, \n",
    "            tree_type=\"MergedTree\"\n",
    "        )\n",
    "\n",
    "    def _save_generic_tree(self, root_node, round_number, tree_type):\n",
    "        from graphviz import Digraph\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            return feat.split('=')[0] if '=' in feat else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1\n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = np.argmax(node.labels)\n",
    "                class_label = self.unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = self.feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # √Årbol tipo SuperTree\n",
    "            if hasattr(node, \"children\") and node.children is not None and hasattr(node, \"intervals\"):\n",
    "                for i, child in enumerate(node.children):\n",
    "                    try:\n",
    "                        fname = self.feature_names[node.feat]\n",
    "                    except:\n",
    "                        fname = f\"X_{node.feat}\"\n",
    "\n",
    "                    if '=' in fname:\n",
    "                        attr, val = fname.split('=')\n",
    "                        edge = f\"= {val}\" if i == 1 else f\"‚â† {val}\"\n",
    "                    else:\n",
    "                        original_feat = base_name(fname)\n",
    "                        if hasattr(self, \"scaler\") and original_feat in self.numeric_features:\n",
    "                            idx = self.numeric_features.index(original_feat)\n",
    "                            mean = self.scaler.mean_[idx]\n",
    "                            std = self.scaler.scale_[idx]\n",
    "                            val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                            val = val * std + mean\n",
    "                        else:\n",
    "                            val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        edge = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "\n",
    "                    add_node(child, curr, edge)\n",
    "\n",
    "            # √Årbol binario cl√°sico\n",
    "            elif hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = self.feature_names[node.feat]\n",
    "                except:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                if '=' in fname:\n",
    "                    attr, val = fname.split('=')\n",
    "                    left_label = f\"‚â† {val}\"\n",
    "                    right_label = f\"= {val}\"\n",
    "                else:\n",
    "                    original_feat = base_name(fname)\n",
    "                    if hasattr(self, \"scaler\") and original_feat in self.numeric_features:\n",
    "                        idx = self.numeric_features.index(original_feat)\n",
    "                        mean = self.scaler.mean_[idx]\n",
    "                        std = self.scaler.scale_[idx]\n",
    "                        thresh = node.thresh * std + mean if node.thresh is not None else None\n",
    "                    else:\n",
    "                        thresh = node.thresh\n",
    "\n",
    "                    if thresh is not None:\n",
    "                        left_label = f\"<= {thresh:.2f}\"\n",
    "                        right_label = f\"> {thresh:.2f}\"\n",
    "                    else:\n",
    "                        left_label = \"‚â§ ?\"\n",
    "                        right_label = \"> ?\"\n",
    "\n",
    "                if node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        add_node(root_node)\n",
    "        folder = f\"Ronda_{round_number}/{tree_type}_Cliente_{self.client_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        filepath = f\"{folder}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        dot.render(filepath, format=\"png\", cleanup=True)\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    \n",
    "    # dataset_name = context.node_config.get(\"dataset_name\", \"pablopalacios23/Iris\")\n",
    "    # class_col = context.node_config.get(\"class_col\", \"target\")\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=num_partitions)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# üì¶ IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ‚öôÔ∏è CONFIGURACI√ìN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan din√°micamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "\n",
    "# ============================\n",
    "# üß† UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net est√° en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# üöÄ SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se carg√≥ antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] üå≤ Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            scaler_means = None\n",
    "            scaler_stds = None\n",
    "            feature_names = None\n",
    "\n",
    "            for client_idx, (_, evaluate_res) in enumerate(results):\n",
    "                metrics = evaluate_res.metrics\n",
    "\n",
    "                if metrics.get(\"scaler_mean\") and metrics.get(\"scaler_std\"):\n",
    "                    scaler_means = json.loads(metrics[\"scaler_mean\"])\n",
    "                    scaler_stds = json.loads(metrics[\"scaler_std\"])\n",
    "\n",
    "                if \"encoded_feature_names\" in metrics:\n",
    "                    feature_names = json.loads(metrics[\"encoded_feature_names\"])\n",
    "\n",
    "                trees_json = metrics.get(\"tree_ensemble\")\n",
    "                if trees_json:\n",
    "                    try:\n",
    "                        trees_list = json.loads(trees_json)\n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "                            if root:\n",
    "                                tree_dicts.append(root)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[CLIENTE {client_idx+1}] ‚ùå Error al parsear √°rbol: {e}\")\n",
    "\n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] ‚ö†Ô∏è No se recibieron √°rboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "\n",
    "            supertree = SuperTree()\n",
    "            supertree.mergeDecisionTrees(tree_dicts, num_classes=len(UNIQUE_LABELS), feature_names=feature_names)\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            supertree.merge_equal_class_leaves()\n",
    "\n",
    "            _save_supertree_plot(supertree.root, server_round, feature_names, UNIQUE_LABELS, scaler_means, scaler_stds)\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ‚ùå Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# üß© FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "def _save_supertree_plot(root_node, round_number, feature_names=None, class_names=None, scaler_means=None, scaler_stds=None):\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def base_name(feat):\n",
    "        return feat.split('=')[0] if '=' in feat else feat\n",
    "\n",
    "    def add_node(node, parent=None, label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_index = np.argmax(node.labels)\n",
    "            class_label = class_names[class_index] if class_names else f\"Clase {class_index}\"\n",
    "            label_text = f\"Clase: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            try:\n",
    "                fname = feature_names[node.feat]\n",
    "                label_text = base_name(fname)\n",
    "            except:\n",
    "                label_text = f\"X_{node.feat}\"\n",
    "\n",
    "        dot.node(curr, label_text)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            for i, child in enumerate(node.children):\n",
    "                try:\n",
    "                    feat_val = feature_names[node.feat]\n",
    "                except:\n",
    "                    feat_val = f\"X_{node.feat}\"\n",
    "\n",
    "                if '=' in feat_val:\n",
    "                    attr, val = feat_val.split('=')\n",
    "                    edge_label = f\"= {val}\" if i == 1 else f\"‚â† {val}\"\n",
    "                else:\n",
    "                    if scaler_means and scaler_stds:\n",
    "                        idx = feature_names.index(feat_val)\n",
    "                        val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val = val * scaler_stds[idx] + scaler_means[idx]\n",
    "                    else:\n",
    "                        val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                    edge_label = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "\n",
    "                add_node(child, curr, edge_label)\n",
    "\n",
    "    add_node(root_node)\n",
    "    folder = f\"Ronda_{round_number}/Supertree\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filename = f\"{folder}/supertree_ronda_{round_number}\"\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "\n",
    "# ============================\n",
    "# üîß INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:53:34,250\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-06-10 13:53:37,732 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 4] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 3] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 4)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 2] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 4] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 1] ‚úÖ Red neuronal entrenada\n",
      "[CLIENTE 3] ‚úÖ Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "[CLIENTE 2] ü§ñ Predicci√≥n de la red neuronal: virginica\n",
      "[CLIENTE 4] ü§ñ Predicci√≥n de la red neuronal: setosa\n",
      "[CLIENTE 3] ü§ñ Predicci√≥n de la red neuronal: virginica\n",
      "[CLIENTE 1] ü§ñ Predicci√≥n de la red neuronal: virginica\n",
      "\n",
      "[CLIENTE 1] üß™ Instancia a explicar:\n",
      "sepal length (cm)    6.9\n",
      "sepal width (cm)     3.2\n",
      "petal length (cm)    5.7\n",
      "petal width (cm)     2.3\n",
      "dtype: object\n",
      "[CLIENTE 1] üß™ Clase real: virginica\n",
      "\n",
      "[CLIENTE 1] üìú Regla desde √°rbol:\n",
      "   - sepal length (cm) ‚â§ 5.79\n",
      "   - sepal width (cm) ‚â§ 2.65\n",
      "   - petal length (cm) > 3.19 ‚àß ‚â§ 5.03\n",
      "   - petal width (cm) > 0.85\n",
      " ‚áí target = virginica\n",
      "\n",
      "üß¨ [CLIENTE 1] Contrafactuales sugeridos:\n",
      " ‚áí Posible clase: setosa\n",
      "   - sepal length (cm) ‚â§ 5.79\n",
      "   - sepal width (cm) ‚â§ 2.65\n",
      "   - petal length (cm) ‚â§ 3.19\n",
      " ‚áí Posible clase: versicolor\n",
      "   - sepal length (cm) > 4.90 ‚àß ‚â§ 5.79\n",
      "   - sepal width (cm) ‚â§ 2.65\n",
      "   - petal length (cm) > 2.14 ‚àß ‚â§ 3.19\n",
      "   - petal width (cm) > 0.85 ‚àß ‚â§ 1.53\n",
      "num_cumplen:  2\n",
      "\n",
      "üìä [CLIENTE 1] M√©tricas de explicaci√≥n:\n",
      " - Silhouette: 0.929\n",
      " - Fidelity: 1.00\n",
      " - Coverage: 0.002\n",
      " - Precision: 1.000\n",
      " - Complexity: 4.000\n",
      " - Dissimilarity con clase setosa: 2.837\n",
      " - Dissimilarity con clase versicolor: 3.051\n",
      "\n",
      "[CLIENTE 2] üß™ Instancia a explicar:\n",
      "sepal length (cm)    7.7\n",
      "sepal width (cm)     3.0\n",
      "petal length (cm)    6.1\n",
      "petal width (cm)     2.3\n",
      "dtype: object\n",
      "[CLIENTE 2] üß™ Clase real: virginica\n",
      "\n",
      "[CLIENTE 2] üìú Regla desde √°rbol:\n",
      "   - petal length (cm) > 2.35 ‚àß ‚â§ 3.33\n",
      "   - petal width (cm) > 1.18\n",
      "   - sepal length (cm) > 6.11\n",
      " ‚áí target = virginica\n",
      "\n",
      "üß¨ [CLIENTE 2] Contrafactuales sugeridos:\n",
      "\n",
      "[CLIENTE 4] üß™ Instancia a explicar:\n",
      "sepal length (cm)    5.4\n",
      "sepal width (cm)     3.4\n",
      "petal length (cm)    1.5\n",
      "petal width (cm)     0.4\n",
      "dtype: object\n",
      "[CLIENTE 4] üß™ Clase real: setosa\n",
      "\n",
      "[CLIENTE 4] üìú Regla desde √°rbol:\n",
      "   - petal length (cm) ‚â§ 2.70\n",
      " ‚áí target = setosa\n",
      "\n",
      "üß¨ [CLIENTE 4] Contrafactuales sugeridos:\n",
      " ‚áí Posible clase: versicolor\n",
      "   - petal length (cm) > 1.59 ‚àß ‚â§ 2.70\n",
      "   - petal width (cm) ‚â§ 0.93\n",
      "   - sepal length (cm) > 6.60\n",
      " ‚áí Posible clase: setosa\n",
      "   - petal length (cm) ‚â§ 3.33\n",
      " ‚áí Posible clase: virginica\n",
      "   - petal length (cm) > 1.59 ‚àß ‚â§ 2.70\n",
      "   - petal width (cm) > 0.93 ‚àß ‚â§ 1.98\n",
      "   - sepal length (cm) > 5.93 ‚àß ‚â§ 6.32\n",
      "   - sepal width (cm) > 3.12\n",
      " ‚áí Posible clase: versicolor\n",
      "   - petal length (cm) > 2.35 ‚àß ‚â§ 3.33\n",
      "   - petal width (cm) > 0.43 ‚àß ‚â§ 1.18\n",
      "   - sepal length (cm) > 4.79 ‚àß ‚â§ 6.48\n",
      "num_cumplen:  5\n",
      "\n",
      "üìä [CLIENTE 2] M√©tricas de explicaci√≥n:\n",
      " - Silhouette: 0.922\n",
      " - Fidelity: 1.00\n",
      " - Coverage: 0.005\n",
      " - Precision: 1.000\n",
      " - Complexity: 3.000\n",
      " - Dissimilarity con clase setosa: 2.274\n",
      " - Dissimilarity con clase versicolor: 4.652\n",
      "num_cumplen:  518\n",
      "\n",
      "üìä [CLIENTE 4] M√©tricas de explicaci√≥n:\n",
      " - Silhouette: 0.954\n",
      " - Fidelity: 1.00\n",
      " - Coverage: 0.471\n",
      " - Precision: 1.000\n",
      " - Complexity: 1.000\n",
      " - Dissimilarity con clase versicolor: 1.636\n",
      " - Dissimilarity con clase virginica: 2.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLIENTE 3] üß™ Instancia a explicar:\n",
      "sepal length (cm)    6.1\n",
      "sepal width (cm)     2.9\n",
      "petal length (cm)    4.7\n",
      "petal width (cm)     1.4\n",
      "dtype: object\n",
      "[CLIENTE 3] üß™ Clase real: versicolor\n",
      "\n",
      "[CLIENTE 3] üìú Regla desde √°rbol:\n",
      "   - sepal length (cm) ‚â§ 5.61\n",
      "   - petal length (cm) > 3.11\n",
      "   - petal width (cm) > 1.10 ‚àß ‚â§ 1.78\n",
      " ‚áí target = virginica\n",
      "\n",
      "üß¨ [CLIENTE 3] Contrafactuales sugeridos:\n",
      " ‚áí Posible clase: setosa\n",
      "   - sepal length (cm) ‚â§ 5.61\n",
      "   - petal length (cm) ‚â§ 3.11\n",
      " ‚áí Posible clase: versicolor\n",
      "   - sepal length (cm) > 5.40 ‚àß ‚â§ 5.61\n",
      "   - petal length (cm) > 2.09 ‚àß ‚â§ 3.11\n",
      "   - petal width (cm) ‚â§ 1.10\n",
      "num_cumplen:  0\n",
      "\n",
      "üìä [CLIENTE 3] M√©tricas de explicaci√≥n:\n",
      " - Silhouette: 0.943\n",
      " - Fidelity: 1.00\n",
      " - Coverage: 0.000\n",
      " - Precision: 0.000\n",
      " - Complexity: 3.000\n",
      " - Dissimilarity con clase setosa: 1.658\n",
      " - Dissimilarity con clase versicolor: 1.550\n",
      "\n",
      "[SERVIDOR] üå≤ Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 118.04s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesi√≥n previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
