{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68391f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 13:46:38,542\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.piping.pipe(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.rendering.render(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.unflattening.unflatten(['stagger', 'fanout', 'chain', 'encoding'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.backend.viewing.view(['quiet'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.quote(['is_html_string', 'is_valid_id', 'dot_keywords', 'endswith_odd_number_of_backslashes', 'escape_unescaped_quotes'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.a_list(['kwargs', 'attributes'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.quoting.attr_list(['kwargs', 'attributes'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.clear(['keep_attrs'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.__iter__(['subgraph'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.node(['_attributes'])\n",
      "2025-06-30 13:46:41,694 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.edge(['_attributes'])\n",
      "2025-06-30 13:46:41,710 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.attr(['_attributes'])\n",
      "2025-06-30 13:46:41,710 graphviz._tools DEBUG    deprecate positional args: graphviz.dot.Dot.subgraph(['name', 'comment', 'graph_attr', 'node_attr', 'edge_attr', 'body'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.piping.Pipe._pipe_legacy(['renderer', 'formatter', 'neato_no_op', 'quiet'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.saving.Save.save(['directory'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.render(['directory', 'view', 'cleanup', 'format', 'renderer', 'formatter', 'neato_no_op', 'quiet', 'quiet_view'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.rendering.Render.view(['directory', 'cleanup', 'quiet', 'quiet_view'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.unflattening.Unflatten.unflatten(['stagger', 'fanout', 'chain'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.graphs.BaseGraph.__init__(['comment', 'filename', 'directory', 'format', 'engine', 'encoding', 'graph_attr', 'node_attr', 'edge_attr', 'body', 'strict'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.from_file(['directory', 'format', 'engine', 'encoding', 'renderer', 'formatter'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.__init__(['filename', 'directory', 'format', 'engine', 'encoding'])\n",
      "2025-06-30 13:46:41,712 graphviz._tools DEBUG    deprecate positional args: graphviz.sources.Source.save(['directory'])\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# 📦 IMPORTACIONES\n",
    "# =======================\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "import operator\n",
    "\n",
    "# NumPy, Pandas, Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score, pairwise_distances\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flower\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Context, NDArrays, Metrics, Scalar,\n",
    "    ndarrays_to_parameters, parameters_to_ndarrays\n",
    ")\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LORE\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.rule import Expression, Rule\n",
    "\n",
    "# Otros\n",
    "from graphviz import Digraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41dd2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ⚙️ VARIABLES GLOBALES\n",
    "# =======================\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 2\n",
    "NUM_CLIENTS = 2\n",
    "MIN_AVAILABLE_CLIENTS = NUM_CLIENTS\n",
    "fds = None  # Cache del FederatedDataset\n",
    "CAT_ENCODINGS = {}\n",
    "USING_DATASET = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_dim = max(8, input_dim * 2)  # algo proporcional\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "\n",
    "# =======================\n",
    "# 🔧 UTILIDADES MODELO\n",
    "# =======================\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [\n",
    "        int(tree_model.get_params()[\"max_depth\"] or -1),\n",
    "        int(tree_model.get_params()[\"min_samples_split\"]),\n",
    "        int(tree_model.get_params()[\"min_samples_leaf\"]),\n",
    "    ]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "\n",
    "def set_model_params(tree_model, nn_model, params):\n",
    "    tree_params = params[\"tree\"]\n",
    "    nn_weights = params[\"nn\"]\n",
    "\n",
    "    # Solo si tree_model no es None y tiene set_params\n",
    "    if tree_model is not None and hasattr(tree_model, \"set_params\"):\n",
    "        max_depth = tree_params[0] if tree_params[0] > 0 else None\n",
    "        tree_model.set_params(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=tree_params[1],\n",
    "            min_samples_leaf=tree_params[2],\n",
    "        )\n",
    "\n",
    "    # Actualizar pesos de la red neuronal\n",
    "    state_dict = nn_model.state_dict()\n",
    "    for (key, _), val in zip(state_dict.items(), nn_weights):\n",
    "        state_dict[key] = torch.tensor(val)\n",
    "    nn_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 📥 CARGAR DATOS\n",
    "# =======================\n",
    "\n",
    "def load_data_general(flower_dataset_name: str, class_col: str, partition_id: int, num_partitions: int):\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=flower_dataset_name, partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    if \"adult_small\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset = dataset[~dataset[\"workclass\"].isin([\" ?\"])]\n",
    "        dataset = dataset[~dataset[\"occupation\"].isin([\" ?\"])]\n",
    "\n",
    "    elif \"churn\" in flower_dataset_name.lower():\n",
    "        drop_cols = ['customerID', 'TotalCharges']\n",
    "        dataset.drop(columns=[col for col in drop_cols if col in dataset.columns], inplace=True)\n",
    "        dataset['MonthlyCharges'] = pd.to_numeric(dataset['MonthlyCharges'], errors='coerce')\n",
    "        dataset['tenure'] = pd.to_numeric(dataset['tenure'], errors='coerce')\n",
    "        dataset.dropna(subset=['MonthlyCharges', 'tenure'], inplace=True)\n",
    "\n",
    "\n",
    "    for col in dataset.select_dtypes(include=[\"object\"]).columns:\n",
    "        if dataset[col].nunique() < 50:\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "\n",
    "    class_original = dataset[class_col].copy()\n",
    "\n",
    "    tabular_dataset = TabularDataset(dataset.copy(), class_name=class_col)\n",
    "    descriptor = tabular_dataset.descriptor\n",
    "\n",
    "    # AÑADIR DISTINCT_VALUES si falta en categóricas\n",
    "    for col, info in descriptor[\"categorical\"].items():\n",
    "        if \"distinct_values\" not in info:\n",
    "            info[\"distinct_values\"] = list(dataset[col].dropna().unique())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    dataset[class_col] = label_encoder.fit_transform(dataset[class_col])\n",
    "\n",
    "    dataset.rename(columns={class_col: \"class\"}, inplace=True)\n",
    "    y = dataset[\"class\"].reset_index(drop=True).to_numpy()\n",
    "\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS[:] = label_encoder.classes_.tolist()\n",
    "\n",
    "    numeric_features = list(descriptor[\"numeric\"].keys())\n",
    "    categorical_features = list(descriptor[\"categorical\"].keys())\n",
    "    FEATURES[:] = numeric_features + categorical_features\n",
    "\n",
    "    numeric_indices = list(range(len(numeric_features)))\n",
    "    categorical_indices = list(range(len(numeric_features), len(FEATURES)))\n",
    "\n",
    "    X_array = dataset[FEATURES].to_numpy()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), numeric_indices),\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_indices)\n",
    "    ])\n",
    "\n",
    "    X_encoded = preprocessor.fit_transform(X_array)\n",
    "\n",
    "    encoder = ColumnTransformerEnc(descriptor)\n",
    "    feature_names = list(encoder.encoded_features.values())\n",
    "\n",
    "    split_idx = int(0.8 * len(X_encoded))\n",
    "    return (\n",
    "        X_encoded[:split_idx], y[:split_idx],\n",
    "        X_encoded[split_idx:], y[split_idx:],\n",
    "        tabular_dataset, feature_names, label_encoder,\n",
    "        preprocessor.named_transformers_[\"num\"], numeric_features, encoder, preprocessor\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "\n",
    "DATASET_NAME = \"pablopalacios23/adult_small\"\n",
    "CLASS_COLUMN = \"class\"\n",
    "\n",
    "\n",
    "# DATASET_NAME = \"pablopalacios23/Iris\"\n",
    "# CLASS_COLUMN = \"target\"\n",
    "\n",
    "\n",
    "# DEMASIADO GRANDE EL DATASET :/\n",
    "# DATASET_NAME = \"pablopalacios23/churn\"\n",
    "# CLASS_COLUMN = \"Churn\" \n",
    " \n",
    "\n",
    "# =======================\n",
    "\n",
    "\n",
    "# load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 13:46:41,765 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-30 13:46:41,962 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/main/README.md HTTP/11\" 404 0\n",
      "2025-06-30 13:46:42,148 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small HTTP/11\" 200 612\n",
      "2025-06-30 13:46:42,280 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/adult_small.py HTTP/11\" 404 0\n",
      "2025-06-30 13:46:42,280 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-06-30 13:46:42,580 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/pablopalacios23/adult_small/pablopalacios23/adult_small.py HTTP/11\" 404 0\n",
      "2025-06-30 13:46:42,713 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/README.md HTTP/11\" 404 0\n",
      "2025-06-30 13:46:42,829 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-06-30 13:46:43,014 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-06-30 13:46:43,014 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-06-30 13:46:43,448 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=pablopalacios23/adult_small HTTP/11\" 200 None\n",
      "2025-06-30 13:46:43,564 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-06-30 13:46:43,697 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/tree/475f19aed5f80dea1d48deab705f11928fe27493?recursive=False&expand=False HTTP/11\" 200 204\n",
      "2025-06-30 13:46:43,831 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/tree/475f19aed5f80dea1d48deab705f11928fe27493/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2025-06-30 13:46:43,948 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-30 13:46:44,113 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/pablopalacios23/adult_small/revision/475f19aed5f80dea1d48deab705f11928fe27493 HTTP/11\" 200 612\n",
      "2025-06-30 13:46:44,231 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/pablopalacios23/adult_small/resolve/475f19aed5f80dea1d48deab705f11928fe27493/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-06-30 13:46:44,247 filelock     DEBUG    Attempting to acquire lock 2071041420432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-06-30 13:46:44,248 filelock     DEBUG    Lock 2071041420432 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-06-30 13:46:44,248 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-06-30 13:46:44,248 filelock     DEBUG    Attempting to release lock 2071041420432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-06-30 13:46:44,248 filelock     DEBUG    Lock 2071041420432 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_pablopalacios23___adult_small_default_0.0.0_475f19aed5f80dea1d48deab705f11928fe27493.lock\n",
      "2025-06-30 13:46:44,281 filelock     DEBUG    Attempting to acquire lock 2071045619104 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-06-30 13:46:44,282 filelock     DEBUG    Lock 2071045619104 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-06-30 13:46:44,282 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/pablopalacios23___adult_small/default/0.0.0/475f19aed5f80dea1d48deab705f11928fe27493/dataset_info.json\n",
      "2025-06-30 13:46:44,282 filelock     DEBUG    Attempting to release lock 2071045619104 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n",
      "2025-06-30 13:46:44,282 filelock     DEBUG    Lock 2071045619104 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\pablopalacios23___adult_small\\default\\0.0.0\\475f19aed5f80dea1d48deab705f11928fe27493_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 X_train (primeras filas):\n",
      "           0         1    2    3    4    5    6    7    8    9\n",
      "0  -1.654556  0.545673  2.0  6.0  2.0  6.0  1.0  1.0  1.0  0.0\n",
      "1   1.293562  0.022425  2.0  2.0  1.0  2.0  0.0  1.0  1.0  0.0\n",
      "2   0.451243 -0.500823  0.0  6.0  0.0  0.0  2.0  1.0  0.0  0.0\n",
      "3  -1.338686  1.592170  2.0  3.0  1.0  6.0  0.0  1.0  1.0  0.0\n",
      "4   1.714722  1.592170  2.0  6.0  1.0  0.0  0.0  1.0  1.0  0.0\n",
      "5  -0.812237  1.068922  2.0  2.0  1.0  2.0  0.0  1.0  1.0  0.0\n",
      "6   0.661823 -0.500823  0.0  2.0  1.0  4.0  0.0  1.0  1.0  0.0\n",
      "7   0.451243  0.545673  3.0  5.0  0.0  6.0  1.0  1.0  1.0  0.0\n",
      "8  -0.706947  0.022425  2.0  1.0  3.0  0.0  2.0  1.0  0.0  0.0\n",
      "9   0.030083 -2.384518  2.0  4.0  1.0  4.0  3.0  1.0  0.0  0.0\n",
      "10 -0.391077 -0.500823  2.0  6.0  2.0  2.0  1.0  1.0  0.0  0.0\n",
      "\n",
      "🎯 y_train (primeros valores):\n",
      "[1 1 0 1 1 1 1 1 0 1 1]\n",
      "\n",
      "📦 X_test (primeras filas):\n",
      "          0         1    2    3    4    5    6    7    8    9\n",
      "0 -1.233397 -0.500823  2.0  0.0  2.0  3.0  2.0  0.0  0.0  0.0\n",
      "1  0.767112 -0.500823  2.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0\n",
      "2  0.767112 -0.500823  1.0  6.0  1.0  5.0  0.0  1.0  1.0  0.0\n",
      "\n",
      "🎯 y_test (primeros valores):\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, dataset, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor = load_data_general(\n",
    "    DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS\n",
    ")\n",
    "\n",
    "# Mostrar 5 primeros valores\n",
    "print(\"\\n📦 X_train (primeras filas):\")\n",
    "print(pd.DataFrame(X_train))\n",
    "\n",
    "print(\"\\n🎯 y_train (primeros valores):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\n📦 X_test (primeras filas):\")\n",
    "print(pd.DataFrame(X_test))\n",
    "\n",
    "print(\"\\n🎯 y_test (primeros valores):\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e6dc",
   "metadata": {},
   "source": [
    "# Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab462923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 🌼 CLIENTE FLOWER\n",
    "# ==========================\n",
    "import operator\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    log_loss, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.common import parameters_to_ndarrays\n",
    "\n",
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from lore_sa.rule import Expression, Rule\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class TorchNNWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            return outputs.argmax(dim=1).numpy()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            return probs.numpy()\n",
    "        \n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, tree_model, nn_model, X_train, y_train, X_test, y_test, dataset, client_id, feature_names, label_encoder, scaler, numeric_features, encoder, preprocessor):\n",
    "        self.tree_model = tree_model\n",
    "        self.nn_model = nn_model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.dataset = dataset\n",
    "        self.client_id = client_id\n",
    "        self.feature_names = feature_names\n",
    "        self.label_encoder = label_encoder\n",
    "        self.scaler = scaler\n",
    "        self.numeric_features = numeric_features\n",
    "        self.encoder = encoder\n",
    "        self.unique_labels = label_encoder.classes_.tolist()\n",
    "        self.y_train_nn = y_train.astype(np.int64)\n",
    "        self.y_test_nn = y_test.astype(np.int64)\n",
    "        self.received_supertree = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def _train_nn(self, epochs=10, lr=0.01):\n",
    "        self.nn_model.train()\n",
    "        optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        X_tensor = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train_nn, dtype=torch.long)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.nn_model(X_tensor)\n",
    "            loss = loss_fn(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"[CLIENTE {self.client_id}] ✅ Red neuronal entrenada\")\n",
    "\n",
    "    def decode_X(self, X, preprocessor, numeric_features, encoder):\n",
    "        num_scaler = preprocessor.named_transformers_[\"num\"]\n",
    "        cat_encoder = preprocessor.named_transformers_[\"cat\"]\n",
    "\n",
    "        # Separar partes numéricas y categóricas\n",
    "        X_num = X[:, :len(numeric_features)]\n",
    "        X_cat = X[:, len(numeric_features):]\n",
    "\n",
    "        # Desescalar numéricas\n",
    "        X_num_inv = num_scaler.inverse_transform(X_num)\n",
    "\n",
    "        # Decodificar categóricas a índices enteros\n",
    "        X_cat_inv = X_cat.astype(int)\n",
    "\n",
    "        # Crear DataFrame\n",
    "        df_num = pd.DataFrame(X_num_inv, columns=numeric_features)\n",
    "        df_cat = pd.DataFrame(X_cat_inv, columns=list(encoder.dataset_descriptor[\"categorical\"].keys()))\n",
    "\n",
    "        # Mapear índices a valores originales\n",
    "        for col in df_cat.columns:\n",
    "            valores = encoder.dataset_descriptor[\"categorical\"][col][\"distinct_values\"]\n",
    "            df_cat[col] = df_cat[col].map(dict(enumerate(valores)))\n",
    "\n",
    "        # print(encoder.dataset_descriptor[\"categorical\"][\"occupation\"][\"distinct_values\"])\n",
    "\n",
    "        return pd.concat([df_num, df_cat], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # print(f\"[CLIENTE {self.client_id}]\")\n",
    "            # print(\"X TEST instancia a explicar:\")\n",
    "            # print(pd.DataFrame(self.X_test).iloc[2])\n",
    "            # print(\"y TEST:\")\n",
    "            # print(pd.DataFrame(self.y_test).iloc[2])\n",
    "\n",
    "            # print(pd.DataFrame(self.X_train).shape)\n",
    "\n",
    "\n",
    "            # print(f\"[CLIENTE {self.client_id}]\")\n",
    "            # df_decoded = self.decode_X(self.X_test, self.preprocessor, self.numeric_features, self.encoder)\n",
    "            \n",
    "            # print(\"Instancia a explicar (EN EL FIT):\")\n",
    "            # print(df_decoded.iloc[2])\n",
    "\n",
    "            # print(\"target:\")\n",
    "            # print(df_decoded.iloc[2][-1])\n",
    "\n",
    "\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "            self._train_nn()\n",
    "        \n",
    "            # self.print_tree_human_readable(self.tree_model, FEATURES, self.numeric_features, self.scaler, self.encoder)\n",
    "            # print(\"\\n\")\n",
    "\n",
    "\n",
    "        nn_weights = get_model_parameters(self.tree_model, self.nn_model)[\"nn\"]\n",
    "        return nn_weights, len(self.X_train), {}\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "\n",
    "        set_model_params(self.tree_model, self.nn_model, {\"tree\": [-1, 2, 1], \"nn\": parameters})\n",
    "\n",
    "        if \"supertree\" in config:\n",
    "            try:\n",
    "                print(\"Recibiendo supertree....\")\n",
    "                supertree_dict = json.loads(config[\"supertree\"])\n",
    "                self.received_supertree = SuperTree.convert_SuperNode_to_Node(SuperTree.SuperNode.from_dict(supertree_dict))\n",
    "                self.global_mapping = json.loads(config[\"global_mapping\"])\n",
    "                self.feature_names = json.loads(config[\"feature_names\"])\n",
    "\n",
    "                # self.received_supertree = SuperTree.SuperNode.from_dict(supertree_dict)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CLIENTE {self.client_id}] ❌ Error al recibir SuperTree: {e}\")\n",
    "\n",
    "        try:\n",
    "            _ = self.tree_model.predict(self.X_test)\n",
    "        except NotFittedError:\n",
    "            self.tree_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        supertree = SuperTree()\n",
    "        root_node = supertree.rec_buildTree(self.tree_model, list(range(self.X_train.shape[1])), len(self.unique_labels))\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "        self._save_local_tree(root_node, round_number, FEATURES, self.numeric_features, self.scaler, UNIQUE_LABELS, self.encoder)\n",
    "        tree_json = json.dumps([root_node.to_dict()])\n",
    "\n",
    "        \n",
    "        if self.received_supertree is not None:\n",
    "            self._explain_local_and_global(config)\n",
    "\n",
    "        return 0.0, len(self.X_test), {\n",
    "            f\"tree_ensemble_{self.client_id}\": tree_json,\n",
    "            f\"scaler_mean_{self.client_id}\": json.dumps(self.scaler.mean_.tolist()),\n",
    "            f\"scaler_std_{self.client_id}\": json.dumps(self.scaler.scale_.tolist()),\n",
    "            f\"encoded_feature_names_{self.client_id}\": json.dumps(FEATURES),\n",
    "            f\"numeric_features_{self.client_id}\": json.dumps(self.numeric_features),\n",
    "            f\"unique_labels_{self.client_id}\": json.dumps(self.unique_labels),\n",
    "            f\"encoder_descriptor_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor),\n",
    "            f\"distinct_values_{self.client_id}\": json.dumps(self.encoder.dataset_descriptor[\"categorical\"])\n",
    "        }\n",
    "    \n",
    "    def _explain_local_and_global(self, config):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        import numpy as np\n",
    "    \n",
    "        num_row = 2\n",
    "\n",
    "        # Reconstruir DataFrame original codificado\n",
    "        # feature_cols = self.feature_names\n",
    "\n",
    "        # 1. Visualizar instancia escalada y decodificada usando el encoder/preprocessor ORIGINAL\n",
    "        # print(f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (escalada):\")\n",
    "        # print(pd.Series(self.X_test[num_row], index=self.feature_names))\n",
    "\n",
    "        decoded_instance = self.decode_X(\n",
    "            self.X_test[num_row].reshape(1, -1),\n",
    "            self.preprocessor,\n",
    "            self.numeric_features,\n",
    "            self.encoder\n",
    "        ).iloc[0]\n",
    "\n",
    "        print(f\"\\n[CLIENTE {self.client_id}] 🧪 Instancia a explicar (decodificada):\")\n",
    "        print(decoded_instance)\n",
    "        print(f\"[CLIENTE {self.client_id}] 🧪 Clase real: {self.label_encoder.inverse_transform([self.y_test_nn[num_row]])[0]}\")\n",
    "\n",
    "        # 2. Construir DataFrame para LORE (si es necesario, solo para TabularDataset)\n",
    "        local_df = pd.DataFrame(self.X_test, columns=FEATURES).astype(np.float32)\n",
    "        local_df[\"target\"] = self.label_encoder.inverse_transform(self.y_test_nn)\n",
    "        local_tabular_dataset = TabularDataset(local_df, class_name=\"target\")\n",
    "\n",
    "        # Explicabilidad local\n",
    "        nn_wrapper = TorchNNWrapper(self.nn_model)\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(nn_wrapper)\n",
    "        lore = TabularGeneticGeneratorLore(bbox, local_tabular_dataset)\n",
    "\n",
    "        instance_scaled = local_tabular_dataset.df.iloc[num_row][:-1]\n",
    "\n",
    "\n",
    "\n",
    "        # Explicación LORE\n",
    "        explanation = lore.explain_instance(instance_scaled.astype(np.float32), merge=True, num_classes=len(UNIQUE_LABELS), feature_names= self.feature_names, categorical_features=list(self.global_mapping.keys()), global_mapping=self.global_mapping)\n",
    "        lore_tree = explanation[\"merged_tree\"]\n",
    "        round_number = config.get(\"server_round\", 1)\n",
    "\n",
    "        encoder_for_print = {\n",
    "            \"categorical\": {\n",
    "                k: {\"distinct_values\": v} for k, v in self.global_mapping.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        self.print_tree_readable(\n",
    "            lore_tree.root,\n",
    "            self.feature_names,\n",
    "            self.unique_labels,\n",
    "            numeric_features=self.numeric_features,\n",
    "            scaler={\n",
    "                \"mean\": [self.scaler.mean_[self.numeric_features.index(f)] for f in self.numeric_features],\n",
    "                \"std\": [self.scaler.scale_[self.numeric_features.index(f)] for f in self.numeric_features]\n",
    "            },\n",
    "            encoder=encoder_for_print\n",
    "        )\n",
    "\n",
    "        \n",
    "        self._save_lore_tree(lore_tree.root, round_number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def print_tree_readable(self, node, feature_names, class_names, numeric_features, scaler, encoder, depth=0):\n",
    "        indent = \"|   \" * depth\n",
    "        if node.is_leaf:\n",
    "            class_idx = int(np.argmax(node.labels))\n",
    "            print(f\"{indent}|--- class: {class_names[class_idx]}\")\n",
    "            return\n",
    "\n",
    "        feat_name = feature_names[node.feat]\n",
    "        base_feat = feat_name.split(\"=\")[0] if \"=\" in feat_name else feat_name\n",
    "\n",
    "        if base_feat in encoder[\"categorical\"]:\n",
    "            val_idx = int(node.thresh)\n",
    "            try:\n",
    "                val = encoder[\"categorical\"][base_feat][\"distinct_values\"][val_idx]\n",
    "            except IndexError:\n",
    "                val = f\"[desconocido ({val_idx})]\"\n",
    "            print(f\"{indent}|--- {base_feat} <= \\\"{val}\\\"\")\n",
    "            self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            print(f\"{indent}|--- {base_feat} > \\\"{val}\\\"\")\n",
    "            self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "        elif base_feat in numeric_features:\n",
    "            idx = numeric_features.index(base_feat)\n",
    "            threshold = node.thresh * scaler[\"std\"][idx] + scaler[\"mean\"][idx]\n",
    "            print(f\"{indent}|--- {base_feat} <= {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "            print(f\"{indent}|--- {base_feat} > {threshold:.2f}\")\n",
    "            self.print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def _save_local_tree(self, root_node, round_number, feature_names, numeric_features, scaler, unique_labels, encoder, tree_type= \"LocalTree\"):\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            return feat.split('=')[0] if '=' in feat else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1\n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = np.argmax(node.labels)\n",
    "                class_label = unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    label = base_name(fname)\n",
    "                except:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Árbol tipo SuperTree\n",
    "            if hasattr(node, \"children\") and node.children is not None and hasattr(node, \"intervals\"):\n",
    "                for i, child in enumerate(node.children):\n",
    "                    try:\n",
    "                        fname = feature_names[node.feat]\n",
    "                    except:\n",
    "                        fname = f\"X_{node.feat}\"\n",
    "\n",
    "                    original_feat = base_name(fname)\n",
    "                    if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                        val_idx = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val_idx = int(val_idx)\n",
    "                        val = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"][val_idx] if val_idx < len(encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]) else f\"desconocido({val_idx})\"\n",
    "                        edge = f'≠ \"{val}\"' if i == 0 else f'= \"{val}\"'\n",
    "                    elif original_feat in numeric_features:\n",
    "                        idx = numeric_features.index(original_feat)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        val = node.intervals[i] if i == 0 else node.intervals[i - 1]\n",
    "                        val = val * std + mean\n",
    "                        edge = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "                    else:\n",
    "                        edge = \"?\"\n",
    "\n",
    "                    add_node(child, curr, edge)\n",
    "\n",
    "            elif hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                except:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                original_feat = base_name(fname)\n",
    "                if original_feat in encoder.dataset_descriptor[\"categorical\"]:\n",
    "                    val_idx = int(node.thresh)\n",
    "                    val = encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"][val_idx] if val_idx < len(encoder.dataset_descriptor[\"categorical\"][original_feat][\"distinct_values\"]) else f\"desconocido({val_idx})\"\n",
    "                    left_label = f'= \"{val}\"'\n",
    "                    right_label = f'≠ \"{val}\"'\n",
    "                elif original_feat in numeric_features:\n",
    "                    idx = numeric_features.index(original_feat)\n",
    "                    mean = scaler.mean_[idx]\n",
    "                    std = scaler.scale_[idx]\n",
    "                    thresh = node.thresh * std + mean\n",
    "                    left_label = f\"<= {thresh:.2f}\"\n",
    "                    right_label = f\"> {thresh:.2f}\"\n",
    "                else:\n",
    "                    left_label = \"≤ ?\"\n",
    "                    right_label = \"> ?\"\n",
    "\n",
    "                if node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        add_node(root_node)\n",
    "        folder = f\"Ronda_{round_number}/{tree_type}_Cliente_{self.client_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        filepath = f\"{folder}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        dot.render(filepath, format=\"png\", cleanup=True)\n",
    "\n",
    "    def _save_lore_tree(self, root_node, round_number):\n",
    "         self._save_generic_tree(\n",
    "            root_node,\n",
    "            round_number,\n",
    "            tree_type=\"LoreTree\",\n",
    "            feature_names=self.feature_names,\n",
    "            categorical_features=list(self.global_mapping.keys()),\n",
    "            global_mapping=self.global_mapping,\n",
    "            scaler=self.scaler,\n",
    "            numeric_features=self.numeric_features\n",
    "        )\n",
    "\n",
    "    def _save_merged_tree(self, root_node, round_number):\n",
    "        self._save_generic_tree(\n",
    "            root_node, \n",
    "            round_number, \n",
    "            tree_type=\"MergedTree\"\n",
    "        )\n",
    "\n",
    "    def _save_generic_tree(self, root_node, round_number, tree_type, feature_names=None,categorical_features=None,global_mapping=None,scaler=None,numeric_features=None):\n",
    "        from graphviz import Digraph\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        dot = Digraph()\n",
    "        node_id = [0]\n",
    "\n",
    "        def base_name(feat):\n",
    "            return feat.split('=')[0] if '=' in feat else feat\n",
    "\n",
    "        def add_node(node, parent=None, edge_label=\"\"):\n",
    "            curr = str(node_id[0])\n",
    "            node_id[0] += 1\n",
    "\n",
    "            # Etiqueta del nodo\n",
    "            if node.is_leaf:\n",
    "                class_index = np.argmax(node.labels)\n",
    "                class_label = self.unique_labels[class_index]\n",
    "                label = f\"class: {class_label}\\n{node.labels}\"\n",
    "            else:\n",
    "                try:\n",
    "                    fname = feature_names[node.feat] if feature_names else f\"X_{node.feat}\"\n",
    "                    label = base_name(fname)\n",
    "                except Exception:\n",
    "                    label = f\"X_{node.feat}\"\n",
    "\n",
    "            dot.node(curr, label)\n",
    "            if parent:\n",
    "                dot.edge(parent, curr, label=edge_label)\n",
    "\n",
    "            # Árbol tipo SuperTree (ramas como lista children+intervals)\n",
    "            if hasattr(node, \"children\") and node.children is not None and hasattr(node, \"intervals\"):\n",
    "                for i, child in enumerate(node.children):\n",
    "                    try:\n",
    "                        fname = feature_names[node.feat] if feature_names else f\"X_{node.feat}\"\n",
    "                    except Exception:\n",
    "                        fname = f\"X_{node.feat}\"\n",
    "\n",
    "                    original_feat = base_name(fname)\n",
    "                    edge = \"?\"\n",
    "\n",
    "                    # Categórica\n",
    "                    if categorical_features and original_feat in categorical_features and global_mapping:\n",
    "                        idx = node.intervals[i]\n",
    "                        if original_feat in global_mapping and idx < len(global_mapping[original_feat]):\n",
    "                            val_real = global_mapping[original_feat][idx]\n",
    "                            if len(node.children) == 2:\n",
    "                                edge = f\"= {val_real}\" if i == 0 else f\"≠ {val_real}\"\n",
    "                            else:\n",
    "                                edge = f\"= {val_real}\"\n",
    "                        else:\n",
    "                            edge = f\"= {idx}\"\n",
    "                    # Numérica\n",
    "                    elif scaler and numeric_features and original_feat in numeric_features:\n",
    "                        idx = numeric_features.index(original_feat)\n",
    "                        mean = scaler.mean_[idx]\n",
    "                        std = scaler.scale_[idx]\n",
    "                        val = node.intervals[i]\n",
    "                        val_real = val * std + mean\n",
    "                        if len(node.children) == 2:\n",
    "                            edge = f\"<= {val_real:.2f}\" if i == 0 else f\"> {val_real:.2f}\"\n",
    "                        else:\n",
    "                            edge = f\"= {val_real:.2f}\"\n",
    "                    # Genérico\n",
    "                    else:\n",
    "                        val = node.intervals[i]\n",
    "                        if len(node.children) == 2:\n",
    "                            edge = f\"<= {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "                        else:\n",
    "                            edge = f\"= {val:.2f}\"\n",
    "\n",
    "                    add_node(child, curr, edge)\n",
    "\n",
    "            # Árbol binario clásico (ramas como _left_child/_right_child)\n",
    "            elif hasattr(node, \"_left_child\") or hasattr(node, \"_right_child\"):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat] if feature_names else f\"X_{node.feat}\"\n",
    "                except Exception:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "\n",
    "                original_feat = base_name(fname)\n",
    "                # Categórica\n",
    "                if categorical_features and original_feat in categorical_features and global_mapping:\n",
    "                    idx = int(node.thresh) if node.thresh is not None else None\n",
    "                    if idx is not None and original_feat in global_mapping and idx < len(global_mapping[original_feat]):\n",
    "                        val_real = global_mapping[original_feat][idx]\n",
    "                        left_label = f\"= {val_real}\"\n",
    "                        right_label = f\"≠ {val_real}\"\n",
    "                    else:\n",
    "                        left_label = \"= ?\"\n",
    "                        right_label = \"≠ ?\"\n",
    "                # Numérica\n",
    "                elif scaler and numeric_features and original_feat in numeric_features:\n",
    "                    idx = numeric_features.index(original_feat)\n",
    "                    mean = scaler.mean_[idx]\n",
    "                    std = scaler.scale_[idx]\n",
    "                    thresh = node.thresh * std + mean if node.thresh is not None else None\n",
    "                    left_label = f\"<= {thresh:.2f}\" if thresh is not None else \"≤ ?\"\n",
    "                    right_label = f\"> {thresh:.2f}\" if thresh is not None else \"> ?\"\n",
    "                # Genérico\n",
    "                else:\n",
    "                    thresh = node.thresh\n",
    "                    left_label = f\"<= {thresh:.2f}\" if thresh is not None else \"≤ ?\"\n",
    "                    right_label = f\"> {thresh:.2f}\" if thresh is not None else \"> ?\"\n",
    "\n",
    "                if hasattr(node, \"_left_child\") and node._left_child:\n",
    "                    add_node(node._left_child, curr, left_label)\n",
    "                if hasattr(node, \"_right_child\") and node._right_child:\n",
    "                    add_node(node._right_child, curr, right_label)\n",
    "\n",
    "        add_node(root_node)\n",
    "        folder = f\"Ronda_{round_number}/{tree_type}_Cliente_{self.client_id}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        filepath = f\"{folder}/{tree_type.lower()}_cliente_{self.client_id}_ronda_{round_number}\"\n",
    "        dot.render(filepath, format=\"png\", cleanup=True)\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    \n",
    "    # dataset_name = context.node_config.get(\"dataset_name\", \"pablopalacios23/Iris\")\n",
    "    # class_col = context.node_config.get(\"class_col\", \"target\")\n",
    "\n",
    "    dataset_name = DATASET_NAME \n",
    "    class_col = CLASS_COLUMN \n",
    "\n",
    "    (X_train, y_train,X_test, y_test,dataset, feature_names,label_encoder, scaler,numeric_features, encoder, preprocessor) = load_data_general(flower_dataset_name=dataset_name,class_col=class_col,partition_id=partition_id,num_partitions=num_partitions)\n",
    "\n",
    "    tree_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, random_state=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    nn_model = Net(input_dim, output_dim)\n",
    "    return FlowerClient(tree_model=tree_model, \n",
    "                        nn_model=nn_model,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test,\n",
    "                        dataset=dataset,\n",
    "                        client_id=partition_id + 1,\n",
    "                        feature_names=feature_names,\n",
    "                        label_encoder=label_encoder,\n",
    "                        scaler=scaler,\n",
    "                        numeric_features=numeric_features,\n",
    "                        encoder=encoder,\n",
    "                        preprocessor=preprocessor).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c927a9",
   "metadata": {},
   "source": [
    "# Servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6042e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📦 IMPORTACIONES NECESARIAS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "from graphviz import Digraph\n",
    "from lore_sa.surrogate.decision_tree import SuperTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ⚙️ CONFIGURACIÓN GLOBAL\n",
    "# ============================\n",
    "# MIN_AVAILABLE_CLIENTS = 4\n",
    "# NUM_SERVER_ROUNDS = 2\n",
    "\n",
    "FEATURES = []  # se rellenan dinámicamente\n",
    "UNIQUE_LABELS = []\n",
    "LATEST_SUPERTREE_JSON = None\n",
    "GLOBAL_MAPPING_JSON = None\n",
    "FEATURE_NAMES_JSON = None\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 🧠 UTILIDADES MODELO\n",
    "# ============================\n",
    "def create_model(input_dim, output_dim):\n",
    "    from __main__ import Net  # necesario si Net está en misma libreta\n",
    "    return Net(input_dim, output_dim)\n",
    "\n",
    "\n",
    "def get_model_parameters(tree_model, nn_model):\n",
    "    tree_params = [-1, 2, 1]\n",
    "    nn_weights = [v.cpu().detach().numpy() for v in nn_model.state_dict().values()]\n",
    "    return {\n",
    "        \"tree\": tree_params,\n",
    "        \"nn\": nn_weights,\n",
    "    }\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    total = sum(n for n, _ in metrics)\n",
    "    avg: Dict[str, List[float]] = {}\n",
    "    for n, met in metrics:\n",
    "        for k, v in met.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                avg.setdefault(k, []).append(n * float(v))\n",
    "    return {k: sum(vs) / total for k, vs in avg.items()}\n",
    "\n",
    "# ============================\n",
    "# 🚀 SERVIDOR FLOWER\n",
    "# ============================\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    global FEATURES, UNIQUE_LABELS\n",
    "\n",
    "    # Justo antes de llamar a create_model\n",
    "    if not FEATURES or not UNIQUE_LABELS:\n",
    "        \n",
    "        load_data_general(DATASET_NAME, CLASS_COLUMN, partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "\n",
    "    FEATURES = FEATURES or [\"feat_0\", \"feat_1\"]  # fallback por si no se cargó antes\n",
    "    UNIQUE_LABELS = UNIQUE_LABELS or [\"Class_0\", \"Class_1\"]\n",
    "\n",
    "\n",
    "    model = create_model(len(FEATURES), len(UNIQUE_LABELS))\n",
    "    initial_params = ndarrays_to_parameters(get_model_parameters(None, model)[\"nn\"])\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=MIN_AVAILABLE_CLIENTS,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=initial_params,\n",
    "    )\n",
    "\n",
    "    strategy.configure_fit = _inject_round(strategy.configure_fit)\n",
    "    strategy.configure_evaluate = _inject_round(strategy.configure_evaluate)\n",
    "    original_aggregate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        aggregated_metrics = original_aggregate(server_round, results, failures)\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n[SERVIDOR] 🌲 Generando SuperTree - Ronda {server_round}\")\n",
    "            tree_dicts = []\n",
    "            all_distincts = defaultdict(set)\n",
    "            client_encoders = {}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"distinct_values_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        client_encoders[client_id] = json.loads(value)\n",
    "                        for feat, d in client_encoders[client_id].items():\n",
    "                            all_distincts[feat].update(d[\"distinct_values\"])\n",
    "\n",
    "            global_mapping = {feat: sorted(list(vals)) for feat, vals in all_distincts.items()}\n",
    "\n",
    "            for (_, evaluate_res) in results:\n",
    "                metrics = evaluate_res.metrics\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"tree_ensemble_\"):\n",
    "                        client_id = key.split(\"_\")[-1]\n",
    "                        trees_list = json.loads(value)\n",
    "                        local_encoder = client_encoders[client_id]\n",
    "                        feature_names = json.loads(metrics.get(f\"encoded_feature_names_{client_id}\"))\n",
    "                        numeric_features = json.loads(metrics.get(f\"numeric_features_{client_id}\"))\n",
    "                        unique_labels = json.loads(metrics.get(f\"unique_labels_{client_id}\"))\n",
    "                        scaler = {\n",
    "                            \"mean\": json.loads(metrics.get(f\"scaler_mean_{client_id}\")),\n",
    "                            \"std\": json.loads(metrics.get(f\"scaler_std_{client_id}\")),\n",
    "                        }\n",
    "\n",
    "                        for tdict in trees_list:\n",
    "                            root = SuperTree.Node.from_dict(tdict)\n",
    "\n",
    "                            def normalize_thresholds(node):\n",
    "                                if node is None or node.is_leaf:\n",
    "                                    return\n",
    "\n",
    "                                fname = feature_names[node.feat]\n",
    "\n",
    "                                if fname in numeric_features:\n",
    "                                    try:\n",
    "                                        idx = numeric_features.index(fname)\n",
    "                                        real_val = node.thresh * scaler[\"std\"][idx] + scaler[\"mean\"][idx]\n",
    "                                        node.real_thresh = real_val  # ⬅️ Lo guardamos\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"[WARNING] No se pudo desescalar {fname}: {e}\")\n",
    "\n",
    "                                elif fname in local_encoder and fname in global_mapping:\n",
    "                                    try:\n",
    "                                        local_vals = local_encoder[fname][\"distinct_values\"]\n",
    "                                        real_val = local_vals[int(node.thresh)]\n",
    "                                        node.thresh = global_mapping[fname].index(real_val)\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"[WARNING] No se pudo normalizar {fname}: {e}\")\n",
    "\n",
    "                                normalize_thresholds(getattr(node, \"_left_child\", None))\n",
    "                                normalize_thresholds(getattr(node, \"_right_child\", None))\n",
    "\n",
    "                            normalize_thresholds(root)\n",
    "\n",
    "                            encoder_for_print = {\n",
    "                                \"categorical\": {\n",
    "                                    k: {\"distinct_values\": v}\n",
    "                                    for k, v in global_mapping.items()\n",
    "                                    if k in FEATURES and k not in numeric_features\n",
    "                                }\n",
    "                            }\n",
    "\n",
    "                            # print(f\"\\n[CLIENTE {client_id}] 🌳 Árbol normalizado:\")\n",
    "                            # print_tree_readable(\n",
    "                            #     root,\n",
    "                            #     feature_names,\n",
    "                            #     unique_labels,\n",
    "                            #     numeric_features=numeric_features,\n",
    "                            #     scaler={\n",
    "                            #         \"mean\": [scaler[\"mean\"][i] for i, f in enumerate(feature_names) if f in numeric_features],\n",
    "                            #         \"std\": [scaler[\"std\"][i] for i, f in enumerate(feature_names) if f in numeric_features]\n",
    "                            #     },\n",
    "                            #     encoder=encoder_for_print\n",
    "                            # )\n",
    "                            # print(\"Local tree del cliente\", client_id)\n",
    "                            # print(root.to_dict())\n",
    "                            tree_dicts.append(root)\n",
    "                            \n",
    "            # print(tree_dicts)\n",
    "            \n",
    "            if not tree_dicts:\n",
    "                print(\"[SERVIDOR] ⚠️ No se recibieron árboles. Se omite SuperTree.\")\n",
    "                return aggregated_metrics\n",
    "            \n",
    "                        \n",
    "            supertree = SuperTree()\n",
    "            # print(\"feature_names: \", feature_names)\n",
    "            # print(\"global_mapping: \", global_mapping)\n",
    "            # print(\"global_mapping keys: \", list(global_mapping.keys()))\n",
    "            \n",
    "            supertree.mergeDecisionTrees(tree_dicts, num_classes=len(UNIQUE_LABELS), feature_names=feature_names, categorical_features=list(global_mapping.keys()), global_mapping=global_mapping)\n",
    "            supertree.prune_redundant_leaves_full()\n",
    "            supertree.merge_equal_class_leaves()\n",
    "\n",
    "            # print(\"\\n[SERVIDOR] 🌳 SuperTree legible:\")\n",
    "            \n",
    "            # print_supertree_readable_fusionado(\n",
    "            #     node=supertree.root,\n",
    "            #     global_mapping=global_mapping,\n",
    "            #     feature_names=feature_names,\n",
    "            #     class_names=UNIQUE_LABELS,\n",
    "            #     numeric_features=numeric_features,\n",
    "            #     scaler=scaler  # sin acceder a .mean_ ni .scale_\n",
    "            # )\n",
    "\n",
    "            _save_supertree_plot(\n",
    "                root_node=supertree.root,\n",
    "                round_number=server_round,\n",
    "                feature_names=feature_names,\n",
    "                class_names=UNIQUE_LABELS,\n",
    "                scaler_means=scaler[\"mean\"],\n",
    "                scaler_stds=scaler[\"std\"],\n",
    "                global_mapping=global_mapping,\n",
    "                numeric_features=numeric_features\n",
    "            )\n",
    "\n",
    "            LATEST_SUPERTREE_JSON = json.dumps(supertree.root.to_dict())\n",
    "            GLOBAL_MAPPING_JSON = json.dumps(global_mapping)\n",
    "            FEATURE_NAMES_JSON = json.dumps(feature_names)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SERVIDOR] ❌ Error en SuperTree: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return aggregated_metrics\n",
    "\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "    return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=NUM_SERVER_ROUNDS))\n",
    "\n",
    "# ============================\n",
    "# 🧩 FUNCIONES AUXILIARES\n",
    "# ============================\n",
    "def _inject_round(original_fn):\n",
    "    def wrapper(server_round, parameters, client_manager):\n",
    "        global LATEST_SUPERTREE_JSON, GLOBAL_MAPPING_JSON, FEATURE_NAMES_JSON\n",
    "        instructions = original_fn(server_round, parameters, client_manager)\n",
    "        for _, ins in instructions:\n",
    "            ins.config[\"server_round\"] = server_round\n",
    "            \n",
    "            if LATEST_SUPERTREE_JSON:\n",
    "                ins.config[\"supertree\"] = LATEST_SUPERTREE_JSON\n",
    "                ins.config[\"global_mapping\"] = GLOBAL_MAPPING_JSON\n",
    "                ins.config[\"feature_names\"] = FEATURE_NAMES_JSON\n",
    "                \n",
    "        return instructions\n",
    "    return wrapper\n",
    "\n",
    "def print_supertree_readable_fusionado(node, global_mapping, feature_names, class_names, numeric_features, scaler, depth=0):\n",
    "    if node is None:\n",
    "        print(f\"{'|   ' * depth}|--- [Nodo None]\")\n",
    "        return\n",
    "\n",
    "    indent = \"|   \" * depth\n",
    "\n",
    "    if node.is_leaf:\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}|--- class: {class_names[class_idx]}\")\n",
    "        return\n",
    "\n",
    "    feat_name = feature_names[node.feat]\n",
    "    base_feat = feat_name.split(\"=\")[0] if \"=\" in feat_name else feat_name\n",
    "\n",
    "    is_numeric = base_feat in numeric_features\n",
    "    is_categorical = base_feat in global_mapping and not is_numeric\n",
    "\n",
    "    for i, child in enumerate(node.children):\n",
    "        if hasattr(node, \"intervals\") and node.intervals is not None:\n",
    "            if i < len(node.intervals):\n",
    "                val_idx = node.intervals[i]\n",
    "            else:\n",
    "                val_idx = node.intervals[0]\n",
    "        else:\n",
    "            val_idx = i  # fallback\n",
    "\n",
    "        if is_numeric:\n",
    "            idx = numeric_features.index(base_feat)\n",
    "            if hasattr(node, \"real_thresh\"):\n",
    "                real_val = node.real_thresh\n",
    "            else:\n",
    "                real_val = val_idx * scaler[\"std\"][idx] + scaler[\"mean\"][idx]\n",
    "            op = \"≤\" if i == 0 else \">\"\n",
    "            print(f\"{indent}|--- {base_feat} {op} {real_val:.2f}\")\n",
    "        elif is_categorical:\n",
    "            values = global_mapping[base_feat]\n",
    "            val_idx = int(val_idx)\n",
    "            val = values[val_idx] if 0 <= val_idx < len(values) else f\"[desconocido {val_idx}]\"\n",
    "            op = \"=\" if i == 0 else \"≠\"\n",
    "            print(f\"{indent}|--- {base_feat} {op} \\\"{val}\\\"\")\n",
    "        else:\n",
    "            print(f\"{indent}|--- {base_feat} [tipo desconocido]\")\n",
    "\n",
    "        # Recursivo\n",
    "        print_supertree_readable_fusionado(\n",
    "            node=child,\n",
    "            global_mapping=global_mapping,\n",
    "            feature_names=feature_names,\n",
    "            class_names=class_names,\n",
    "            numeric_features=numeric_features,\n",
    "            scaler=scaler,\n",
    "            depth=depth + 1\n",
    "        )\n",
    "\n",
    "\n",
    "def print_tree_readable(node, feature_names, class_names, numeric_features, scaler, encoder, depth=0):\n",
    "    indent = \"|   \" * depth\n",
    "    if node.is_leaf:\n",
    "        class_idx = int(np.argmax(node.labels))\n",
    "        print(f\"{indent}|--- class: {class_names[class_idx]}\")\n",
    "        return\n",
    "\n",
    "    feat_name = feature_names[node.feat]\n",
    "    base_feat = feat_name.split(\"=\")[0] if \"=\" in feat_name else feat_name\n",
    "\n",
    "    if base_feat in encoder[\"categorical\"]:\n",
    "        val_idx = int(node.thresh)\n",
    "        try:\n",
    "            val = encoder[\"categorical\"][base_feat][\"distinct_values\"][val_idx]\n",
    "        except IndexError:\n",
    "            val = f\"[desconocido ({val_idx})]\"\n",
    "        print(f\"{indent}|--- {base_feat} <= \\\"{val}\\\"\")\n",
    "        print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "        print(f\"{indent}|--- {base_feat} > \\\"{val}\\\"\")\n",
    "        print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "    elif base_feat in numeric_features:\n",
    "        idx = numeric_features.index(base_feat)\n",
    "        threshold = node.thresh * scaler[\"std\"][idx] + scaler[\"mean\"][idx]\n",
    "        print(f\"{indent}|--- {base_feat} <= {threshold:.2f}\")\n",
    "        print_tree_readable(node._left_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "        print(f\"{indent}|--- {base_feat} > {threshold:.2f}\")\n",
    "        print_tree_readable(node._right_child, feature_names, class_names, numeric_features, scaler, encoder, depth + 1)\n",
    "\n",
    "\n",
    "def _save_supertree_plot(root_node, round_number, feature_names=None, class_names=None,\n",
    "                         scaler_means=None, scaler_stds=None,\n",
    "                         global_mapping=None, numeric_features=None):\n",
    "    dot = Digraph()\n",
    "    node_id = [0]\n",
    "\n",
    "    def base_name(feat):\n",
    "        return feat.split('=')[0] if '=' in feat else feat\n",
    "\n",
    "    def add_node(node, parent=None, label=\"\"):\n",
    "        curr = str(node_id[0])\n",
    "        node_id[0] += 1\n",
    "\n",
    "        if node.is_leaf:\n",
    "            class_index = int(np.argmax(node.labels))\n",
    "            class_label = class_names[class_index] if class_names else f\"Clase {class_index}\"\n",
    "            label_text = f\"Clase: {class_label}\\n{node.labels}\"\n",
    "        else:\n",
    "            try:\n",
    "                fname = feature_names[node.feat]\n",
    "                label_text = base_name(fname)\n",
    "            except Exception:\n",
    "                label_text = f\"X_{node.feat}\"\n",
    "\n",
    "        dot.node(curr, label_text)\n",
    "        if parent:\n",
    "            dot.edge(parent, curr, label=label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            for i, child in enumerate(node.children):\n",
    "                try:\n",
    "                    fname = feature_names[node.feat]\n",
    "                    base = base_name(fname)\n",
    "                except Exception:\n",
    "                    fname = f\"X_{node.feat}\"\n",
    "                    base = fname\n",
    "\n",
    "                if global_mapping and base in global_mapping:\n",
    "                    # Variable categórica\n",
    "                    values = global_mapping[base]\n",
    "                    try:\n",
    "                        val_idx = node.intervals[i] if node.intervals and i < len(node.intervals) else node.thresh\n",
    "                        val = values[int(val_idx)] if int(val_idx) < len(values) else f\"? ({val_idx})\"\n",
    "                    except Exception:\n",
    "                        val = \"?\"\n",
    "                    edge_label = f'= \"{val}\"' if i == 0 else f'≠ \"{val}\"'\n",
    "\n",
    "                elif numeric_features and base in numeric_features:\n",
    "                    # Variable numérica\n",
    "                    try:\n",
    "                        idx = feature_names.index(base)\n",
    "                        raw_val = node.intervals[i] if node.intervals and i < len(node.intervals) else node.thresh\n",
    "                        val = raw_val * scaler_stds[idx] + scaler_means[idx] if scaler_means and scaler_stds else raw_val\n",
    "                        edge_label = f\"≤ {val:.2f}\" if i == 0 else f\"> {val:.2f}\"\n",
    "                    except Exception:\n",
    "                        edge_label = \"?\"\n",
    "\n",
    "                else:\n",
    "                    edge_label = \"?\"\n",
    "\n",
    "                add_node(child, curr, edge_label)\n",
    "\n",
    "    add_node(root_node)\n",
    "    folder = f\"Ronda_{round_number}/Supertree\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filename = f\"{folder}/supertree_ronda_{round_number}\"\n",
    "    dot.render(filename, format=\"png\", cleanup=True)\n",
    "    return f\"{filename}.png\"\n",
    "\n",
    "# ============================\n",
    "# 🔧 INICIALIZAR SERVER APP\n",
    "# ============================\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d278d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 13:46:48,925\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-06-30 13:46:52,371 flwr         DEBUG    Asyncio event loop already running.\n",
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":job_id:01000000\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 2] ✅ Red neuronal entrenada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIENTE 1] ✅ Red neuronal entrenada\n",
      "[CLIENTE 2] ✅ Red neuronal entrenada\n",
      "Recibiendo supertree....\n",
      "Recibiendo supertree....\n",
      "\n",
      "[CLIENTE 1] 🧪 Instancia a explicar (decodificada):\n",
      "age                              53.0\n",
      "hours-per-week                   40.0\n",
      "workclass                 Federal-gov\n",
      "education                        10th\n",
      "marital-status     Married-civ-spouse\n",
      "occupation               Craft-repair\n",
      "relationship            Not-in-family\n",
      "race                            Black\n",
      "sex                            Female\n",
      "native-country          United-States\n",
      "Name: 0, dtype: object\n",
      "[CLIENTE 1] 🧪 Clase real:  >50K\n",
      "\n",
      "[CLIENTE 2] 🧪 Instancia a explicar (decodificada):\n",
      "age                              38.0\n",
      "hours-per-week                   50.0\n",
      "workclass                   Local-gov\n",
      "education                Some-college\n",
      "marital-status     Married-civ-spouse\n",
      "occupation               Craft-repair\n",
      "relationship                  Husband\n",
      "race               Asian-Pac-Islander\n",
      "sex                              Male\n",
      "native-country            Philippines\n",
      "Name: 0, dtype: object\n",
      "[CLIENTE 2] 🧪 Clase real:  <=50K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- occupation <= \" Craft-repair\"\n",
      "|   |--- class:  <=50K\n",
      "|--- occupation > \" Craft-repair\"\n",
      "|   |--- class:  >50K\n",
      "|--- sex <= \" Female\"\n",
      "|   |--- hours-per-week <= 44.99\n",
      "|   |   |--- native-country <= \" Philippines\"\n",
      "|   |   |   |--- class:  >50K\n",
      "|   |   |--- native-country > \" Philippines\"\n",
      "|   |   |   |--- race <= \" Black\"\n",
      "|   |   |   |   |--- workclass <= \" Local-gov\"\n",
      "|   |   |   |   |   |--- class:  <=50K\n",
      "|   |   |   |   |--- workclass > \" Local-gov\"\n",
      "|   |   |   |   |   |--- class:  >50K\n",
      "|   |   |   |--- race > \" Black\"\n",
      "|   |   |   |   |--- class:  <=50K\n",
      "|   |--- hours-per-week > 44.99\n",
      "|   |   |--- class:  >50K\n",
      "|--- sex > \" Female\"\n",
      "|   |--- class:  >50K\n",
      "\n",
      "[SERVIDOR] 🌲 Generando SuperTree - Ronda 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 14.75s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import logging\n",
    "import warnings\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ray\").setLevel(logging.WARNING)\n",
    "logging.getLogger('graphviz').setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"flwr\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesión previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
