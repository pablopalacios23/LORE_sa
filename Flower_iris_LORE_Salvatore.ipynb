{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:19:17,916\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "c:\\Users\\pablo\\anaconda3\\Lib\\site-packages\\flwr_datasets\\utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: hitorilabs/iris.\n",
      "  warnings.warn(\n",
      "2025-03-10 13:19:22,580 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-03-10 13:19:22,777 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/hitorilabs/iris/resolve/main/README.md HTTP/11\" 200 0\n",
      "2025-03-10 13:19:23,128 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/hitorilabs/iris/resolve/fa62476c42edcf9259f895f43da1a7bf9e2697ae/iris.py HTTP/11\" 404 0\n",
      "2025-03-10 13:19:23,128 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2025-03-10 13:19:23,428 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/hitorilabs/iris/hitorilabs/iris.py HTTP/11\" 404 0\n",
      "2025-03-10 13:19:23,694 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/revision/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 1868\n",
      "2025-03-10 13:19:23,814 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/hitorilabs/iris/resolve/fa62476c42edcf9259f895f43da1a7bf9e2697ae/.huggingface.yaml HTTP/11\" 404 0\n",
      "2025-03-10 13:19:23,814 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2025-03-10 13:19:24,052 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=hitorilabs/iris HTTP/11\" 200 None\n",
      "2025-03-10 13:19:24,185 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/revision/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 1884\n",
      "2025-03-10 13:19:24,324 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/tree/fa62476c42edcf9259f895f43da1a7bf9e2697ae?recursive=False&expand=False HTTP/11\" 200 291\n",
      "2025-03-10 13:19:24,464 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/hitorilabs/iris/paths-info/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 281\n",
      "2025-03-10 13:19:24,609 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/tree/fa62476c42edcf9259f895f43da1a7bf9e2697ae/data?recursive=False&expand=False HTTP/11\" 200 249\n",
      "2025-03-10 13:19:24,609 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-03-10 13:19:24,765 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/revision/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 1884\n",
      "2025-03-10 13:19:24,925 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/hitorilabs/iris/resolve/fa62476c42edcf9259f895f43da1a7bf9e2697ae/dataset_infos.json HTTP/11\" 404 0\n",
      "2025-03-10 13:19:25,058 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"POST /api/datasets/hitorilabs/iris/paths-info/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 281\n",
      "2025-03-10 13:19:25,077 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-03-10 13:19:25,274 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/hitorilabs/iris/revision/fa62476c42edcf9259f895f43da1a7bf9e2697ae HTTP/11\" 200 1884\n",
      "2025-03-10 13:19:25,274 filelock     DEBUG    Attempting to acquire lock 1708717970912 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_hitorilabs___iris_default_0.0.0_fa62476c42edcf9259f895f43da1a7bf9e2697ae.lock\n",
      "2025-03-10 13:19:25,290 filelock     DEBUG    Lock 1708717970912 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_hitorilabs___iris_default_0.0.0_fa62476c42edcf9259f895f43da1a7bf9e2697ae.lock\n",
      "2025-03-10 13:19:25,290 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/hitorilabs___iris/default/0.0.0/fa62476c42edcf9259f895f43da1a7bf9e2697ae/dataset_info.json\n",
      "2025-03-10 13:19:25,303 filelock     DEBUG    Attempting to release lock 1708717970912 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_hitorilabs___iris_default_0.0.0_fa62476c42edcf9259f895f43da1a7bf9e2697ae.lock\n",
      "2025-03-10 13:19:25,303 filelock     DEBUG    Lock 1708717970912 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\_Users_pablo_.cache_huggingface_datasets_hitorilabs___iris_default_0.0.0_fa62476c42edcf9259f895f43da1a7bf9e2697ae.lock\n",
      "2025-03-10 13:19:25,363 filelock     DEBUG    Attempting to acquire lock 1708717046432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\hitorilabs___iris\\default\\0.0.0\\fa62476c42edcf9259f895f43da1a7bf9e2697ae_builder.lock\n",
      "2025-03-10 13:19:25,363 filelock     DEBUG    Lock 1708717046432 acquired on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\hitorilabs___iris\\default\\0.0.0\\fa62476c42edcf9259f895f43da1a7bf9e2697ae_builder.lock\n",
      "2025-03-10 13:19:25,363 fsspec.local DEBUG    open file: C:/Users/pablo/.cache/huggingface/datasets/hitorilabs___iris/default/0.0.0/fa62476c42edcf9259f895f43da1a7bf9e2697ae/dataset_info.json\n",
      "2025-03-10 13:19:25,363 filelock     DEBUG    Attempting to release lock 1708717046432 on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\hitorilabs___iris\\default\\0.0.0\\fa62476c42edcf9259f895f43da1a7bf9e2697ae_builder.lock\n",
      "2025-03-10 13:19:25,376 filelock     DEBUG    Lock 1708717046432 released on C:\\Users\\pablo\\.cache\\huggingface\\datasets\\hitorilabs___iris\\default\\0.0.0\\fa62476c42edcf9259f895f43da1a7bf9e2697ae_builder.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE_LABELS: ['Versicolor', 'Virginica', 'Setosa']\n",
      "FEATURES: ['petal_length', 'petal_width', 'sepal_length', 'sepal_width']\n",
      "\n",
      " Contenido del TabularDataset:\n",
      "   petal_length  petal_width  sepal_length  sepal_width      target\n",
      "0           4.8          1.8           5.9          3.2  Versicolor\n",
      "1           3.5          1.0           5.7          2.6  Versicolor\n",
      "2           5.6          1.4           6.1          2.6   Virginica\n",
      "3           1.5          0.2           4.6          3.1      Setosa\n",
      "4           4.9          1.8           6.3          2.7   Virginica\n",
      "\n",
      " Descriptor del TabularDataset:\n",
      "{'numeric': {'petal_length': {'index': 0, 'min': 1.100000023841858, 'max': 6.699999809265137, 'mean': 3.9026663, 'std': 1.7214837074279785, 'median': 4.5, 'q1': 1.7000000476837158, 'q3': 5.099999904632568}, 'petal_width': {'index': 1, 'min': 0.10000000149011612, 'max': 2.5, 'mean': 1.228, 'std': 0.7334774732589722, 'median': 1.399999976158142, 'q1': 0.3500000089406967, 'q3': 1.7999999523162842}, 'sepal_length': {'index': 2, 'min': 4.300000190734863, 'max': 7.900000095367432, 'mean': 5.910667, 'std': 0.8623621463775635, 'median': 5.900000095367432, 'q1': 5.1499998569488525, 'q3': 6.450000047683716}, 'sepal_width': {'index': 3, 'min': 2.200000047683716, 'max': 4.199999809265137, 'mean': 3.0146666, 'std': 0.4056004583835602, 'median': 3.0, 'q1': 2.700000047683716, 'q3': 3.25}}, 'categorical': {}, 'ordinal': {}, 'target': {'target': {'index': 4, 'distinct_values': ['Versicolor', 'Virginica', 'Setosa'], 'count': {'Versicolor': 29, 'Virginica': 24, 'Setosa': 22}}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from flwr.common import NDArrays\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lore_sa.dataset import TabularDataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# This information is needed to create a correct scikit-learn model\n",
    "UNIQUE_LABELS = []\n",
    "FEATURES = []\n",
    "NUM_SERVER_ROUNDS = 5\n",
    "NUM_CLIENTS = 2\n",
    "MIN_AVAILABLE_CLIENTS = 2\n",
    "fds = None  # Cache FederatedDataset\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    \"\"\"Obtener los parámetros del modelo de manera segura.\"\"\"\n",
    "    params = [\n",
    "        max(1, int(model.n_estimators)),  # Asegurar que `n_estimators ≥ 1`\n",
    "        int(model.max_depth) if model.max_depth is not None else -1,  # Convertir `None` en `-1`\n",
    "        max(2, int(model.min_samples_split)),  # Asegurar que min_samples_split ≥ 2\n",
    "        max(1, int(model.min_samples_leaf)),  # Asegurar que min_samples_leaf ≥ 1\n",
    "    ]\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_model_params(model, params):\n",
    "    \"\"\"Asegurar que los parámetros son válidos antes de asignarlos.\"\"\"\n",
    "    n_estimators_value = max(1, int(params[0]))  # Si `n_estimators=0`, convertirlo a `1`\n",
    "    max_depth_value = int(params[1]) if int(params[1]) > 0 else None  # Convertir `0` en `None`\n",
    "    min_samples_split_value = max(2, int(params[2]))  # Si `min_samples_split=0`, convertirlo a `2`\n",
    "    min_samples_leaf_value = max(1, int(params[3]))  # Si `min_samples_leaf=0`, convertirlo a `1`\n",
    "\n",
    "    model.set_params(\n",
    "        n_estimators=n_estimators_value,  # Corregido `n_estimators`\n",
    "        max_depth=max_depth_value,  # Corregido `max_depth`\n",
    "        min_samples_split=min_samples_split_value,  # Asegurar que min_samples_split ≥ 2\n",
    "        min_samples_leaf=min_samples_leaf_value,  # Asegurar que min_samples_leaf ≥ 1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_rand_forest_and_instantiate_parameters():\n",
    "    \"\"\"Crea un RandomForestClassifier con los parámetros iniciales.\"\"\"\n",
    "    return RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        criterion='entropy',\n",
    "        n_estimators=100,\n",
    "        max_depth=40,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_data(partition_id: int, num_partitions: int):\n",
    "    \"\"\"Carga los datos del dataset, inicializa UNIQUE_LABELS y FEATURES, y divide en train/test.\"\"\"\n",
    "    global fds, UNIQUE_LABELS, FEATURES\n",
    "\n",
    "    if fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        fds = FederatedDataset(dataset=\"hitorilabs/iris\", partitioners={\"train\": partitioner})\n",
    "\n",
    "    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n",
    "\n",
    "    # Convertir la columna objetivo a valores numéricos si es categórica\n",
    "    target_column = dataset.columns[-1]  \n",
    "\n",
    "    if dataset[target_column].dtype == \"object\":\n",
    "        label_encoder = LabelEncoder()\n",
    "        dataset[target_column] = label_encoder.fit_transform(dataset[target_column])\n",
    "\n",
    "    else:\n",
    "\n",
    "        dataset[target_column] = dataset[target_column].map({0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"})  # Revertir a nombres\n",
    "\n",
    "    dataset.rename(columns={target_column: \"target\"}, inplace=True)\n",
    "\n",
    "    # # Guardar nombres de características antes de convertir a numérico\n",
    "    # categorical_columns = dataset.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    # if categorical_columns:\n",
    "    #     dataset = pd.get_dummies(dataset, columns=categorical_columns) \n",
    "\n",
    "    dataset.rename(columns={target_column: \"target\"}, inplace=True)\n",
    "\n",
    "    # Guardar etiquetas únicas y nombres de características\n",
    "    if not UNIQUE_LABELS:\n",
    "        UNIQUE_LABELS = dataset[\"target\"].unique().tolist()\n",
    "\n",
    "    if not FEATURES:\n",
    "        FEATURES = dataset.drop(columns=[\"target\"]).columns.tolist()\n",
    "\n",
    "    # Convertir dataset a formato compatible con LORE\n",
    "    tabular_dataset = TabularDataset(dataset, \"target\")\n",
    "\n",
    "    # Dividir en train/test (80% train - 20% test)\n",
    "    X = dataset[FEATURES]\n",
    "    y = dataset[\"target\"]\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    # Devolvemos dataset completo para que LORE pueda acceder a los nombres de las columnas\n",
    "    return X_train, y_train, X_test, y_test, tabular_dataset\n",
    "\n",
    "\n",
    "# Cargar datos e inicializar variables automáticamente\n",
    "X_train, y_train, X_test, y_test, dataset = load_data(partition_id=0, num_partitions=NUM_CLIENTS)\n",
    "\n",
    "print(\"UNIQUE_LABELS:\", UNIQUE_LABELS)\n",
    "print(\"FEATURES:\", FEATURES)\n",
    "\n",
    "# Imprimir los datos reales en el `TabularDataset`\n",
    "print(\"\\n Contenido del TabularDataset:\")\n",
    "print(dataset.df.head())  # Muestra las primeras filas del dataset\n",
    "\n",
    "# Imprimir el descriptor del dataset (información sobre las variables)\n",
    "print(\"\\n Descriptor del TabularDataset:\")\n",
    "print(dataset.descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir el cliente federado con Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sklearnexample: A Flower / sklearn app.\"\"\"\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from flwr.common import Context\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys  # Para forzar la impresión en consola sin demoras\n",
    "from threading import Lock\n",
    "import random\n",
    "\n",
    "# NUEVO: Importar LORE\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "from lore_sa.lore import TabularRandomGeneratorLore\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, client_id):\n",
    "    \"\"\"Plotea y muestra la matriz de confusión.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"[CLIENTE {client_id}] 📊 MATRIZ DE CONFUSIÓN\")\n",
    "    print(\"-\"*100)\n",
    "    print(cm)\n",
    "    print(\"-\"*100)\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, model, X_train, y_train, X_test, y_test, dataset, client_id):\n",
    "        self.model = model\n",
    "        self.X_train = X_train.values\n",
    "        self.y_train = y_train.values\n",
    "        self.X_test = X_test.values\n",
    "        self.y_test = y_test.values\n",
    "        self.dataset = dataset  # Guardamos dataset completo para LORE\n",
    "        self.unique_labels = np.unique(y_train)  # Asegurar etiquetas correctas\n",
    "        self.client_id = client_id  # 🔹 Usamos el partition_id como ID fijo\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Entrenar el modelo antes de cada actualización.\"\"\"\n",
    "        set_model_params(self.model, parameters)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        return get_model_parameters(self.model), len(self.X_train), {}\n",
    "    \n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluar el modelo y aplicar LORE para explicabilidad.\"\"\"\n",
    "        set_model_params(self.model, parameters)\n",
    "\n",
    "        if not hasattr(self.model, \"estimators_\") or len(self.model.estimators_) == 0:\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_pred_proba = self.model.predict_proba(self.X_test)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(self.y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        auc = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr')\n",
    "        loss = log_loss(self.y_test, y_pred_proba)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"[CLIENTE {self.client_id}] 🔍 INICIO DE EVALUACIÓN\")\n",
    "        print(\"=\"*100)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # **Matriz de Confusión**\n",
    "        plot_confusion_matrix(self.y_test, y_pred, labels=self.unique_labels, client_id=self.client_id)\n",
    "\n",
    "        # **Aplicar LORE**\n",
    "        print(f\"\\n[CLIENTE {self.client_id}] 🔍 Aplicando LORE para explicabilidad...\")\n",
    "        print(\"=\" * 100)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Convertimos el modelo en \"caja negra\" para LORE\n",
    "        bbox = sklearn_classifier_bbox.sklearnBBox(self.model)\n",
    "\n",
    "        # Seleccionamos una muestra de prueba del dataset para explicar\n",
    "        num_row = random.randint(0, len(self.dataset.df) - 1)  # Se puede cambiar para seleccionar una muestra específica, en cada ronda se elegirá una muestra aleatoria\n",
    "        x = self.dataset.df.iloc[num_row, :-1]  # Excluir la variable objetivo\n",
    "\n",
    "        # Aplicamos LORE para generar la explicación\n",
    "        tabularLore = TabularRandomGeneratorLore(bbox, self.dataset)\n",
    "        explanation = tabularLore.explain(x)\n",
    "\n",
    "        # Obtener la predicción real del modelo\n",
    "        predicted_class = self.model.predict([x])[0]  # `x` es la muestra que estamos explicando\n",
    "        \n",
    "        # Mostramos la explicación\n",
    "        print(f\"\\n[CLIENTE {self.client_id}] 📌 EXPLICACIÓN DEL CLIENTE:\\n\")\n",
    "        print(format_explanation(explanation, predicted_class, self.model, self.dataset))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(3)  # Pausa para evitar solapamientos\n",
    "\n",
    "        return float(loss), len(self.X_test), {\n",
    "            \"Accuracy\": float(accuracy),\n",
    "            \"Precision\": float(precision),\n",
    "            \"Recall\": float(recall),\n",
    "            \"F1_Score\": float(f1),\n",
    "            \"AUC\": float(auc)\n",
    "        }\n",
    "\n",
    "def format_explanation(explanation, predicted_class, model, dataset):\n",
    "    \"\"\"Formatea la explicación de LORE incluyendo la predicción y las clases en los contrafactuales.\"\"\"\n",
    "    result = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "    result += \"📌 EXPLICACIÓN DEL MODELO\\n\"\n",
    "    result += \"=\"*50 + \"\\n\\n\"\n",
    "    \n",
    "    # Mostrar la predicción realizada\n",
    "    result += f\"🎯 **Predicción realizada:** {predicted_class}\\n\\n\"\n",
    "\n",
    "    rule = explanation['rule']\n",
    "    result += \"✅ **Condiciones para la predicción:**\\n\"\n",
    "    for condition in rule['premises']:\n",
    "        attr = condition['attr'].replace(\"-\", \" \").capitalize()\n",
    "        val = condition['val']\n",
    "        op = condition['op']\n",
    "        op_text = {\"<=\": \"≤\", \">=\": \"≥\", \"!=\": \"NO es\"}.get(op, op)\n",
    "        result += f\"  - {attr} {op_text} {val}\\n\"\n",
    "    \n",
    "    result += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "    result += \"🔄 **Casos contrafactuales donde el modelo predice otra clase:**\\n\"\n",
    "    result += \"-\"*50 + \"\\n\\n\"\n",
    "\n",
    "    for idx, cf in enumerate(explanation['counterfactuals'], start=1):\n",
    "        # Aplicamos los cambios del caso contrafactual a la muestra original\n",
    "        x_cf = dataset.df.iloc[10, :-1].copy()\n",
    "        for condition in cf['premises']:\n",
    "            attr = condition['attr']\n",
    "            val = condition['val']\n",
    "            x_cf[attr] = val  # Aplicamos los cambios\n",
    "\n",
    "        # Usamos el modelo para predecir la nueva clase tras el cambio\n",
    "        counterfactual_class = model.predict([x_cf.values])[0] \n",
    "\n",
    "        if counterfactual_class == predicted_class:\n",
    "            result += f\"🛑 **CASO {idx}:** Si se cumplen estas condiciones, la predicción NO cambiaría de **{predicted_class}**\\n\"\n",
    "        else:\n",
    "            result += f\"🛑 **CASO {idx}:** Si se cumplen estas condiciones, la predicción cambiaría a **{counterfactual_class}**\\n\"\n",
    "\n",
    "        for condition in cf['premises']:\n",
    "            attr = condition['attr'].replace(\"-\", \" \").capitalize()\n",
    "            val = condition['val']\n",
    "            op = condition['op']\n",
    "            op_text = {\"<=\": \"≤\", \">=\": \"≥\", \"!=\": \"NO es\"}.get(op, op)\n",
    "            result += f\"  - {attr} {op_text} {val}\\n\"\n",
    "        result += \"\\n\" + \"-\"*50 + \"\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\"Construir un cliente Flower asegurando que los datos estén cargados correctamente.\"\"\"\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    # Cargar datos correctamente\n",
    "    X_train, y_train, X_test, y_test, dataset = load_data(partition_id, num_partitions)\n",
    "\n",
    "    # Crear modelo RandomForest\n",
    "    model = create_rand_forest_and_instantiate_parameters()\n",
    "\n",
    "    return FlowerClient(model, X_train, y_train, X_test, y_test, dataset, client_id=partition_id + 1).to_client()\n",
    "\n",
    "# 🚀 Crear la aplicación cliente de Flower\n",
    "client_app = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurar el Servidor de Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sklearnexample: A Flower / sklearn app.\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Dict[str, Scalar]:\n",
    "    \"\"\"Compute weighted average.\n",
    "\n",
    "    It is a generic implementation that averages only over floats and ints and drops the\n",
    "    other data types of the Metrics.\n",
    "    \"\"\"\n",
    "    # num_samples_list can represent the number of samples\n",
    "    # or the number of batches depending on the client\n",
    "    num_samples_list = [n_batches for n_batches, _ in metrics]\n",
    "    num_samples_sum = sum(num_samples_list)\n",
    "    metrics_lists: Dict[str, List[float]] = {}\n",
    "    for num_samples, all_metrics_dict in metrics:\n",
    "        #  Calculate each metric one by one\n",
    "        for single_metric, value in all_metrics_dict.items():\n",
    "            if isinstance(value, (float, int)):\n",
    "                metrics_lists[single_metric] = []\n",
    "        # Just one iteration needed to initialize the keywords\n",
    "        break\n",
    "\n",
    "    for num_samples, all_metrics_dict in metrics:\n",
    "        # Calculate each metric one by one\n",
    "        for single_metric, value in all_metrics_dict.items():\n",
    "            # Add weighted metric\n",
    "            if isinstance(value, (float, int)):\n",
    "                metrics_lists[single_metric].append(float(num_samples * value))\n",
    "\n",
    "    weighted_metrics: Dict[str, Scalar] = {}\n",
    "    for metric_name, metric_values in metrics_lists.items():\n",
    "        weighted_metrics[metric_name] = sum(metric_values) / num_samples_sum\n",
    "\n",
    "    return weighted_metrics\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"Construct components that set the ServerApp behavior.\"\"\"\n",
    "\n",
    "    # penalty = context.run_config.get(\"penalty\", \"l1\")\n",
    "    model = create_rand_forest_and_instantiate_parameters()\n",
    "    ndarrays = get_model_parameters(model)\n",
    "    global_model_init = ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    # Define the strategy\n",
    "    min_available_clients = context.run_config.get(\"min-available-clients\", MIN_AVAILABLE_CLIENTS)\n",
    "    strategy = FedAvg(\n",
    "        min_available_clients=min_available_clients,\n",
    "        fit_metrics_aggregation_fn=weighted_average,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        initial_parameters=global_model_init,\n",
    "    )\n",
    "    \n",
    "    # Guardamos la versión original de la función de agregación\n",
    "    original_aggregate_evaluate = strategy.aggregate_evaluate\n",
    "\n",
    "    def custom_aggregate_evaluate(server_round, results, failures):\n",
    "        \"\"\"Forzar espera antes de continuar con la siguiente ronda del servidor.\"\"\"\n",
    "        aggregated_metrics = original_aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "        # **Sincronización:** Esperar antes de iniciar la siguiente ronda\n",
    "        print(f\"\\n⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\\n\")\n",
    "        time.sleep(10)  # Añadimos una pausa antes de la siguiente ronda\n",
    "\n",
    "        return aggregated_metrics\n",
    "\n",
    "    # Sustituye la agregación original por la personalizada (corrigiendo la recursión)\n",
    "    strategy.aggregate_evaluate = custom_aggregate_evaluate\n",
    "\n",
    "    num_rounds = context.run_config.get(\"num-server-rounds\", NUM_SERVER_ROUNDS)\n",
    "    config = ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create ServerApp\n",
    "server_app = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pasos que se realizan en el notebook:**\n",
    "\n",
    "1. El servidor inicializa el modelo y lo envía a cada uno de los clientes.\n",
    "\n",
    "2. Cada cliente entrena un RandomForest con su respectivo subconjunto de datos o partición que hemos realizado al principio.\n",
    "\n",
    "3. Los clientes entrenan, y mandan sus hiperparámetros (Nº de árboles, profundidad, etc.) al servidor.\n",
    "\n",
    "4. El servidor combina los parámetros y actualiza el modelo global.\n",
    "\n",
    "5. Se mide el rendimiento del modelo sobre cada cliente, obteniendo también sus contrafactuales y se repite el proceso las rondas que deseemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Fases del proceso en cada ronda**\n",
    "\n",
    "**1. El servidor inicia una nueva ronda `(ServerApp)`**\n",
    "\n",
    "- Llama a `server_fn()`, donde se inicializa la estrategia FedAvg con los parámetros globales.\n",
    "\n",
    "- Llama a `configure_fit()`, que selecciona qué clientes participarán en la ronda.\n",
    "\n",
    "---\n",
    "\n",
    "**2. El servidor envía los parámetros a los clientes**\n",
    "\n",
    "- Llama a `client_fn(context)`, lo que crea un nuevo cliente (`FlowerClient`).\n",
    "\n",
    "- Dentro del cliente, se ejecuta:\n",
    "\n",
    "    ```python\n",
    "\n",
    "    set_model_params(self.model, parameters)  # Recibe los parámetros globales\n",
    "\n",
    "    ```\n",
    "\n",
    "- Cada cliente actualiza su modelo local con los parámetros del servidor.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**3. Cada cliente entrena su modelo en sus propios datos**\n",
    "\n",
    "- Se ejecuta fit() en FlowerClient, que llama a:\n",
    "    ```python\n",
    "\n",
    "    self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    ```\n",
    "\n",
    "- Se entrena un nuevo modelo con los datos locales de cada cliente.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**4. Los clientes envían los parámetros actualizados al servidor**\n",
    "\n",
    "- Después de entrenar, cada cliente ejecuta:\n",
    "    ```Python\n",
    "\n",
    "    return get_model_parameters(self.model), len(self.X_train), {}\n",
    "\n",
    "    ```\n",
    "- Los parámetros del modelo local se devuelven al servidor.\n",
    "\n",
    "---\n",
    "\n",
    "**5. El servidor actualiza el modelo global con FedAvg**\n",
    "\n",
    "- Recibe los parámetros de cada cliente.\n",
    "\n",
    "- Llama a: \n",
    "\n",
    "    ```python\n",
    "\n",
    "    strategy.aggregate_fit(server_round, results, failures)\n",
    "\n",
    "    ```\n",
    "\n",
    "- Se hace un promedio ponderado de los parámetros recibidos.\n",
    "\n",
    "- Se actualizan los parámetros globales del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**6. Cada cliente evalúa el modelo**\n",
    "\n",
    "- Se llama a `evaluate()` en `FlowerClient`, donde:\n",
    "\n",
    "    ```python\n",
    "\n",
    "    set_model_params(self.model, parameters)  # Recibe el modelo global actualizado\n",
    "\n",
    "    y_pred = self.model.predict(self.X_test)\n",
    "\n",
    "    ```\n",
    "\n",
    "- Cada cliente evalúa el modelo con su conjunto de prueba.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Los clientes envían los resultados de la evaluación al servidor**\n",
    "\n",
    "- Devuelven métricas Accuracy, Precision, Recall, etc.\n",
    "\n",
    "- Se ejecuta en el servidor:\n",
    "\n",
    "    ```python\n",
    "\n",
    "    strategy.aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "    ```\n",
    "\n",
    "- Se obtiene la evaluación global.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Se espera antes de iniciar la siguiente ronda**\n",
    "\n",
    "- Se ejecuta el `custom_aggregate_evaluate` para hacer una pausa de 10 segundos antes de la siguiente ronda.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecutar la Simulación Federada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 13:13:05,386\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n",
      ":actor_name:ClientAppActor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[4 0 0]\n",
      " [0 8 0]\n",
      " [0 0 3]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[6 0 0]\n",
      " [0 2 0]\n",
      " [0 1 6]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLIENTE 1] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 1] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Virginica\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 5.158425331115723\n",
      "  - Petal_length > 2.6255003213882446\n",
      "  - Sepal_length ≤ 6.8689398765563965\n",
      "  - Sepal_width > 2.833991527557373\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Virginica**\n",
      "  - Petal_length > 5.158425331115723\n",
      "  - Sepal_length > 4.651970148086548\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_length ≤ 2.6255003213882446\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 3:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_length ≤ 5.158425331115723\n",
      "  - Petal_length > 2.6255003213882446\n",
      "  - Sepal_length > 6.8689398765563965\n",
      "  - Petal_width ≤ 1.5435811281204224\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "[CLIENTE 2] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Virginica\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_width ≤ 0.7979415357112885\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_width > 0.7979415357112885\n",
      "  - Petal_length ≤ 4.5760838985443115\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[6 0 0]\n",
      " [0 2 0]\n",
      " [0 1 6]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[4 0 0]\n",
      " [0 4 4]\n",
      " [0 0 3]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLIENTE 2] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 1] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Virginica\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 2.5453015565872192\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Virginica**\n",
      "  - Petal_length > 5.085720777511597\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length ≤ 4.7956788539886475\n",
      "  - Petal_length > 2.5453015565872192\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 3:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length ≤ 5.085720777511597\n",
      "  - Petal_length > 4.7956788539886475\n",
      "  - Sepal_length ≤ 6.952340602874756\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "[CLIENTE 1] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Setosa\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 4.8720057010650635\n",
      "  - Sepal_width > 3.6030008792877197\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Setosa**\n",
      "  - Petal_length ≤ 4.8720057010650635\n",
      "  - Sepal_width ≤ 2.756852388381958\n",
      "  - Sepal_length ≤ 5.443597793579102\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[4 0 0]\n",
      " [0 7 1]\n",
      " [0 0 3]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[6 0 0]\n",
      " [0 2 0]\n",
      " [0 0 7]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLIENTE 1] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Setosa\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_width > 1.779465138912201\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Setosa**\n",
      "  - Petal_width ≤ 0.7932414710521698\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Setosa**\n",
      "  - Petal_width ≤ 1.779465138912201\n",
      "  - Petal_width > 0.7932414710521698\n",
      "  - Petal_length ≤ 5.412226915359497\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "[CLIENTE 1] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Setosa\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 4.736778736114502\n",
      "  - Petal_width > 0.6529271006584167\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length ≤ 4.736778736114502\n",
      "  - Petal_width ≤ 0.6529271006584167\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length > 4.736778736114502\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[4 0 0]\n",
      " [1 4 3]\n",
      " [0 0 3]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[6 0 0]\n",
      " [0 2 0]\n",
      " [0 0 7]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLIENTE 1] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 1] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Versicolor\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 4.651082277297974\n",
      "  - Petal_length > 2.951871633529663\n",
      "  - Sepal_length ≤ 5.370950698852539\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Versicolor**\n",
      "  - Petal_length ≤ 4.651082277297974\n",
      "  - Sepal_length > 5.370950698852539\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Versicolor**\n",
      "  - Petal_length ≤ 2.951871633529663\n",
      "  - Sepal_length ≤ 5.370950698852539\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "[CLIENTE 2] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Versicolor\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length > 2.5667777061462402\n",
      "  - Petal_width ≤ 1.4469894170761108\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_length > 2.5667777061462402\n",
      "  - Petal_width > 1.7341139912605286\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_length ≤ 2.5667777061462402\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 🔍 INICIO DE EVALUACIÓN\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 1] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[4 0 0]\n",
      " [0 8 0]\n",
      " [0 0 3]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[CLIENTE 2] 📊 MATRIZ DE CONFUSIÓN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[6 0 0]\n",
      " [0 2 0]\n",
      " [0 0 7]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLIENTE 1] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 🔍 Aplicando LORE para explicabilidad...\n",
      "====================================================================================================\n",
      "\n",
      "[CLIENTE 2] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Versicolor\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 3.001419425010681\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción cambiaría a **Virginica**\n",
      "  - Petal_length ≤ 4.895909309387207\n",
      "  - Petal_length > 4.7893226146698\n",
      "  - Sepal_length ≤ 5.9342942237854\n",
      "  - Sepal_length > 4.692183017730713\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Virginica**\n",
      "  - Petal_length ≤ 6.598191976547241\n",
      "  - Petal_length > 4.895909309387207\n",
      "  - Sepal_length > 4.674283504486084\n",
      "  - Sepal_width > 2.4307583570480347\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 3:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Versicolor**\n",
      "  - Petal_length ≤ 4.7893226146698\n",
      "  - Petal_length > 3.001419425010681\n",
      "  - Sepal_length > 4.692183017730713\n",
      "  - Petal_width > 0.18091244250535965\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 4:** Si se cumplen estas condiciones, la predicción NO cambiaría de **Versicolor**\n",
      "  - Petal_length > 6.598191976547241\n",
      "  - Sepal_width ≤ 2.7738577127456665\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "[CLIENTE 1] 📌 EXPLICACIÓN DEL CLIENTE:\n",
      "\n",
      "\n",
      "==================================================\n",
      "📌 EXPLICACIÓN DEL MODELO\n",
      "==================================================\n",
      "\n",
      "🎯 **Predicción realizada:** Virginica\n",
      "\n",
      "✅ **Condiciones para la predicción:**\n",
      "  - Petal_length ≤ 4.788024425506592\n",
      "  - Petal_width > 0.6871498823165894\n",
      "  - Sepal_width > 2.4253560304641724\n",
      "\n",
      "--------------------------------------------------\n",
      "🔄 **Casos contrafactuales donde el modelo predice otra clase:**\n",
      "--------------------------------------------------\n",
      "\n",
      "🛑 **CASO 1:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length > 4.788024425506592\n",
      "  - Petal_width > 1.5501418113708496\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 2:** Si se cumplen estas condiciones, la predicción cambiaría a **Setosa**\n",
      "  - Petal_length ≤ 4.788024425506592\n",
      "  - Petal_width ≤ 0.6871498823165894\n",
      "\n",
      "--------------------------------------------------\n",
      "🛑 **CASO 3:** Si se cumplen estas condiciones, la predicción cambiaría a **Versicolor**\n",
      "  - Petal_length ≤ 4.788024425506592\n",
      "  - Petal_width > 1.4919059872627258\n",
      "  - Sepal_width ≤ 2.4253560304641724\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ [SERVIDOR] Esperando 10 segundos antes de iniciar la siguiente ronda...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 101.57s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 1.2014551129705717\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 6.007275564852859\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 1.2014551129705717\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 4.805820451882287\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 2.220446049250313e-16\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'AUC': [(1, 0.9816849816849818),\n",
      "\u001b[92mINFO \u001b[0m:      \t         (2, 0.9122405372405373),\n",
      "\u001b[92mINFO \u001b[0m:      \t         (3, 0.9826388888888888),\n",
      "\u001b[92mINFO \u001b[0m:      \t         (4, 0.9299242424242424),\n",
      "\u001b[92mINFO \u001b[0m:      \t         (5, 1.0)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'Accuracy': [(1, 0.9666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.8333333333333334),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.9666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.8666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 1.0)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'F1_Score': [(1, 0.9687179487179487),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.8398290598290598),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.967936507936508),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.8629629629629629),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 1.0)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'Precision': [(1, 0.9777777777777777),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (2, 0.9206349206349206),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (3, 0.975),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (4, 0.9233333333333333),\n",
      "\u001b[92mINFO \u001b[0m:      \t               (5, 1.0)],\n",
      "\u001b[92mINFO \u001b[0m:      \t 'Recall': [(1, 0.9666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (2, 0.8333333333333334),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (3, 0.9666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (4, 0.8666666666666667),\n",
      "\u001b[92mINFO \u001b[0m:      \t            (5, 1.0)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import ray\n",
    "\n",
    "ray.shutdown()  # Apagar cualquier sesión previa de Ray\n",
    "ray.init(local_mode=True)  # Desactiva multiprocessing, usa un solo proceso principal\n",
    "\n",
    "backend_config = {\"num_cpus\": 1}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
